And state agency, why is the price of your house? Which is the number? Do you want to predict the price of the house based on some feature of the apartment? For example, the house, the number bathroom, the quality of the furniture, location. Oh, you collect this information which for simplicity area number the vector. My example is before. So now you have a new apartment. You know these feature you want to predict the price out based on history. So you have some experience. So you have a first apartment that you sell the price, second apartment the price. Notice that these X1X2 exam are the neighbor of the example. These are the coordinates, so it's better perhaps to appear in order to. So now you want the function from Rd. is the expected price of your new apartment. So what is some feature of this problem? First of all, the Y has no idea. For example, the seller does not know that the flight next door has a Van Gogh in the apartment, but the buyer, which is a different no, the price is bigger than the expected one. On the opposite, the standard no that the apartment is busy with the serious structural problem, the buyer does not know yet, so the price is less than the expected one, but also the inputs are noisy for tourism. If you are your estate agency is in Milan, do you want to sell an apartment? General? General is important. The distance from the sea is not a part second, for example, are not uniformly around distributed in your space, but they belong to a small manifold. For example, if your apartment is very big, you expect to have many rooms with respect to the small apartment. So it is like to have a teen manifold in the space. So let me produce some notation. The set X is called the input space. The set of Y is called output space. This family of example is called. And N which is a key quantity, is the number of example. And what you want is a function that depend on the training set. So you have a method that provides for each training set function from your. This is called learning algorithm. So in principle, I have to write the full notation fully dependent from the training sector, but in general you say that the choker is simply right. Director, Director, remember the dependence on the principle and if it only depends on the number of OK, so. This is typical example of a regression problem that you see. Assume that X&Y are R so you can draw a picture the black bound. You have some example. As I said before, for the same accent, the same kind of a part house, you can have two different values and the input does not problem with why what you want is to find. A function that predicts your. What is the best example? The simplest example is this square, so your estimator. Is an affine function where A is given by this formula and B is where X Sigma X ^2, y bar Sigma y ^2 r The mean and standard division minus of X1, XN and Y1 and row XY is the correlation. So instead I have an estimation, good or not, this is a rule. Now this contest, this contest, we are at least three different class of problems which more or less are covered by this board. Summer school, 1st statistical property. This is what you will see in current classes. Second point are function. This is. So in order to find an estimator we need space of functions where we are looking for our and then we have rules select. Active space. This is my duty. 3rd problem is optimization computational. I am a mathematician. I am able to provide rules. Lorenzo. We show that these rules are good estimating the problem. But I am a mathematician. Then I go down the process department. You have to implement. Usually this is a difficult task, so you need optimization technique to implement your rules. To an example, neural networks are very odd. The first idea is from the middle of the last century, but at the time was impossible to implement to compute the solution. Now we can use neural network because smart guy play games. So we have GPU and we can implement the rule so play games. OK. And this is? Now we have to ask for a point. What is the difference between learning from a sample and the statistics? If you submit a grant machine learning, you get the more money that you submit a grant statistics. Why? Because usually here we. What does it means is that B command like that B is the number of feature is very big and the number of examples is very big for example. Which is the best data set that you want to best some algorithm? OK, these are the inputs are images, they are the different sizes. But if you take the average is is number 473 and four business that is more or less 180,000. Usually if you use the standard size, this is more or less on the same order. How many examples you have for the millions of examples? Why this is a problem? Because in that dimension you are alone in the space you see this, which is the so-called the curse of. Assume that you are into space. Is cube of size 1. And you sample uniformly and points. This means that they are not uniformly distributed. They are sample IID according to uniform distribution. When there are some point where the your points are more dense that the one that are not so dense, you look to the start. This is the situation. Now you sample an extra point. These are your friends. This is you. You want to know which is the near the closest neighborhood. So what do you do? Simply detect the mean over your friends. This is the distance. Not very important. OK, this is a random quantity. If you run again the experiment, you get a different situation. So you have an idea, you take the expectation if you want. This is very simple exercise. This is a constant depending only on D 1 / 1. As expected more point we have these become smaller, but exponent is 1 / P The cost of CD is of the other one, not the. So the depends on of N is good, it is in the dominator. But this is you see put in the computer to go on Wikipedia, which is the right constant. Then two pair the case where D is equal to and the case where D is equal to 10 10 no 101,010 OK, what is? You are in the cube? What is the maximal distance? Is the Gabriel dimension 2 Very good? You are connected with your friends 10 This is more or less 1 / 10 The mathematics you are alone in this space 10. Every data set that we mentioned here is bigger than that. So this is a problem why statistics needs some improvement. OK, now as registered by the range we want to do some dynamics. So we need to make some assumption and these assumptions are usually called statistical model. And to be clear, they are never satisfied with the true applications. But it's only a way to do mathematics. So accent is a subtle and lower with the Sigma. If you know what is a Sigma algebra, you're happy. If you don't know, you can give every step. It's not really important example. The typical example is acts as a subset of a deep. X is in the space. X is a topological space. X is the space of probability measure over a set of. One of these examples the spaces the Sigma generated by the open or the subset of your. Why this class is is a subset of with the Sigma reduced by Sigma of R? Or the possibility is π is a finite set. That is a classification problem. And the classic example is Y binary classification. For example, Y is a Banach space. And this is the functional regression. Apparently this is the simplest problem. This is the medium problem. This is the model. The most difficult problem this one because it is difficult to have an example of function taking value. It is easy to construct function taking value in a balance space. You can ask for your best friend analysis. So I will discuss this point and for sake OK, now there is the basic assumption train set. Is the realization of a family X1 by one XN of. Define on the same Omega Omega that this is the measure such that. And these are the 2 assumptions. They are independent and X power is identical distributed as a para random variable. X&Y acts take value in calligraphic acts and Y take that. So this is a real and variable. This take value in X. So in short IID South. So the statistical model is completely described, but this too far, so you can look to determine this as the same center. And as the new example, OK, so X is energy, this is a backstop, random backstop this is. So I need some location. Rho is the law of the back and Y. So this Mr. rho is a probability measure defined by. Or prefer is the image measure of the function X&Y regardless as a map from Omega to X times. Rho X is the law of X and it is the marginal distribution of rho. So if you define π rho, XXY, Y then for each pair select first component is the main measure of last ingredient is the so-called tower property. Or conditional probability acts in the phase the resistor probability measure drop, which is probability measure on. Why such that the following? The integration with respect to the full measure of can be written as anything integral. The standard integral is done with respect to budget distribution. The inner integral super wide and this is all true for all integrable function. This formula can be written in a different way using the concept of condition expectation. The expectation is assumption. I'm sure that this is finite. This is the condition and expectation with respect to the next variable and is well known that this is a function that depend on. So the standard expectation is precise. But there is also the converse. If G is measurable, which is for us always true, and map up here to put what Russo this is finite, then G is integrable. OK, so I'm sorry I have no time to give you an example, but if you think that the raw has density with respect to the back measure, OK. So to the Max is result of true things. R is very simple. If Y is an arbitrary measure space, this form is not. There are some examples you you don't have, but if the Y is a polished space, the trisable the parable complete topological space. So it is very large class. So in shock always almost true now is that we can sing the same procedure in two way. Remember that the role is the role of XY and X1Y1XL IID. This means that X1Y1XL. So this is important because if you want to do simulation, you fix it all. You are God, you know everything wrong. But our formula which is not symmetric, you can write it. The RO XY is the product of the marginal distribution and the condition probability. So this give you a different point of view. This is a 2 step percent consider #1 the sample. Only the inputs accordance 2 given an example XI. Why? All right, so you have a picture. Assume that X is in R as a density. GX measure this distribution. So the probability to select the example in this region is proportional precise area. Now you select the point. Here you have the vertical alignment. You put the vertical alignment horizontally resolved. You sample input according to this. For example, and our distribution. So please give you the practical mean of of this. This is the sampling distribution of tax, even as this is the sampling distribution of OK. The second ingredient is the lost function. Now what is our goal? To find a function? Forget the follow up for the moment. That independent example is a function 2. Why? Because I have only friend analysis, so I'm changing the program or perfect. Why? Because so now I want to the rule to quantify how good is a good estimate, OK? Remember that the goal is is that this should be close enough. OK, this is done by two steps. Even X&Y&Y I compare the true value with the predicted Y by means of the function LL is from Y 2IN positive, possibly infinite. L is called the loss function. It is a good wise evaluation how to pay by predicting Y in an X examples. There is one which is standard one. Why the square loss better than the other one? Because usually you have an inner problems. Clearly you get replace. For example, with 1 you get. There are if you notice important that if you fix why? And if you look to the dependence of the future, this example and almost all the example, this function is continuous. And more important, convex, you know that convex the main task is for free. So these two properties are very important. Almost all the result that we will see with the square loss all too because we have this. No, for classification there are many losses saving time to introduce. Sorry for that, but come back to my problem. This point is not useful evaluation. So the second step is remember that Y&X are random variable. This is a real positive random variable. It's difficult. You take the expectation. This is the point we assume to that we call this expected. And integration. So now we decide to measure the performance of a functional by this quantity. And R is good if L is no OK. So there are two problems 1 that these. Second, what does it mean? Small can be 0 integration, so the first is to select the best that you can do. This is called the target function. OK, is minimizer take all the function take the inference over all the farms from X to RI need Because I am a mathematician, it's a technical condition. So remember, if this is convex, this is space, this is the expectation. It's linear. This function is complex. So you are a match problem. It was two principles. We can compute this one. Let me check it. So we use the power property. Remember that we are minimizing over all the back shots. Forget for the moment. OK, so observe that the dependence on F is clear, which is attacks Y external integral does not make any at all. So check the group in the afternoon F raw axis is the argument over. Axis Bizzle here. Now after that you have to do the measurable if possible. So this problem reduced to simple 1 dimensional OK Instead of a launcher you can do the computation with the. This become a quadratic formal compute. Find that F rho star X is the integral, that is, F is the actual function. In order to do the computation you need some assumption, and your assumption is that Y as final second momentum by for almost all X finite and one can show a simple computation that Alan. Expected the loss of alpha square is the distance between the F and the fro in the, so this in general is not zero so small anything and so the key quantity to control. Is minus in the difference between and the minimum value of the disease. And with the square loss, this is so you have an idea that when you repeat this on the right, this is called the accents loss. And so R is good if which is always is very small. OK, a cup. There are two ingredients, the definition, the loss function and. These are rules. If you want to build a different theory you can do this assumption. Now there is a problem because app is not app is algorithm and when I translated the random valuable these are random variable, terrible random variable because it is in the space of measurable function. But we are lucky because we want to measure the performance according to the rule. Again in my expectable function. Now there is only the issue. This expectation is done only with respect XY&X. Remember that this means that you have this the badness, so you take the expectation. So the keep on getting the square lasso is this quantity and then you write see is earlier. So I give you the idea that you make a break. To ask that this quantity is small does not make sense. Think is very simple example. If you flip the fur coin and times it can happen that you get an ads or and takes. This is what you get is more or less that you get approximate. So we need to make a quantification of these segments. There are some water coffee machine. Need to be back down the back of yes I feel as I need to go back to the AIT. We would. We would like that this is smaller. Control absolute sign on. Because I want that asylum. So one way to ask that this is so the probability to have a training set. Remember that this is depending on your training set, the number of trims that where you are. This is larger is the program is very small. So how to quantify this 6 epsilon? Your measure control accessories is dark that so if you are familiar with the probability this is the start of this quantity in probability and because we are mathematician we have to do this for our epsilon. Second way way is remember that is already an expectation to take this petition also on the. So this expectation is now with respect to determines after. So we do first the definition of expectation with backs up and then we take expectation of what it is this time next. So this different way give you 2 definition. Consistency learning algorithm is universal system. System if all X&Y in the text value XY. This condition is for square loss, otherwise you have to replace some different condition. You have diffusion probability when N goes to Infinity, the probability that the test loss bigger than 0 epsilon silo equal to 0 epsilon bigger than 0 in expectation. So as I said before, this is the standard convergence of these random variable 0 probability. There are a relationship between these two concepts. Remember that this is a positive variable Markov inequality. You combine the probability, the tails, the expectation. Also from that you get the convergent consistency in expectation implies. The Columbus in general is not true, but the formula for expectation tell you that expectation. And if you have some condition like the bag monotone convergence together, if you believe it, you put the limit. This side is good to see so, but from this you can have that from convergence. Probably you can have convergence. Usually machine learning use a different formalism which is based on the following solution. Remember the picture? This is the distribution. The fact that this is the best. Now sign on. What does mean is the consistency probability that for every epsilon when M equal to 0 is 0 opposite you fix the. Because we are probability ETA is between zero and one and you select epsilon in such a way that it is ETA. So you define epsilon which depend on the if over all the epsilon epsilon such that. This probability is larger than ETA. You are very familiar with this. If this is the normal distribution, this is the. This is a different distribution. This is the question. So by the finish you have much learning. You can rephrase the property in with probability one. Very tough. This one as probably it is usually called final simple bound. So you have to do not try the probability. So you can work within a point like using analysis and consistency in probability is equivalent required that sorry. Clearly this quantity and was wrong. The dependence on M is here. This is the ETA confidence level, sorry. So consistency probability is that for almost all the Princess. How many one your accessory flow? The cost is less than this point because this is important because now you cannot because it depends on the law of X&Y. For example, you can compute the critical value if you know the normal distribution. So the next step is instead of using, deciding what is because you want this good to see you can have an upper bound, so you go over to provide. Or. There is a good news. The good news that is quantity which is called upper convergence rates can also to distinguish between different algorithm. If this is good faster than another, other prefer to use this. So this is a quantification about the speed your algorithm go to 0. This is important because you can decide how many examples you have to pay. Remember that you have an example, you have to pay as a for example. So what is the to find rates which are independent? This is a fundamental theorem learning. Assume that acts as. Clearly, in fact you have only one point. It's not the problem for your space can be rich enough. Fix the sequence of positive number going to 0. This can be your uniform convergence array for any learning algorithm. Any neural networks? Super neural networks? Super super neural networks? The resistor X and Y. It is a model of our problem. It is good enough. This is stated for the square larsso and. This should be 0 convergence rate should exist so that the convergence rate on this distribution is less than you get a bad distribution. So I have no time to discuss this point. To have uniform units you need some a priori assignment on the process. So you cannot have a uniform rates. You need some assumption on distribution here. You perhaps can see it here on the rent. So. But this is the deal. So in order to have CN you need to restrict the class of model. OK, so so this is and the first class of. We see and algorithm and. Is important because if you understand the linear model, you understand all the currents you will see on Thursday. So motivation, which is our goal, minimize this quantity, which is. We have two problems. We cannot compute this. Because the band, for example the space of all my harbor functions. A problem B OK, I will show the first B and then A simply replace this space and now. Examples H. In your mother, HP is the space of all linear function. We define a number of parameters. Another example. Well Sigma, this is not a space map, but it has a fire down the bias. The offset is the box is an example of parametric. This is a vector space. This is not a vector space. But in most cases there is a problem, remember. This quantity, if I now replace this phase with H engine is bigger. So the choice of H is crucial. For example, with the linear model is always bigger than this one neural network. By the classical theorem, in order when you have equality, you say the H is universal. Notice that if H is not unified and that's, then you cannot have consistency. But you can replace the excess loss with so you know that this is bigger, not. If you decide to use a linear, this is the best you can do. They want an algorithm that this is small. So now this one is the right one to study the statistical problem of your algorithm from the space age age. What about the first problem? There are many different techniques, so to start that one is to replace the expectation command that is in the ground. This is a you know the things Santa. This is called a. Notice that this is a. None. Easy and minimize. Comments H is a nasty space. Like our networks, it is not clear that it minimizes existence. You prove much more importantly, you need them. Second, the result depends. This is a real algorithm. So in order to start this property we do very simple example where this is the score loss and this is the linear. This space we have an idea, this is the classical this one or we can discuss some problem. From now on and H so X is subset of our D number that D is the number of feature. So first observation, since this phase of function is parameterized by the finite number of parameter, we can simply replace the space of parameter. So that we can start with the following problem. And what is the minimizer? And when we have to compute this, isn't it OK? So this is more or less what you know. OK, now I put all the data together. I construct the data matrix, put in the example the row. The vector are called vector, so X1 transpose. This is and times the matrix, not that we have the two key parameters and the number of examples of the space. Of the outputs in a fact and so I can rewrite this quantity. OK, this is a very simple problem, we will discuss the solution. But now this factor it's all true for me. Loss function. We will see this in Symbian classes. We expect this parameter. It is convex, so when EU the normal W goes to Infinity, so the standard result of continuous partial they always exist. How to compute? I have to introduce some notation. OK semantics, but your example are column and this is time matrix. I can compute the problem. This is AT times T matrix which has a nice property. It is semi positive defined and it is so by standard result you have a base of eigenvector where the name base is an orthonormal base. And family of course number. OK. So remember that Sigma positive or equal to 0, the stifty positive to 0 again maybe play a different role because. The range of this matrix is equal to the orthogonal complex of the kernel, which is equal to the this kernel. Is it this by definition? But it is also the span of your example. This is important. So the measure of this matrix is precise the linear span of your data. So you can introduce a strange operator. So doing first reach on your base. Which is the inverse on this space, and it is zero. If your axis is injective. This is simply the inverse. Other one is the English only on the. Which is the. OK. All the miser are of the form can be written as a sum of 2 contribution. The first solution is given by applied to this guy and this is in the image of cosmetics and this is any vector. So the proof is very simple linear algebra that I would like to stress some factor. OK, look to this one. This is unique as a fight. So the minimize are are not unique unless the so. So first property which is algebra among all the solution this is the minimum. So I mean. What does it mean? You are a problem? The minimizer is not unique, but it is a way to select one by requiring that the solution as the smallest norm. This is important. You decide to use the norm of the vameter to select the solution. Now there is a different point of view. I recall that the what is the range of this matrix? Another way, the special solution is the only one that can be written as a linear combination of your examples. So this is a very natural machine learning. You have an example also loading a function. If you have a few examples you can explore the space you select. Luckily this is the minimum solution. You can give more information. Let me little bit fast on this point. I have to and then what does it means that this is your formula? You have a different representation, so I see. So this is classical. This is. What is the difference? Here you are a matrix which is D times. Here you are a matrix which is D times North call back to which is the difference? What is the difference? This is D * D matrix. This is and times and. To involve a matrix is very heavy depend on the size of your matrix. So if D is smaller as usual when you do regression in classical. But if learning machine learning you have less example than dimension. Let me comment briefly at this point. We have two regime. You have more examples than the dimension. There is a thermosphere and algebra that the dimension of the image is D minus the dimension of the curve. Remember that the dimension of this is the dimension of the curve, so this quantity is smaller than D and injured and is larger. So the range of X is in general smaller space of. There are some very specific situation where this is, so this is always true. So what does it mean in general? Why your input does not? This means that does not exist W In such you feel you're done. This is the fitting problem. Oh, this is impossible. This is the starting point when you're doing an algebra, you want to solve this system. This isn't has no solution as well, which is the square simply. You take it do Y that as minimal distance. Usually if X1 are linear independent, now there is the second point, the over. OK, so these you can use the formula with the compute use this this metric setting you get the same formula. D is very larger, so this can be. You cannot be bigger than so exist, but it is never unique because if this is 0D equal. So in this setting we became interpolated when we have a degree of freedom. OK, in this few minutes I would like to stress that we have a problem and provide you a different expression for using format. OK, I need to introduce another notation, but well. So this is a vector in Anna and well, is an autonomic. So you can solution following by the sum is over the value of Sigma and just positive. So it is a sign over vector corresponding to three positive eigen user. And it is the component of the vector Y on this. So now Y is a noise problem. So the solution is not stable. Since 4 Sigma very small, so if you have a noise on the larger component, there's no problem. If you have the noise on the on the component corresponding to small, I can bet you have the problem. That is that if you replace this solution with the general solution, because the noise affect this point, this point, but there is no way to, you have to hear. So the next step is region recognition, also called thicker of regularization. This is the norm. This is the norm. OK, good news, the problem is complex, but this positive. So you have problem which is quickly convex up and. Simple computation show the solution. So these are the two former. I wrote before that reduced 20 good news because this is positive. This is simply positive final. This is in fact no problem. You ever presented the ML, This is the coefficient. You see computers met, you have the Lambda you have and you solve the problem because now is Lambda. So this is stable against the noise. But this is done with the parameter. I have started with function spaces, so remember. This is a vector space, but it is possible to put escala Prado such that it become. I will write only the normal, so you can have different linear parameterization that you cannot be unique. You're confused all the norm. You can show that is achieved, but it's not very important. So this becoming the space and my problem is. So this is the square. This problem is unstable. The penalty tab, the stability is done to come up, you know, lasted 50 minutes. I will do 2 extension. You can show that if your loss function is convex and Y primer. This problem is always critical. It is bad children because this fund is positive, so that for any nice age Syria doesn't mean nice age. You are existent and. The specific form you can afford it with the now to extension. Usually you discuss with people doing linear underground statistics. This is not the linear model, the linear model. You have a bias, so there is many different ways to give the bias. 1 is sending the data. It's very start but let me show you a different way which is 1 very important one. We will do the thing. Simply your X in Rd. maps to that, so you can introduce a map of T from your input space to in this case Rd. Boswell PX is call WT alga and the problem ready to minimize. Well, X~ is. So when does this trick reduce this nonlinear problem to a linear one? Simply replaced in the previous form X and external no problem. Now this 5 minutes. Oh, this is very nice. Why recall for example? X1 is the house price. No sorry it is the house area. It is the number of bathrooms. When I do my back door is a linear combination. So many problem these and these axis are more important. In particular, you want the solution that as a few component. Remember that you have a estate agent, you want to know how feature of your partner are more important than that one. Not in our community. There's no time to show that with the binary square never get sparse. They are stable, but they are never sparse. So the idea is the trick. Regression. Remember that this is the square, the sum of the square. So this is the normal change because I like to fit the data. But instead of having the kid and arm and the other one, now I put 2 minutes. I would like to show why this is affected by simple computation. Starting 1D example. I have only one per meter and up to some normalization. Sorry. So our Metro West is in our. I have a lot of parameter, but to simplify the computation I call that on T This is a particular instance of this problem now, because this problem does not depend on 0 if W is 0. OK, we have 3 regimes. If T is bigger than one, both of these coefficient are positive so that the two vertices are in this plane. So wrong because with my normalization in 00 and sorry for the picture. OK, this is so the minimizer. As for the square, no sparsity with T less than -1 the same situation on the opposite side. Both vertices are decided, so we get I mean my ear which is not passed but if T is between zero and one. This minimizer is on the right, This minimizer is on the left. So for full range of parameter which is under your efficient you get this. Yeah this is a very stupid example but one minute OK, thank you very much. You will see all the same. We have a list of Lush.