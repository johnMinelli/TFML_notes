The observation are pairs X1 XIYI OK and this is XIY. IXI. These are IAD. These are the data and the problem is given the data to minimize. 1. More expected. OK, what you've seen also is that this is cute. That is a theoretical question. In practice, what we typically do is that you replace the special measurable function manageable and you replace the expectation with. OK, so the moment you solve the second problem you get the function F. OK, that depends on the data and you have study the quality of this. Solution. OK. And there are many ways. The one which is a bit more natural from a mathematical interpretation for two of you is going back to goal of learning which is not feed the data. But if you predict well the future, the future here is represented by these people in the test interpretation, test error. So in words we want to say. Is the. Test. Error. This. Start the best value is the test error that we achieve close to the best possible test. OK, testing what you want to do consistencies. For example, the fact that the number of points to Infinity, we approach or. Get close to the test. OK, word about notation that this one would be clear, but today it matters. Whenever you see a hat, it means full dependence on this entire set, OK, which means that it depends when these guys are random. It means that FF is a random variable and it depends on the cardinal. OK, in the head there is actually this old stuff, OK, so sometimes you take the limit on going to Infinity where something depends on FF because it means that we let this I from 1 to north, OK, we're good. So what going today do today is to give example of how you actually try to analyze that. And we're going to do it first specific setting of linear B squares. OK, this is my favorite. So because we do linear E squares OK, it means that we're going to have some. We're going to replace the space H with base of linear functions. OK, this has a few implication. And the main 1 is that I'll change the notation a little bit because the moment you study linear functions means that each function is completely characterized by the West. OK, so you can write S, you can write W Strictly speaking, they're not the same thing, but there is a one to one relationship. Make sense? So the first thing I want to do is to say, OK, I was joking. I'm not going to consider this. I'm actually going to consider, let's say you just like this brutally, OK? I want to replace this with ID, OK, and then here. I want to. Replace this with Yi minus W12XI. OK, You see why now? It makes sense. Just a small change of notation. OK, we also need that now. Right here on your head. It makes sense. Just a small change of the other point is that sure, I'd like to. I'd like to shoot for a star, but a star is the best over a possible measure of function, and what I'm actually considering is linear functions. OK, so even if I allow myself to cover all linear functions, the best I can get is what list this with Y minus minimum over West in Rd. of L of West. What is L of West is again simply the expected risk for linear functions. OK, when I use linear function, I cannot get anything better than a linear function. So the natural comparison here is to the best possible linear functions, which means that I introduce an error. OK, which is the difference between the best possible test error and the best possible test error for linear functions? Does it make sense? OK, so this part we will not control. It just depends on the fact that we're dealing with linear function. So what we care about is this time OK? Are so good, right? Good. OK, yes. Is the best possible model on your. Linear functions on my, not on my data. Not on that. On this. On that. OK, so whenever you see an L is test error. Whenever you see an L, hat is training error, training error, test error, training not training. OK, hat data, no hat, no data. So this is an ideal quantity, but it's not the best of the best because it's the best linear function. OK, if I give you infinite data, you will not you will still you can say, oh, infinite data is perfect. No, because you're looking at linear function. We're good. OK, so let's keep it there for a minute. And let's. Remind each other what we actually study, which is not exactly this, but what we study is not generically, but is rich regression. What is rich regression? You take this least squares stuff. OK, but you are a ^2 norm path. OK, as it turns out for this specific choice of loss function and you have some some benefits because you can actually write down the solution in an explicit form with the formula. OK, you remember the formula. So if you allow me, if you remember the minimizer, this is a strictly convex continuous function that meets a unique minimizer. We call it W dot Lambda, OK. And it turns out that this W dot Lambda can be written in a specific form, OK, And I will show you the way you want to write. I will write it like this. I think you wrote this way OK, and I guess you put the north here. I want. Does it make sense? So X transpose X where X is the matrix plus N identity to the -1 X transpose Y. OK, now I want to give names and shorten the notation because you're going to write this equation 50 million times. OK, so I want to do the following. First of all, these NI want to write it not here, but here. So I factorize NI, put it here and move it back. Divide by N means I have an extra N and it close up here. OK, the fact is simple enough. You just factorize N and then you put it out and then you give a name to this guy. You call it Sigma hat and give a name to this guy. We call it H hat. OK, a notation that I'm going to use throughout the next hour and a half is that when I write the matrix and I have Lambda identity, I want to give a notation to that because I don't want to write plus Lambda identity, OK? So if they give you a matrix M and then do plus Lambda identity, I'm going to denote these like this. Whenever there is amma, Lamb, matrix, superscript or underscript or whatever is called Lambda means that I shifted by Lambda. OK And I guess at this point would agreement mean that I can write these in the following form? OK, make sense. So we try to keep it here as a dictionary so that you can keep it. Remember you can see by inspection and you can prove it easily. This matrix is symmetric and positive definite or this positive segment and it goes from D dimension to D dimension. As you see, the theory we do today is they mention independent. So I write anything more in a factor for on day, we're going to say, actually we didn't use it. These are going to become diverse pieces, but today we speak to factors. All right, what else we want to say? We want to say we want to work on this a bit. So this should be more or less a recap of what you've seen yesterday in the 1st a couple of lessons, OK. And the main difference are I call this, I put here vectors rather than functions because that's what we want to study. And I rewrite the regression like this. So remember to assume a couple of assumptions which are by no means the weakest, but by all means the easiest. We're going to assume that Y is almost really bounded by a constant and that X is almost really bounded by OK, because that is easier. Other modules. This would be Gaussian noise. If you're true, you might want to basically this. Just assume that you can do whatever you want in the middle. You don't have, you're not Gaussian or anything nicer like that, but you have no tables. OK, you can imagine. We have bigger. Tables. This just makes like the second thing we want to do is to understand a bit better that there is a lot of structure. In the case of east squares, there is a lot of structure here, OK, Particularly we want to show that this problem, because we are in finite dimension as always, the solution OK. There is always at least a solution W dagger. And in fact there is one special solution which is one with minimum norm, OK? So we don't that's what we need, OK, But we actually need to characterize this a bit. So if you remember that L is the E model this equation, take this equation, take the grain with respect to West, set it equal to 0 and see what you get. OK, so if you do delta, you just write delta LW equal to 0. You get an equation which is this. You can kind of see when I derive this, I get twice this and then I have X transpose in front of it. I split it, I use linearity. You get that. What are these two quantities? What you guess is the same stuff above that be the expectation. So actually Sigma is equal to expectation of X front, H is expectation of X. Do you see it? We're not going to do this computation. We're going to do the interesting computation. Take the gradient. You get twice the stuff and X transpose in front. So you get sorry there, and then you get XY and then you get XX transpose. That's how you get this equation, this stuff. You can just work it out. Do this whenever there is a simple calculation that you can do. OK, this is a nice equation and it shows that this is the question that you have to solve to be an optimal vector. It turns out that in this case, it's always. It's easy to show H. Is it always in the? Sorry. Sigma. Is also symmetrical, OK? It means that I can talk about its power, and in particularly it seems to see that age because it has the specific form, It's always in the image of Sigma 1X, OK. But because we're a finite dimension, OK, this is actually equal to the image of Sigma. This means basically that the equation, which is the normal equation infinite dimension has always the solution, OK. In effect, it has as many solution as the null space of this diner system. However, there's only one solution orthogonal to the null space, and that's what this small remark, OK, you're not going to have a huge importance, but it shows you a bit of things. I'll tell you what's important. One, when you write down the minimum of this, this is not an assumption. It's all OK, that's observation. Observation 2 is W downer is always going to satisfy this and we're not going to use much effect. So. This is always true and we have a disposal this equation. OK, the last thing I want to do and I'll. Do it here is to show you. A relatively simple thing which is the following. Suppose that you take AW and you evaluate W over some X and then you take the square and take it. OK, this is the L2 norm of the function induced by the vector W. This is a square. Roll it out together as a stuff where there is. W. Close to each other and then you get something like this. So literally I split the square. I use symmetry of the inner product and I don't put parentheses. Then you get this guy. Then you because of the inner project, you can move the expectation in front of that. This shows that this is just W transpose Sigma W. OK, but then I can also write this as the square of West 1F and so I get the. Two norm of a linear model can actually be written as the norm in Rd. pre composed with this Sigma one OK. In a minute, we're going to show that this quantity can always written as follow. Now I want these to be the rich regression estimator and we we show that we can write. This OK? So the risk in the case of least squares is a very special form because it's just the norm somewhat with the OK. So how do we show this? OK, you take. I just do the sketch because it's not particularly instructed and. We want to start you. Do this. So you write this down and you say OK actually the computation holds for any W. So I do this, then here I add and subtract X transpose W Duggar. OK, yeah, just subtract it here. And then I get Y minus this guy plus this guy minus this, OK. And I developed this for it. So I get A+B. OK, I write a ^2 + b ^2 minus twice AB and so you should recognize what the this is. This should be a, this is this guy -1 of them. OK, the one with the -, Then I have the one with the +. Minus this, sorry, I said that this holds for any W. So this any W is going to be interesting 1 is going to become reach regression, OK. And then we have, I have the product of this, we got this square and the product of that, let me write it down plus twice expectation of y -, X transpose W, OK. OK, I just wrote it. OK, and here I factorize that transpose because I can't remember X transpose is just the vector, it's just the random variable. X. OK, OK, at this point you can just recognize that this term is actually zero. Why is it 0? Because this X transpose, OK, you can actually put it in front of here and it becomes an X, OK, it's just an inner proper. So you can actually write this this way, if you now this guy doesn't depend on X or Y anymore, the expectation is going to hit here. What is X transpose Y? We gave it the name. H. It's just H and this is X transpose. There's transpose was here when I moved here. There is no transpose. XY is HXX transpose. Expectation is what? But then this is H minus Sigma WWW. But H is Sigma WWW. So this whole thing is zero, OK? There are many ways to prove this is the simplest 1 without advocating this projection, OK? This is constructive and it is usually an algebra, so OK, First of all, OK. OK. But then let me this is 0 because of the equation, this is just expected risk of West or any West. This is just the expected risk of this is just the expected risk of West. This is just the expected risk of West Dutter. So put this on the other side of the quality and you get this term. This is equal to this. If you put now these ones for any W, particularly this guy. So this is equal to this. I can do this, but this is equal to this. Just look at it because this guy, we know that he's just LW Dutter and this OK, you have to look at the equation we just saw. You see, it's just a vector is you see from here. This. Write it like this. You see this is just X transpose the vector. The vector effects to be this guy but it's just effect. But then I can use this. So whenever you have squared norm of a vector blah blah blah, then this is just Sigma 1/2 that vector and we prove what we want. OK, yes. Here. Yes, it is. So we are taking the on the. Training set? Sure. It was for any training set, OK, so here what he's saying is this is a random variable, OK? And so if you want these words for any vector and you can even imagine that actually this guy is deterministic quantity because they just speaks sequence XIYI, which is not even stochastic. OK, So these words for disregarding the stochastic nature of that almost surely for almost deterministic for any vector, if you now get to be random quantity, OK, these all quantity becomes a random variable, OK, So they expect this expectation is only over X&Y OK, then this becomes random quantity to study with the sub kind of OK. So right now again this equation was for any double S so I don't even have anything for example. So the nice or the back thing of this person who you are is what you start to see. It's a key input in it's all about matrices because somewhat hidden everywhere in the regression estimate or in the best, whatever we're discussing today, It's not true for other functions or for other norms. OK, The moment you change the squared error for some other error or you change the squared norm for some other norms, everything you just said goes apart. So these squares become somewhat. It's an easy number because you can have very sharp, very instructive computations, even though somewhat is a bit of a toy algorithm, kind of. Because you. Use it, but it's definitely somewhat that's some special that one plus certain question. And before I well, let me just say if you take any, if you take some, it was a different by Montanati back and such a right period of learning was interesting because at least half of it, maybe more was these squares. So these squares is some other workforce to understand properties about rhythm because of the special. Structure I'm going. To show you. Today is. The simplest thing because. I don't mean the most. Let me just recap what we are because now we're going to start the real deal, OK? The first thing I did is just a reminder that machine learning is learning a function from data. We rest intercept to least squares, the linear model. We add the rich penalty, OK, the square norm penalty. And then we observe that this is the expression of this. You make a couple of assumptions. We've used in a second, OK? And in particular, what you see is that and then we observe that the best possible linear function if you're in P data, OK, it's a vector that satisfy the linear equation. And here we saw that. And then the last, the third bit we saw is that because of the special structure of D squares, the difference between the best error, the best possible, the steroid becomes norm, which is not just the norm. You can check here the difference between putting this norm or not. OK, if you just remove it, this is a pretty reasonable metric. OK and this is used in mathematics all the time. You have two vectors, you know how close they are. But this says I don't really care about the vector, I want to know if they will produce the same predictions. So this guy here basically say I want to weight the similarity of my vector where you know it matters. And because you can think of this with large eigenvalues, more eigenvalue. Turns out that these are the principle components. It also says that you break. If you want to say something intense in physics, you break the symmetry of this space of WS. The space of WS is not true that all direction are the same. Certain direction matter more than some others. The one that matter the most are the one associated with the eigenvector corresponding to large eigenvalues of these matrix. OK, so all of a sudden this becomes a meaningful quantity and again, this is the interpretation of this. So is that like a covariance matrix that is the? Covariance matrix is to be more precise, is the expected. Is the covariance matrix on the population because it's the expectation over all the data, and it's not quite the covariance because we don't reset. So more precise the second moment matrix. OK, this is the. This is also called the second moment matrix, and then most human beings called. Covariance and you. Don't care about recentering, but just once in your life you might want to know that there is no centering here and this is the empirical matrix or the empirical second moment you want to. OK, so yes, if you were to normalize the data that would be required, you have to reset the data. But then you have issues because of what the. But let's say the important thing here is not that you can make it the real, that you don't care, because the real covariance is not somewhat here. Yeah, I guess so. I think the sincere answer is in practice, linear models are not. Very smart choice, so put the WX plus. OK, so if you care about the plus B in discussion would be I want to think of situation where the dimension D is very large and then B doesn't matter B. The fact that you have to pass through the origin is an important observation in low dimension. If dimension is high, now you can actually solve the linear system no matter what. OK, so just as a reminder, the presence of the offset is 1 to DD. OK, so here we don't put it. And so some sense that's why the origin doesn't play any special so, but that's kind of why physically. The. Central class because we think that the data high dimensions, yes. Which one? There are two vectors down. There is this optimal guy and there is this optimal guy. This guy you don't find, only God finds you because you need expectation that you don't have. OK, so this guy doesn't overfit because see all the future in the universe. This guy, I don't know. Check. It overfits it that sucks. That's what you want to study. OK, we want to study. It is overfits and the rho of Lambda and there's some assumptions. That's exactly if you want what's going to say what is overfitting? I don't know. I think overfitting is somehow this sucks and that you call over fitting where you get a very small error on the data, cares about the data, the data is just what you have to do. I think what matters what happens here, OK, if this guy is bad, you mean that you over fit it or under smoothed. So we want to understand the behavior of this to make sense of the word over fitting or under fitting or. Sucks. OK. Anything else? All right, for the purpose of the following discussion, it's useful to introduce an intermediate guy. OK, we need this vector to speak to this vector, but it's we need an intermediary, OK? The intermediary is going to be rich with infinite data. OK. Now, because this is not an interesting part, I want to write down an expression. You look at it and then you justify where it comes from and you do it very much OK. West Lambda is a sibling of this guy, but he's older. He has seen all the data in the future. OK, so it's the same expression, but instead of having access to finite data, it is absolutely. So it means that here you can write Sigma Lambda -1 H OK, now you can compare 1-2 and three. OK, notice that here I could write W dagger equal Sigma dagger the pseudo inverse of Sigma applied to H. But this is the one we use. This is what I'm saying. If this one is also W, dagger equal Sigma dagger H So now you see that before we do the computation. OK, let me give you the intuition. The idea is when you compare this guy has access to Infinity data, but not only is restricted to linear model, it is also adding a rich penalty. So if you remove the rich penalty, it becomes this. So the difference between this guy and this should decrease with Lambda. OK. As a scientist, before you do computation, you make guesses. The guess is when Lambda becomes 0, this should become this OK. The second observation is OK, what about this guy and this guy? The difference between them is that the only different the fact that this guy is actually finite data, and that's what you. So when you let the number of data points increase, those two quantities should get close. However, you also have the role of Lambda, and you should have that when Lambda is very large, you somewhat don't care about the data, because remember this is just a shifted matrix. So you have a matrix plus a million, an identity. If you make the million big enough, you don't see the matrix. So somehow the difference between these two should be decreasing with N and in increasing with N getting larger. Which be. Decreasing with Lambda getting bigger, OK. When Lambda gets big, you confuse them. When N gets large, you don't confuse them anymore, OK. So if before starting OK, this is what we're going to do, we're going to do. Now you have Lambda minus West, Lambda W dagger equal West, and Lambda minus Lambda plus Lambda minus dagger. OK, makes sense. I just added and subtracted the middle guy and I removed the Sigma, the norm because they're just destruction for you have to add that in a minute. But let's look at this. And I'm saying if we have to guess the bound, we would get that this guy should be something like Lambda or Lambda to the alpha or E to the minus Lambda or something that goes to zero, OK. And this guy should be something like N Lambda. Maybe it's square root of N, maybe square root of Lambda, I don't know. But you know, this should become zero when Lambda 0. This should become zero when N goes to Infinity or when N Lambda gets so large that you're basically ignoring the data. Does it make sense? OK. So I strongly encourage everybody that does an experiment or computation to be for guess what you get OK if you start to somewhat be able to figure it out. I think this is at some point in my life is very physics. The only thing I remember OK, so keep them in mind, but I'll delete them because we need space. We're going to get them OK with the precise powers and precise, constant and precise. Also before somebody gets upset again, this is a random quantity. So at some point you see, actually this is not a random quantity, but this is. So the study of this will require a kind of property. Need to be some. Formal. Lower fashion. The thing that we can do here is that we can use this equation here, OK? So now for the next 3 minutes, we're going to do something which has 0 concept content, and it's just going to be linear out right? And I'm going to teach you a future. OK, ready. Do it. And in the next 10 minutes, I'm going to make 15 mistakes. But when you see them, you will get it. OK, It should be wrong, but not wrong. OK, this is the easy guy. And that's the annoying guy. So this guy we're going to do it in a minute. OK, so how can I write H can write it as Sigma W dagger, correct? So let me handle these guys first. You say W Lambda minus W dagger equals Sigma Lambda -1 Sigma. I factorize W dagger and I get minus identity. OK, I just factorize it out. Then I do something stupid, which is an identity. You can roll it if you have an invertible matrix, call it C or M If it's invertible identity is there is MMM -1. OK, So what we're going to do is that we want to write this guy is the trick as Sigma Lambda -1 times Sigma Lambda. So. Sigma Lambda is invertible because I have the identity, right? So now this is invertible. So I'm going to you see that I have Sigma Lambda here and Sigma Lambda here. I'll factorize it out and I get Sigma Lambda -1 and what do I get inside? I get Sigma minus this guy. This guy doesn't have a -1 and it's just Sigma plus Lambda J So I had Sigma minus Sigma minus Lambda J. This goes away. This goes away. This becomes minus Lambda, Sigma, Lambda -1. OK, that's good. All right. Now we might want to remember that this is not what we want because what we want is really this. We pre applied Sigma to the 1/2. You see this is important. This metric is harder. If you look at this quantity, OK, you see that you can show that it goes to zero with Lambda only with with something that depends on the smallest eigenvalue of this, OK. Because if you take the metric norm of this, this is basically one over Sigma minimum eigenvalue plus Lambda. If the minimum eigenvalue is 0, this is a constant in Lambda, OK Instead, we want something that goes to 0. So again, here I'm going to I'm going to shoot for an analysis that is dimension independent, OK, Luckily I don't have to study this that right? Weaker metrics and weaker metric. OK, what is the weaker metric I have to put in front of everything? Sigma 1/2, remember? So I'm going to do it in the dirtiest way, put Sigma 1/2 everywhere and see what it is. OK, so here I have space that I don't here cheat, here I can give you because I'm lazy, but not that. OK now something nice is happening because again, this matrix is order one over Lambda if you compute it just using spectral calculus. So I'm going to skip this norm estimates, right? The way you do it, you just write this as Max over the eigenvalues you take it and you said it equal to 0, not going to do this ever. So when you have a matrix A, which is like in our case, you just take the expression and in this case we always have explicit expression. So the way we compute it is just we compute the maximum eigenvalue of this kind of stuff. For example in this case that you can example one, it will be Max of one over. Sigma. Plus Lambda I from one, OK. And what you see is that if this can be 0, the maximum of this is going to be 1 / 1, which is whatever makes sense. So we take note of this. I'm never going to show you the proof we do. And I'm just going to shoot out norm estimates because I remember them to derive them. You just have to do this. OK, This is one other trick that you use to norm estimate just between the. So this guy goes as one over laptop. What about this guy? But this guy is not this expression anymore. He gets the square root of Sigma and numerator. OK. If you review the computation of d -, M over Sigma, what you get is that this is actually order one over. So the fact that we pre apply Sigma 1/2 makes the whole thing and the final conclusion is that this term. So the norm of this is equal. To. Lambda Sigma 1/2 Sigma -1 half the only quantity we have to estimate is the one we just discussed, the one we just discussed. Sorry, this is the less, this is Lambda one over Lambda. OK, and then delete this. We're going to use another stupid inequality which is the following. A + b ^2 is less equal than twice a ^2 + 2 B squared as well. Inequality is as a name, but I forgot. OK, but you see this I can speak in the party that that depends on West hat and not that you hat and the one which is West Lambda WW. So now I have this term plus twice the square of this term. Let me just write this. The final result is twice Lambda, which is square root of Lambda that is squared. And then OK, let's recap to see if it pans out. Just review everything. So I only added and subtracted these two terms. Then I focus on the second term. The second term is equal to this quantity. I massage it a bit. I write. I notice that I can use the normal equation. I write the identity as something. I factorize it out. OK, and I get this again. Here I use the expression factorize Sigma, Lambda -1 write the identity as Sigma, Lambda, Sigma -1 simplify get this expression. Then I remember that I feel a Sigma 1/2. I plug it in. OK, I get this norm inequality where the only interesting thing is the behavior of this. It actually gives me a good behavior in Lambda because of the square root, it don't cancel out. I get the this whole thing goes as root of Lambda that with dagger and I can just shove it in. So we guessed that this quantity should be increasing Lambda. It is decreasing Lambda and decreasing Lambda with the power of 1/2 and it depends on the size of the vector for a second. So as you see here up to remembering how you do norm estimates, there is 0 advanced technology just needed the algebra of the second year whichever colleges OK, any questions these proofs are that is Muktangi span as they are in inverse problems to the study of linear systems where you go this stuff technical recognition for statistics where you call it Richard. Action. Always 1960, between 1960 and 1970, OK, all the ideas is what happens. Very simple stuff. OK, good. It's not much more difficult, but maybe a couple of lines more to study the first term. That's what they want to do next. In passing, notice that I use like 4 tricks. OK, you may want to put. On the side I write an identity as you know these and then OK, I use the normal equation. I do the this inequality here. OK, That's kind of the bag of tricks when you do I should learn here the statistical stuff. You know you can change your way out of the combination of these signals. OK, any question some of the interpretation we already did right guessed it guessed for you and some of you nodded the behavior and here we find it also notice that this was a worst case thing because if the smallest eigenvalue of the matrix, OK, finite dimension, you can always do it. You can always say I want to have another estimate where I do and. Explicitly. The smallest eigenvalue of the matrix appearing OK and then you see that some more. Depending on how big is that the role of Lambda will be less. So the fact that you have this behavioral Lambda is because dictionary is simplified by killing the smallest I get, but you could have an antibody. Dependent quantity. It will be a sharpest estimate as they depending on the specific distribution. OK. The modern machinery theory is often what is done to try to make very explicit assumption. When you see somebody assuming that the data are distributed according to a calcium OK, which we're not doing here. This will immediately imply that you can make statements about the smallest eigen value because a Gaussian is ridiculous matrix with a very slow tail. The largest eigen value and the smallest is a very well conditioned matrix here because it's simpler and putting myself so the analysis is simpler. But some of this is more general. OK, And I put myself in a situation where the smallest eigen value. I don't know what it does. OK. Remember this because the moment you start to look at pictures in the last year that will over parameterize this and that the whole basically start with Gaussian and making specific assumption about the smallest not doing it is just because the analysis is simpler and they get time the same time. This will be a trend. Make more assumptions. You can do more, OK. Yes, sorry maybe I'm lost, but are we trying to calculate the LW hat minus LW banger or are we trying to find the bound for it for example? We would love to be able you can check. So the question is, are we estimating exactly finding an equality or inequality and inequality you can actually. Track. Which I didn't do and I don't want to do it because I want to move on where we lose, OK, for example, we lost here, OK, we lost whenever we do the norm estimates. And we lost the certainly when we did a couple of things. Because you see. Suppose that this guy is aligned with the first eigenvector of this matrix. And you? Should be able to do much better than this here I'm actually you know, worst case with respect to alignment of West dot ground 30 we're going to make this observation. But wait a minute. Maybe they should keep them together. OK, so when here you can I think it's a very interesting question you track we never we made it in quality appear OK and they're not many places. So it was the split and I think in this computation it is really only here and then estimated. So we lost twice. So we're going to should have anything else same story as before. I'm going to forget about Sigma One after a while. I'm going to add it. Now I make space for it so that it. Doesn't. W at Lambda minus W Lambda is the one right? So we do Sigma Lambda minus H minus and what do I do? I want to compare this guy for the composition. Of function. So what you do is that to compare these two, guy, you are the mixer, OK? You take a bit of this, you need to take a bit of that. You have two possible choices. Turns out that one of them is a bit more appropriate and it makes that a bit easier. And it's going to be the one where you take this matrix and this vector, OK? So here I add and subtract this matrix and this vector. So empirical matrix, ideal vector, OK. And I'm going to skip a couple of steps. OK, so I take this, which is an incomprehensible thing, but this is 1 and this is a hat. So I take this and I factorize the matrix out. OK, so I get, let's remember this space Sigma hat -1. And then I get H hat minus H then again Sigma hat Lambda -1 H Sigma Lambda without the head Sigma hat. OK, so let me write it. OK so I forgot the -1 but it is no hat. So if I give you 2 matrix, if this there were two numbers what would you do? 1/3 -, 1/5 minimum common denominator. Here you have to do the same for matrices. OK so you have to learn to learn. You have to just figure out a weird way to do the minimum common denominator of matrices. It is one over Sigma Lambda hat. This is one over Sigma. OK, you guess that it's going to be something like that. The order will matter. So what do you do? You remember the trick when you look at identity and you write it as the product of two things, You have to use it twice. OK, so you can prove the following Sigma head Lambda -1 minus Sigma. OK, let me write it here. This is the right. Place. -1 equal. Don't look at it and we derive it. OK, let's do it. So you say, OK, I write this guy right in front everyday density. The way I work, the way I want is Sigma Lambda times Sigma Lambda -1. OK, same trick upgrade. Now I take Sigma Lambda -1 and put it on the other side and what do I get? I get Sigma Lambda H -, 1, Sigma Lambda minus identity, Sigma Lambda -1. What do I do next? When you buy a hammer, use the hammer. Write the identity out and the composition of this guy. OK, so you say, oh, I want to write this guy as and just keep it Sigma hetta Lambda -1 Sigma Lambda. OK, I factorize it out. OK, we are basically done. Why only one step left? This is Sigma plus. Remember the notation Lambda identity and this one is Sigma head plus Lambda. But the Lambda identity part is the same goes away. I get whatever, OK, That's it, OK. Here, here. How many Lambda you have One here and one here. This is plus Lambda. This is minus Lambda. OK, that's exactly the point. When you take away the inverse, the Lambda identity goes away. These are not the inverse anymore. I unraveled them. OK, almost done. OK, we need a couple more step and then probably notice that up to here everything. Is. Just all right, let me rewrite this guy. What I get, OK, What I want to do now is that I want to you see that this expression now in front of everything as a Sigma hat, Lambda -1 let me factorize it down, OK? So what do I get? You get Sigma Lambda hat -1 OK, then I get H hat minus H What do I get? I get plus Sigma minus Sigma hat OK, Then I get Sigma Lambda -1 and then I still have to remember that I had an H here. So I put H OK, OK, because there's nothing interesting here. I just plug my impression. What is this? OK, all right, now I'm going to cheat and I'm going to put back. You see, I didn't put sema went out just because you may change nothing. But now I can do this also means that I could have done. You can always check what is the effect of changing the norm up to here. You just take the norm and see what happens. And I think it's a good exercise to learn this stuff. But here I'm going to put back the SEMA one after. OK, so I'm going to get it Sigma 1/2, Sigma 1/2. Amazing space. I left Sigma. 1/2. OK, that's good. These are computations and there are more. Let's take a break and let's interpret stuff, OK? What the hell? Did I do and why am I doing? OK, perfect, fine. I want to look at the bound, but why is true, but why did it come out there is a very obvious. Which is. I want to compare this vector and this vector OK I have the law of large numbers. Number right? I have an. Empirical sum, for example this one and then I have an expectation of something looks like the same. Thing so if. Somebody invented the law of large numbers for vectors or for matrices. I can presumably hope that this vector, which is the expectation of this vector, will be closed OK when depending how many points you have, the same way this matrix which looks like the empirical version of this. Hopefully they're close because you started something. So. We're going to be optimistic and say. What that we have is. The result that say that Sigma hat and Sigma close and HF and HR close. What have we been doing the whole time? Because we haven't discussed this yet, What did we do? We have to worry about the fact that these are not linear function of Sigma hat and H. They are linear functioning H, but they're not linear functioning H hat or Sigma hat and Sigma hat. So this whole business was unraveling the expression to get rid of the inverse and somewhat get H close to H hat and Sigma close to Sigma hat. So the whole inner algebra was to unravel the nonlinearity to exploit the hidden linearity retroversion, which we did. OK, so you see that here they have the quantity that I think might be close, here the quantity I think might be close. And then actually I have some other stuff. This is part innocuous. I still have one place where things are coupled in an annoying way this term. Here and here I have a head and here I don't have it. OK, so we still have to work on that a bit to try to see if we can massage it to so much, Make it depend on something leaner. OK, does it make sense? Never know, right? This is stuff I like interpreting equation. Depending who you are, this may be very annoying or useful. So. Give it for free as an aside to wake up. One observation, if here I had no head, OK, this would be really nice because basically what we saw before is that it's the kind of stuff that you computed and we can hope that if there is no head, this goes as one over square root of. However, these are not the same guy. So what I'm going to do is some trick massaging this quantity here to make it look more like this. OK, but this trick maybe requires a dedicated. This is a fine trick. OK, might not be one that immediately comes to mind, but it's actually this is Sigma H Lambda to the -1 OK, do this OK, and then I add them, subtract Sigma Lambda. OK, this trick can hit the hand. Doesn't make sense. OK, so. Don't try to figure out what I'm. Doing just check that here, here. What can I do? The Lambda identity with the -, with the +. So if you allow me, I'll easily do this right there. So notice that I delete them now I have this and here's what I want to do. I want to factorize this out. OK, let me remember where we put it. So if I put it. On. The. Right identity. Minus. Sigma minus Sigma. Into parentheses. OK, amazing set of parentheses, Sigma Lambda. When he hits this guy, it goes away. When he hits his identity it stays. And it gives me the stuff about agree. Now code is A, code is B. This is AB -1 which is equal to b -, 1 B -1. So OK, what did I want? Remember, what was the game? I have the Sigma 1/2 here. I would love to not have this hat, but then I could do normal estimates. However, I had a hat. So what do I do? In some sense, I do an expansion of Sigma hat to the -1 around Sigma and I get that it is Sigma lamb to the -1 up to an error that depends on how far they are. Make sense? OK, so this is a random quantity. OK, let's call it Delta. I'm going to skip a step here, which is technical, which is trying to estimate the norm of that stuff, OK, When you try to estimate the norm of that stuff, OK, I mean, whatever is here, you can use what is the geometric series for matrices and it's a pretty standard result. It basically says that you can control the norm of this using this these nodes for any vector. If delta. Is more equal if delta is more equal than one in norm OK, then the norm of identity minus delta to the -1. So let me. Read 1. If that is more than one, you see that this man like a geometric series. You can actually use the fact that you from existing, you expand this in a series, you triangle inequality to estimate the norm and you get this. So you go from series of matrices to series of numbers and you get an expression that you like this is just one minus normal debt to the -1. For this, it's enough that this edge is smaller than one. However, if this is smaller than 1/2, then 1 -, 1/2 gives me two. OK, and this is nice. If I can show that this quantity is smaller than 1/2, then this whole thing is counted as a constant. This is almost. What I want? OK. So let me keep the step where I take the noise, OK? So this expression OK remember this will just see my head and then plug it in. Here OK. I'll do that, but as I do that, I will also couple the terms that I like to keep together. OK, so I have Sigma 1/2 I want to couple with this guy. I'm going to get Sigma 1/2, Sigma 1/2, Sigma Lambda that comes from this expression. OK, then I have this stuff determine the parenthesis. This the other part of this Sigma 1/2 here and have this other part here. OK, this is parenthesis 1 minus delta is equal to this, but I only need an upper bound. OK, so for now I'm going to write down and then I still describe. OK. Sorry, I forgot. The OK. Yes, not yet. OK and actually I haven't done it yet because I'm simply find things and it's slightly problem. So the way you should do it. So there is a mistake what I did, but it there to make things simple. OK, you see that somewhat is because I want to separate and anything part and the probabilistic part. Here I must simplify things that this simple enough that you see where this application. Is. That's, that's, that's what it's saying here. Put one over Lambda. And one of the situation where I just put this, that's OK, always going to, I mean, there's a small mistake, which is the following. Let me first tell you what I'm doing and that is. Why? I'm doing, you know. Voluntarily let me one more step, which is the last step we need, which is stupid. OK here I want to get rid of West Lambda. It's the only quantity that still depend on Lambda in the pathway. I only want to have quantity depend on Lambda interior way. OK W Lambda is equal to Sigma Lambda 3 -, 1 H but H is equal to this. So I can write this as Sigma Lambda -1 Sigma West dagger, OK? Which means that when you have to compute the norm of this, you can write it as the norm of this times, the norm of this. But the norm of this again is a norm estimate like the one we did three times. This one is order one. There is -1 here, there is one here in the result thing that is independent of that. You can make it 1 OK. So. OK, so let me simplify it a little bit further so we're almost done. Nice to read Sigma 1/2 Sigma Lambda -1 number which is 1 minus norm of Sigma Sigma head divided by Lambda. Then I have one and I have H hat minus H norm plus norm of Sigma minus Sigma hat WW. OK, so I took this norm. I used the triangle in a point and I split it, and then I replaced this norm by the estimated file. OK, now we're really done with because you see that this last expression, this one I could kill right away. This is the last one I can also kill because you see this is is again just a deterministic quantity. Here there is a -1, here there is a 1/2. This whole thing gives me -1. Half. I. Think now really everything is either Lambda or H minus HH Sigma minus in my head or OK. The only thing we're left is group copy from somewhere lower slash numbers for vectors and for matrices. OK and plug it in there. So that's why the last thing I do. But but it's done. OK, OK. So. You can think about the state. OK, one. 2. 1 Those are the only thing left on the right side of the board up to now. We'll go on anything. The next step is just OK, yes, we do it in the next. All right, What is the lower flash number? You have a random viable Z OK, let's say the classical cases where this is, let's say that now you have ZI. These are ID0 mean. OK then the law of large number this P says that one over NI from one to NZI in absolute value for any epsilon bigger than 0. The probability that this exceeds if wonder is that -0 there I know that the mean is 0, so this says the rate that is exceed that goes away from zero, which is the natural value you should have when it's large. It got depends on epsilon and the law of the number says this should go to. 0. OK, as includes Infinity for any. However, there are what are called concentration inequalities. How many of you know what they are? This addition requires just the speed of convergence of the law of large numbers. Basically say, and there are many version of it. The more you know about this random variable, the better it is. For example, if you know that this is Gaussian, you know that this day is going to be a Gaussian OK, because you know the the weakest result is that when you only know that it's bad, you add I-800, min and Z more than a constant almost surely OK. Then you know that here this is more than twice E to the minus N epsilon squared divided by C ^2. This inequality is the basic form of constitutional equality with the exponential tail. OK, it's not the law of that, it's not the Gaussian, but it's not either. The probability goes pretty. This was testing inequality. OK, it's not enough for us. OK, it's not enough for us because this is a vector and this is a matrix. So we, you know this, these are going to be our two possible choices of C, either this or CI or this C prime. And that's the I. These are the two choices of C that we want to OK. So for both of them this is not enough. The good news is that there are several extension of these kind of results for vectors and we're going to use the one for vectors that basically says, suppose that Z is actually an inner product, essentially A keeper space. OK, so you can think we're only need the finite dimensional version. So you can think about it. But it turns out the results for vector of any size. Then you have to update the statement because instead of absolute values you have to take noise. Other than that exact statement holds. This is the one you do in it. And notice that you can rewrite this in a standard way. This proof for all of it is nothing with. This is what is called the Tate inequality. You could write as an error bound. If you call this whole distribution tail delta and you invert the relation with respect to epsilon, then you can find that. With. Probability 1 minus delta sum of 1 / n I from 1 to NCI is less or equal than. I make a mistake and we check it. Great lunch. OK, this should be. Square root of. Two over delta log C square. I forgot maybe a 2 here. OK, I forgot the two here. So anyway, you got something like this. OK, So if you call this whole thing delta, you invert it, you get something like this and you see that now it looks like a bound. OK, we said oh, it should go down as one already does, OK. It just also depends on the probability. If you can integrate that out and get the resulting expectation, is it slightly more precise because it takes instead of how fast And you see that the dependence on the probability is very weak. It's just. Which is good news. OK, if some of you might be familiar with Markov or Championship. Inequality. Which are much simpler. You get a much worse dependence on that. That's the only difference. If you're lazy and you just want to do championship, that's fine. Does it make sense all? Right. It's a sign that we have to finish. So what do we do? OK, we do two things here. We just say, oh, fine, this is a vector. OK, what I actually want to do is that I want to resend OK? So I want to define zi as that stuff minus the mean. So that is identical and 0 mean. And then what I have to do is just to compute the size of these vectors OK and just stupid the estimate I can do. If you do this and you try to inequality, you should show that this is smaller than Kappa M1. Remember that Kappa was the size of the vector X&M was the size of the vector Y. OK, this by dominant brutal. Then you just apply this and you get this result that says that for me to 1 minus delta H minus HN is less or equal than 4 Kappa and then squared divided by yen log. 2D. OK, OK, I just used it. OK, this was an easy one because these are vectors and all I have to do is just use it in super. OK, what about this? There's a good news OK, which is matrices. You know that sometimes you think of matrix as a vector. There's actually a precise way to do it, which is instead of the operator norm, use what is called the Frobenius or Hilbert Schmidt norm. Turns out that the Frobenius Hilbert norm is a norm for in a Hilbert space it's defined that the effects are Frobenius norm of the matrix. Unlike operator norm is defined by an inner product which is the Frobenius inner product which is just defined as phrase of north transpose F fact 1. So if we put the Frobenius norm then you can view this as a vector in a space with space space of matrices with Frobenius form. So image is officially a vector in this case of. The good news is also that we see that here we need the operator norm, but the Frobenius norm control the operator norm or any M which is what? Is that OK? Now this means that we can now define as our random variable ZXIXI transpose minus Sigma. You view that not as just any matrix but using the matrix under the Frobenius norm. Then you can apply this result because this is where I need your space. You only have to compute the norm. So this is 0 mean you can kind of check it. It's a ID, you only have to check its Frobenius norm. OK, corrected the Frobenius norm of the eye twice. I have to check this four. OK, It's not 4 is 2, so it doesn't matter too much because now it also results like this for sigmas, OK? So like here we have that Sigma minus Sigma hat minus Sigma you probably 1 minus delta equal 24 Kappa AI think it's 2 here and then to go to delta, OK, and we're done. Because now you see here we only had to control this. This is the same as this. We only have to control this quantity. Now we have it OK, we have the estimates, OK. The only thing I want is that I want them to simultaneously OK, so I want this and this to hold at the same time, which is, you know, a delta half for this guy and a delta half for that guy, OK? So if I want this both to hold any one minus delta half here, 4 OK, if you're lost here lunch time, but this is an obvious OK, then I'll stop here because I've already dated. But maybe we'll just we just keep it there. Nobody delete it. Yeah. So because what we have to do here is that we take that left board, we plug it in here, and we put a condition that says that this has to always be such that it's smaller than one. Then we can go, what was the mistake to write this expression? I have to already know that this is more than one. OK? So what I should really do is I stop a bit earlier. I do that stuff. I put the condition into the smaller so that I can write this expression. OK? I only did it because I want to get to the end and then read the mistake. OK. So to do it a bit nicely is that once you're here and you realize this, you get here, you realize this, you stop, go to that board, you do the probabilistic part, you put the condition and allow to say that this data, which is the extent of this part, is smaller than 1/2. And you keep on going. You write this term, you just keep, OK, so we leave it there. The only thing I want to show you is that once you take that stuff and you shove it in here, you finally get the expression we wanted with Lambda and all that. OK, stop it. Starting over time. Is there any question? We do it for this matrix. We define the random value which is XI XI minus Sigma. This is also again 0 mean. You decide what is the norm you consider because the Frobenius norm. Why? Because in Frobenius norm this becomes the Hebrew space value random variable. We estimate the Frobenius norm. You can actually do it by completing the trace. We leave it and you find out the essentially controlled by the norm of the vector again squared. The 2 comes from the stupid triangle inequalities. Here the square comes from the fact that this guy contains X, OK. And then compute this by computing the trace until these control Vegas. Now all you have to do is to apply this inequality for this random variable and for this random variable, OK. If you do it separately, you get executives 2 over Lambda. If you do it jointly, you have to do a union bound. You have to ask this inequality to hold seamless transfer of them. So what you get is that you get the four here. OK. The second observation is that what we need is a bounding operator not not in Frobenius, but if you have a bound in Frobenius. So if you want the strict application of this would be Frobenius. But Frobenius is always bigger than operator. So. This immediately implies OK. So this is the orderly summary what you said in the morning. At this point we can do this separately, which is OK. Now that I'm here, let me make this assumption that Lambda is smaller than Kappa square, so that the first step and it's bigger than this, OK. Because I remember that they needed this, the normal disk to be smaller than 1/2, OK, So this is the condition. I'm sure that indeed this is the normal more than 1/2. I'm not going to remind you. In that case, we can bound the normal all this by two. OK? So now we put everything together. I have twice all these BS squared. OK, I just want to see what happens. So I have twice this guy is just one estimate and it becomes one over root of Lambda 1 / 1. Then I have this whole thing which is more than two when I squared becomes 4. Then I have this I shove it in this I shove it in here. I shove it in and then I have some squares. OK, I hardly. Have read the constant. But I think this should be worse. If you look at this now, you can say, OK, it depends on constants, which one, this one and this one. And then depends on what the probability which make this statement here is 1 minus delta. We have exponential tail. That's what we call with high probability. It's not an expectation bound. And then it depends on Lambda. And you see that here N is squared. So this bound goes as one. This whole thing is order one over Lambda N OK, so the total bound is Lambda plus one over Lambda N At this point you can optimize Lambda, get the bound, and you see that the overall bound when you optimize Lambda is one of those. Makes sense? Any question, it should be more or less the wrap up of what we said in the morning. OK, I make a few remarks. Why do you think, if you have any one remark how up to this result, which is both results, we start from here and from this group, because there's nothing, just a whole bunch of tricks which are coming from the idea of exploiting observation. This is by the mean sharp inequality, OK or right. This can be optimal under the assumption we made, which are almost no assumption aside from bounded fields. The moment you start to make assumptions, for example, for example, you have something about the structure of the covariance. For example, you have something about the fact that WW is not unrelated to the covariance. You can do more OK, but this is optimal unless you say something in particular. The way we derive it is dimension independent. So if you take everything I said, you said, Oh no, actually I'm doing everything in the Hilbert space. Instead of this W. Transpose X right in a product it holds the same. So it's also worst case in terms of dimension, which again means that if you assume you're infinite dimension, you can get something OK, so someone cheated you because they wrote everything infinite dimension, but everything this worst case OK, because the way your first things, it also only holds for positive regulation. OK, the way we did it does not allow to consider Lambda equal to 0. OK in finite dimension you can definitely do an analysis for Lambda equal to 0. So this is a byproduct of the fact that they are dimensionally independent. OK, in the last few years there are pretty complicated analysis. If you show that, you can still do, but the condition of maximum complicated energy is also. OK. Any question, My last comment then we move on. Very good question is what do you do with this? Are they practically useful? Events. If you study physics and you remember F equal to may and you want to put the numbers and get the number no, there is a lot of industry or try to get bound that are usable. OK, and you can imagine this pure theory is the opposite. This holds as long as your disk if it's cats, dogs, text, the speech, finance or whatever, it holds the same. So this bound at the amount of generality, but it's hard. You know it's going to describe everything. So it very slowly. OK, so in my experience and that's what about this useful bound from a quantitative perspective, I suggest something for example, some tension between the sampling part and the other part. OK, they show that there is some of the trade off between stability to sampling and getting in front. They show that someone is pretty some qualitatively. So I'm happy to discuss the coffee time, but it is just a warning. These bounds deeply are are not they keep inside. They force you to somewhat break the algorithm in pieces and think about it. That's the practical. Part. This bound itself is more of a, you know, of a mean to an end. It's not an ending set. This is the end is computed as pretty. No comments you have for lunch, all right. What you want? To do now is to do the following differences. OK, So what did I do? I replace the east squares by another loss function and I replace the norm by another square norm by another functional. Let's just call the regularizer. For that. The two examples we want to discuss are in fact, I get the square norm and maybe that one norm OK, by the treaty that we discuss both for any the good news is that the mental picture that you might have built stays the same. We're going to introduce the middle guy. We're going to split the air. We're going to deterministic part of the VR. All the details are called OK. It's not true that we're going to have this equation. It is not true that we have this equation. It is not true that we will have this decomposition. Certainly it's not true that we have this. This may be all. Useless. Yes, something is left now before 1. Good news is that on Thursday maybe. I guess when we discuss Colonel methods, we see that the fact that everything we discussed holds for infinite dimension without. And then we're going to take a shot of improving things a bit because if there infinite dimension, a few things are not for granted and certain things you can imagine, OK, for example, if your space is basically yeah, you know, with a certain smoothness, then you can exploit it to give other bounds. OK, So we see that we can basically do the same stuff. We stop a bit earlier and we'll upgrade everything a bit, OK? And it's become, it will become a bit more, you know, it's not much more familiar. It's a bit more technical because we use right the more refined rules. OK. So take a look at what we've done today if you're interested in what happens when you upgrade because we're going to take exactly easy, OK? As you go to other loss functions, it turns out that you don't have those equations anymore. So you're going to follow the different effort rather than working mini masters. You don't have an explicit expression. It's possible sometimes to explore, but in generality, no, there is an approach which is totally oblivious about the shape of the loss. OK. And we're going to discuss that and you'll see that the main character of today is going to be what is called the empirical process. OK, the theory of empirical process. So the setup is the same. Turns out that we don't need this, but you cannot. Then you see why we need to say something about the loss function. We're going to assume the loss function to be convex and continuous in the second argument. In particular, we just need to be. So we assume rule Y&R make a difference. When you take the loss at zero, you have a constant that governs the bigger and when you take the loss any 2 values. Remember that the loss is an argument Y and say double transpose X but they're both numbers. OK, so now we say fix the first guy. Then if you put zero is controlled, fix the first guy. Then for any A and a prime that you can put in the second term, you have a leap sheet property, meaning that the loss function is controlled by the second argument. That's some constant CL which is division. OK. The two primary example of loss function that you might want to have in mind are like, I still want to cross set of your logistic clause, OK. If you think about when you remember the logistic clause looks like this log 1 plus minus a. If you put A equal to 0, OK, you see that immediately. That's bounded OK. And if you compute the first step, you see that faster than one OK. But that's the primary example we want to keep in mind. If you look at the hinge loss works the same, OK. Is that OK? Do you see that the amount, the bound on the Y is not here anymore because it's replaced by the one on the bound. You take the square loss for example. You see that this bound holds as long as Y is limited to be an interval. And so you also say that Y for D squares. This was a fact for general loss function is. The fact that there is a set of minimizer is isn't a it actually turns out. That yes, for anyone. Yeah, if you want the square. Loss. It's. Strong point or not depending on the covariance, but they I want to discuss these two technically. So let's say the square loss, the expected risk of the square loss if I have to throw it if the covariance is invertible. If the covariance is not invertible, it looks like this has a one loss can be or not from the complex, but it's still complex. But it is also coarse. When you go to the boundary, it goes up the logistic. Loss is like this. OK. And so actually when you go that way, you don't go up. That's what it means that it's not coarse, it's where internal minimum doesn't exist. Particularly if you take data which are a distribution which is made of two blocks, so it's linearly separable, you see that the best solution will go to Infinity, OK. However, if you have, there are few situations, for example, if your data are not linearly separable where you can show the thing, OK. So here we're going to assume that we're in one of those situation and we just keep on. I'm only mentioned this because yesterday we're curious and expect up in our working on conditions, but that this is a pretty standard. OK, so just. Bear in mind that it's not. It's not OK. So now I want to play the same game as we did before. We're going to add and subtract. Stuff here is going to be a bit more annoying because we are going to subtract more. Bear with me. Your eyes are going to get crossed in a minute. OK, bear with me because I'm going to add and subtract a lot of stuff. So as I write, keep track of what I've been and what I'm subtracting, OK? OK. This is. Almost inequality, but let's check what I did and where. I lost something and it's getting in OK, so I'm trying to point out equal stuff. This guy is the same as this. This guy is the same as this. This guy is the same as this No, this guy is the same as this OK. This guy is the same as this. This was already there. This was already there losing through that this one, which is also wrong. OK, so I added a bunch of things and then I have one. Why do I do this? What is your Commission? Why would I throw this mess? All right, How much is the second term? So we're going to end up with two terms. OK, so one observation is how much is this? It cannot be bigger than something. This is a functional, OK, And this guy is it's minimizer. This is the same functional somewhere else. Which number is bigger this one or this one with the plus this guy is always bigger, right? This is the minimizer. So this number is 3. This guy cannot is -3 or bigger. So this old thing is going to contribute either negative or being zero. OK, so this term is bigger or smaller or equal than 0. And what do I do the the loss of the real? This do you see the question, this is, you know, the question is the intuition against the mouth. OK, so he's saying well. This. Guy knows all the data, so this should be better than that. Well, but this guy defined to be the best on the data. So, you know, this guy on the data is the best and the training set is the minimizer of empirical risk plus better. So this guy might have seen more data, but he was doing something else. So L is smaller. But the L path for this, of course, of course, the point is that here there is head and head, you have L. It will be something, but how do I compare it? So here the old point is I go for the fact that that's the best and I use it. OK. Right away. OK. The last term, is it stochastic or deterministic? There's no data, there's no head. OK, Is the difference between regularizing or not OK, and we'll see that we can heal it in a minute. Is the bias term, is the approximation error is what we call before West Lambda minus W dagger here is there is no norm, it's just the same thing. OK, thank you. Let me deal with these two guys Second, OK. They want to. Spend a minute. OK, maybe let's get rid of this right away, because this is actually really, really easy. OK. Do you mind me? So let me do the the annoying I have to use this equation. Just look at this. Just look at this expression. OK? What's the value of this expression? It's not the same as the one we just discussed this guy now this is not L hat it's L It's true that this guy is the best of the bestest of the best, but not on the regularized function where this guy is the best. OK, so this number is going to be smaller than the absolute value of this number of the same reason as above. Just recycle. OK, This whole thing is more agreed. Easy enough, same as before, which means that if now I pull this guy on the other side. OK, I found out that my error is never bigger than Lambda R of WW which is kind of nice because if you remember before because the expression of that bias was Lambda normal www.squared. So we find out the exact same thing and now we find it more general. We could have used this proof. Report. OK. Is that OK? So. Now we can remove the views and then just keep in mind that this term is smaller and Lambda are. Is that OK? Good news is the. The read of half of the problem that bad news is the other one. I hate it, it's annoying and while OK, so I want to say something simple first, which is. If. OK, I have to do this, then I can try to maybe write it down a second. OK, and remember a second ago I wrote down the law of just number for real value random value, but I said that desktop is founded then I can just use it, OK? It turns out that this conditional enough here is 0 OK. You put. The vector is that they basically have to show that it's not OK, we're going to do it in a second, OK? So I would argue, and you should agree to my chapter, that if this was my problem, I could use the law of the baby one, OK? The question is, what do I have to do here? I have to control this guy, which looks like that, and I have to control this guy which also looks pretty similar. OK, you can say I want the order is inverted. OK, so let's say if you believe the law of this number, you should be able to write down the probability 1 minus delta. This guy is more or equal then it's constant log 2 / n two over delta, N square root. OK. We probably 1 minus delta, Can we do this? Can we just use this equation there twice? So positive pieces today in general. Since. So the point is, check the hats, check what the hats are doing. OK, this guy doesn't contain the hats, and so all practical purposes is just a determined statement. So you could use this. Why? The key point is that here to use this you need this to be a sum of CI's. How are the CI's centered that kind of centered by hand IIT? OK. Suppose that now I put here instead of West, the West Pact of some sort. What does hat? Means is that effectively when you write W hat. You. Should write West of X1Y1X NYN. That's what it really means, right? It's a random variable that depends on all of them, which means that these ZI are not independently OK, because each ZI depends on this guy, which are ID, but that depends on this one, depends on all of them. It makes sense. So you cannot do, you cannot just go on and do the law of large numbers, OK? And you have to do something. Does it make sense if you're already seeing This is obvious, but this to me is the most important concept of point to just justify the mess we're going to do in a second. OK, All right, all right. Now tricks. I want to show you a factor which is very simple to show. That is the following. If you do Lambda, I want to prove that RW hat Lambda is more or equal than C0 Lambda. Don't spend time thinking about it, because the proof is so easy that you can see why it's true. OK, and in the following you write this. OK, you add the the empirical risk. Why is this true? Because it's positive. Then you evaluate these old thing in 0. Forget the stuff. So we're going to assume that are. 0. 0 OK, see why it's true for that normal that one. Normally I need it here. You're going to want to say this is 0 is the assumption I just made. This is the sum of the loss at 0, but the loss at 0, it's more by C0. So I sum NC 0 in times and divide by north, which means that this whole thing is more than C0. OK, it means that this guy is more than C0. Divide and get OK. OK, prefer we see what it means. Let's just check that I could do the same exact result for this. How would you do it? Lambda is the guy that has access to infinite data. What do I do? I write this, I add not empirical, which has nothing to do with this guy. Expected. OK, because it's positive. Then I check this sum in zero. This guy is still 0. This is no longer a sum, but it's an interval. It's an expectation. There's an interval but probably 21 and got the same stuff. OK, so this means that both. Right here. RW head Lambda and RW are smaller than. 0. Lambda. This is not an assumption, it's a consequence. OK. How we're drawing? Suppose that this is Rd. and suppose that this is the border of radius C0 Lambda OK. What we just showed. Says that with probability 1. I don't know where is West at Lambda. I don't know where is this guy exactly, but I know it cannot be outside of this bowl. OK, same thing for this guy. I don't know where it is, but I know it. I know it cannot be outside as soon as I fix Lambda and the factor restricting the search of the solution within the bowl. Agree. OK, what is that intuition? Because what I'm going to do next, suppose that I give you results strong enough to ensure that the empirical and expected are close anywhere inside the ball. OK, so now I write down. Suit. ORW. C0. Divided by Lambda. Equal. The two things are the same. If I could do this, then it will hold no matter where they are because it holds for all the vector at once. In particular, this would allow me to control both this term and this term. So here the things I want to show you is following. This is obvious. This is smaller than the stuff that you already know is smaller than Lambda RWW. This is more equal than 0. This guy is more than that because it's in there. This guy is also. Smaller than that and so these two terms are certainly smaller than twice RW more. Than. 0 Lambda L head W minus OK, so instead of a law of large number, I get what is called the uniform law of large number over a ball subspace subset. Let's say also you take this guy, you compare it. So this term is more than the sum of the two because this one is this in its minimum is more than this in zero, OK, But in zero, this is zero and this is more than C0. That's what we do. OK. So we basically compare the value of this in its minimum against the values of this in zero. Then we derive that, OK, it's written here RW Lambda and RW are more than C0 divided by Lambda. The sketch of the proof is compare the value of the minimizer. To. Sorry, could you also explain again the second line that you said zero? Yeah. This is the. Why we get rid of it and why we get? Rid of it because this is the minimizer of these two numbers This function is minimized here so let's say let me write this for a second this number has to be bigger than this so when I subtract it, this whole thing cannot be bought it's negative so I do 5 -, 2 is smaller than five that's. It. But again. The contribution is negative. I'm losing something. OK, but they're they're saying the whole equation sure should be more than the left hand side. Right. Should be. Say that this number as an absolute value smaller than the absolute value of this number. So when I subtract them, the difference is smaller or equal than 0. So this whole thing contribute the numbers smaller than 0, Yes? Why? Why is your suit because you want to be in the right? Sorry, sorry, sorry, this is just leftover. I wrote it correctly here. So the point is, I I want to do the lower flash number for W hat Lambda and I want to do it for West Lambda OK, but I don't know where they are because they're random. I checked and I proved that they're always in space again. Let's see what I mean here. What I mean here, maybe it should be. This is the case. So that's called, it's going to be used to define B OK to B, the vector in Rd. such that R of West is smaller than C0 Lambda OK. These are the set of vectors whose R is smaller than C0 Lambda. That's what I meant here. This is more precisely what I meant. The drawing was for the specific case of the square. This is more, this is your B OK. I don't know where they are, but I know they're there because that's what we just reminded. And so now I'm going to replace this term and this term with the soup over this thing. OK. And if you want now you can. Just. OK, BI drop all the stuff because I want I have to write it OK, other questions. So I hope you like it. The reason why I don't like it is because I find the other one which are a bit more you manipulate the object that makes sense individual here. The way I see this is OK. In fact, it's very very and what we want to do are basically tricks. The first thing is simitization, OK, which is a very classical tool in critical process theory. And the second one is estimation of the support before we do that. OK, here you can proceed in different ways as an exercise. If you have never done it, if you've never seen this stuff in your life, I suggest that. 2 throw. Away the motivation and just assume that B rather than B of this form is made of finitely many vectors W OK and the one that the two the three West capital A. Then try to prove that stuff. You see immediately that instead of one law of large number, you need to do as many as the size of that set. You have to do a union bound and so you get another expression which is not as nice as just one single. And somewhat you see that how many functions that will matter. OK, so I've never seen it. I would suggest. To do it. OK, we're going to jump the gun and do it more generally, but the idea is we should pay a price of the fact that we don't do the log number of one guy like infinitely many. So the reason why these are for a matter of complexity is because they're somewhat ways to measure the size of the set. The way it's done is not by standard dimension measures like covering numbers or box counting dimension or entropy numbers, but they use things that became popular, which are for statistical damage. That's what we're going to do. And you see that this is very probabilistic, very little metric ready. The first thing we're going. To do is not too bad, OK? What we're going to do is we're going to say, OK, look, this is an expectation. If you give me what is the ghost sample, meaning another training set that doesn't exist in real life paper, because I'm being a mathematician, I can write this as the expectation of an inferior practice, OK? The claim here is I can write LW if you give me X prime, Y prime. OK, again, it's a training set that I invent for mathematical purposes. I can write this as the expectation. Of. One over north I of 1 NL. Of Yi. Prime W prime plus XI prime OK Y because those are iid. When the expectation enters linearly in the sum, they become all the same stuff, which is LW. I have to sum it up and times divide by north, and so that's trivially true. OK, because of linearity. Does it make sense? Which means that I can rewrite this expression a bit. Perversely, if you wish, I should write W in B, but of course it's lucky with B. Now, probabilists who spend their entire life writing expectations love to do something which is really audible for the rest of the humankind. And I'm going to do the same because I think there is a reason why just a notation becomes lighter, even though it's a bit harder to What do they do whenever they write expectation? They don't write anything here. It's left to you to figure out. Not that this is really annoying because that's an integral over a measure. And I never tell you what is the measure I'm integrating against. And you just have to figure out here and there. That's awful. So what I'm going to do is I'm going to try to point out here and there what is the expectation. OK. Or you maybe you help me out. So I should here the expectation is over this sample. Sorry, this is X1 and and of them. OK, this is just over that. That's an easy one. OK, OK, let's trick one. I want to write that guy this way. OK, is enough. The second thing is that you might want to notice that I can write this expectation, OK? I can write this expectation in front of everything because this guy don't depend on those. OK, So the expectation of a constant. Is the same. OK, so let me let me do it without, OK and move their expectation to everything because this guy doesn't depend on them. That makes sense. So the distribution they give us is the same, but this guy are independent to this these X prime, what prime are a batch. These are another batch. They have nothing to do with each other. The expectation is over XY. Again, this integral is over this guy. OK, so here, here we have N random variables. Here we have 2 N random variables. And if we wish for now, we're conditioning over this. These are fixed. Is that OK? Then we want to use two tricks. I want to take this expectation and move it in front of the soup to do. Is that to do tricks. One is the observation that you know, if you take this, I guess this is always more OK, you can move the absolute value inside the integral and it's bit that's easy. The other thing you can do is that you can show that expectation is a bit more unknown to write down. If Z is a random variable in some space and F let's say goes from BC in R OK, so it's a factor that depends on the domain on the random variable. So what I do expectation of S of? 6 W you. Do Z. So this is an expectation over Z of this stuff here and you do this over. F WZ integrate over Z, which is the only random quantity, takes the suit over WD OK. This, by definition is bigger than. Let's remember this works. The famous rule against you will prove it in the wrong side. So F of WZ almost surely is smaller than the suit. OK. Then you take expectation of Brazil on both sides. OK, Now you take the soup on both sides, but not both sides. Depend on West only the left hand side OK. So. I don't write the soup here because that thing doesn't depend on West. So what we prove is that it's one way to prove it. I guess you could also prove so. This shows that the expectation I have here after the soup I can move it first, I can move it outside the absolute value and then we can move it in front of this. So. This becomes. This is now an inequality become expectation over post sample 1 / 10 sum. I want and notice that here I'm going to write this stuff under 1 sum because I can do it. OK, that makes sense. OK, the expectation of set of the parenthesis so far this petition over Yeah, XI prime, Yi prime and times. So this is row N, OK, on this OK. Now for now, I was assuming that just to be given these data sets, what we're going to do is not analyzing high probability because it's slightly what? Because it's not the interesting part. We're going to analyze an expectation, OK? This is, again, remember that we got this expectation because we did some black magic, but this is just the number, OK? The random quantity is this one. This depends not on X prime, Y prime, but on XY, OK? So here, if you allow me to be annoying, I'm going to write here an expectation, OK? And I write it here too. And I'm going to write it here too. And here we don't because they played the probabilities 3 which is expectation of. Everything. OK, but so this expectation is over what the training state? The real one, the one I get, the one that is ahead? OK, so remember that here I have 10 random variable which are different. OK. Is the same here? No here already I wrote this. Here is the same OK because I just read this one integral here. I now created another N random variables because I can. OK so now we have 2 expectations This one is over these random variables and this one is over these random variables. OK then I do black magic to put it out and then it is expected. This expectation is at the same time is again is an expectation over 2 N random variables the XY and X prime Y. This make sense? OK now the black magic comes now. OK, what is the black magic? The following observation you need some symmetry, OK. If you. Call this quantity CI. So CI or let's call it CIW. CIW is a random variable OK? It maps the probability space into numbers. How it takes M random variable in that case it takes a random variable. It takes a copy of the random variable and it makes the difference OK. What is the mean of the random variable? Zero. OK. And if I take it? It's. Minus. Do we get something different? So what that is that there is some symmetry because if I do this, OK, it's not going to be equality in terms of numbers, but it's in equality in terms of what is sometimes called. Distribution. What does it mean? The law of the random variable is CI for annual West is the same as the law of minus ZI. Why? This is another way to say that this thing is symmetric. OK, after all, this guy and this guy are the same are just different ones. OK if I do XI, if it's a name, if I put it XY and X prime Y prime or I call it X prime Y prime XY, it doesn't matter. OK here you know I'm going to do this stuff the way you see it written up, which is I I raise my voice when I see the turn out. What you should do is to sit down and prove this, the distribution of this, but it's there's nothing interesting there, so let's just keep it there. Intuitively makes sense and you cannot make sense. And now we want to make a whole bunch of implications, OK, of this. The first one is that if you not take. A random. Variable Sigma OK which only has two values plus one or -1 and we actually take this value. We need help Probability. OK, does it make sense? So this is this is a but with values plus one and -1 instead of 01. Then it's a 0 mean random value that is symmetric by construction. OK, what does he do if you're not taking instead of this guy? If you shove in here, instead of taking the minus, you shove your Sigma I. Again, you should prove this. But it really is still true, right? Because what I'm doing is that you take the number and it's random. OK, You flip the sign, but you do it with probability. One up, one up. Either you flip it or not. So the distribution doesn't really change. OK, Again, we're taking these as an intuitive fact. Does it make some sense? Yes. Oh, it's just, it's a. It's a. It's called the random marker or random marker if you wish, and it's just the Bernoulli with values plus -1 instead of 0 as it usually is. OK, the next step I want to do is to say, OK, this is true for any fixed W, OK, and for anything ZI. But if you now take this and sum them up, it's still going to be true, right? This is not sometimes it looks like this is not an Omega, it's a WM because this random variable depend on West. So I do this. This is equal to this OK, do the absolute value. This is still true. OK, I take the soup. It's also still true. At the moment there are two random variables whose low is the same. The expected value is the same. OK, careful with expected value. Now it's slightly different things. OK, the first expected value depend on the stochasticity in CI which are the same random variable. This one depends on the on both. OK, so I'm going to write the following. Maybe let me just do this? Here. OK, so instead of saying that they have the same law, I said that they have the same expected value because they have the same law. Is that OK? Distribution for. These are in this, so again in our setting we are assuming, so they are exactly this form. So they are inherited from to any independent random by. What is that here actually holds for any set of any independent random by for a family of any independent function parameterized over West and these are to be IID. You know what I mean? OK, we're good. OK, so again, let me write here that CI Omega Oh, sorry, WI want it to be L of Yi XI minus LY prime XI. Because now what I want to do is to tell you that if I use the result of the right, I can simply add in front of everything Sigma Phi. And now the expectation is over XYX prime, Y prime and the choice of Sigma Phi want to write it? And if you want to write it, you should write the data set D, the data set D prime and then a vector of random Sigma is the expectation of these three things. OK, OK, so again, what do we do? So. Far. We wrote this ghost simple trick and then we really start to use this symmetrization tree, whereas somewhat we notice that there is some symmetry in this stochastic process, OK, and we use it to stop the right. The last step is just to just say, OK, what I can do is that I can take this term, this term and use a triangle inequality, OK and write twice a certain quantity. Write this quantity right 2 and expectation soup of vectors in P absolute one over north some I-1 Sigma IL YIW transpose XI. So again, I just simply take the random inequality. I split again the sum if you wish and get twice the same. OK, so again, you think you're driving any particular intuition, but you put it there should not so completed fact that we're annoying meter. What's going on here? Why do I do this? In the end it works. OK. The basic equation that I wanted, I want to check how many times these two things are different because basically you tell me that they're close. I still don't have a fantastic intuition so one of the complexity control, but I think it's kind of fine. Just this old business. You will see the minute that this is an interesting point. It's more like. How you think about it? Why someone? I mean, it's really the symmetry, but. I don't. Think this part is interesting because I'm sharing my confusion. It's more like this thing. It's a technical fact. We put theoretical foundation in the title not to be bothered too much. OK, this is the freaking math. And in the end we're going to get the result that makes sense. OK, so again, the step here is put the sample, OK, a whole bunch of convexity treat, put the expectation out and this symmetry stuff to introduce these randomized. Suppose that you have a sequence ETA one is the end of variable which are fixed. OK, they leave in some space which could be anything, I don't care. OK any of the space of functions of maps go from the domain of this Z to R and equal it H OK, what do you define as the random market complexity? Or if you want, let's say there are the market complexity of the space, H is the following thing. You go with the Sigma I, you go take HH over the CI. You do this flipping trick, you compute the exact quantity, then you take the Sup over H, OK? This is what is called the empirical red market complexity, or the market complexity on the sequence C1 CN, OK? It's a name. If you look at this, you see that it's not very far from what we have, OK. What we have is that we actually have a class H that depends on what at the same time on L and on that, OK, So this is not the best name because usually H, we put the space, but now it's too late. So I keep on going. It's not the space, it's just space. And for us, this space is governed by. So for example, let's call it G OK, actually thought about it, yet they want to write in the noise. So sorry, it's not good, just good G It's just this. OK, then this case you want to consider is the GK, which is function of the. So I'm going to take as C, OK? I'm going to take of Y so that when you write CI and my G let me write of CI is going to be L of YIW transpose XI. That's example. If you allow me to write that, then this is exactly the expectation of the rather market of the class G defining the example. OK, yes, all function from C2R or in general subset of functions from C2R here is anything OK? Here is the set of function from Z2R. So it's not the whole function, the set OK, it's a subset of functions. And here it is the specific instance they want, the one that are OK by concatenating the dinner function and. This complexity measure like looks like how these functions scatter out the data OK. That's your question. Want to understand this? So I would say the following, OK, there is a point I would take discussion about the intuition about this offline, OK, I'm happy to try to finish this. So that you see that this actually would depend on, for example, how big is the ball in the R topology. You will see in a minute that they're going to abuse that expression, kill it and get it actually depends on the size of the ball, OK. And then again, if you go the other way around, which is usually using the covering numbers, OK, then you see that some situation, this is much harder to. So again, I think you're there. I give you the pragmatic view and I would say we do that because that's easy to compute, OK, Because it's going to disappear soon and we're going to get something that we really understand, OK, But another answer is like, OK, what is the meaning? And then it's something about something like. Again, you randomize and check with that, but I don't know another answer. The. Same. Because whenever you see Sigma, it's just OK. So again, let's say that random capacity are useful because they're going to disappear, OK, OK. And they're going to disappear in the basis of one basic fact, which is the following take now let's call these for now, G is going to be this guy. And there is a fundamental result that says the following OK, there are the market complexity of the class GG. Now is in this example OK is lesser equal than the Lipschitz constant of L times times the complexity of the class. Remember that here we want to take W OK, So for us we don't take W anywhere. We take W within that subset C0 over Lambda. So remember that this big B is just that set. OK, so now what I'm telling you is luckily we can relate the this quantity on what happens not from L blah, blah, blah, but just what happens here. You see that now I get rid of L Need a minute? That's where RW is going to be here. OK, so. I. Delete everything, but let me keep it for a second so that we can do of the suit of M of West minus. M. Of West equals them. 2. It comes from the triangle inequality CL. It comes from this thing which is called the contraction principle. Is it application of the construction and here we have expectation of? There. Are many expectations here in this world? OK, see once again we're fixed. So this expectation is over what? The only thing can be clear, which is the Sigma. But there is an expectation of a which is the one over Sigma. OK, here the expectation is over. Also the CI. OK, so now ZI instead of just the set of vectors become instances of random viable. Then maybe it was comfortable. OK, now this expectation contains everything, so it's going to contain both the of these I'm. Sorry, rather be. That delete is OK, so we repeat it a bunch of times so it's clear. I only have to. The abuse here is just that as G. Now again, this is general. They only have to take as G the function induced by linear function. So with a bit of an abusive notation, sort of like the FW induced by WI. Write that one directly. So I get the sum I-1 N Sigma I W transpose XI. OK, so there is no Y anymore because I kill it through the of the loss. OK, that's where I use the assumption. And here my G again. There's a bit of an abuse of notation. Where is there writing all the function used by the inner function address right directly the vectors because the relation is 1 to 1. Where does the rod P less than CL rod P come from? By the result by Michelle Telegram Michelle do and. Also that is the. Construction or if you want them or precisely this is an implication of the construction. The construction lemma is a slightly more general version. Basically said that you have two function class, you compose them and the outer one is deep sheets. Then there are complexity of the upper class of the whole thing for by the 1, yes, So the expectation of. Sigma. Again, the way I wrote it, this one here is if you want. This. Is condition over X1 XN OK? Then this one is only over X1 XN OK? Or if you want, yes, that's the way it should be. This one is over X1 XN and this one is the expectation over CI condition on X1 XN. OK. So in the definition of, the definition of the complexity is not. Over. The CI. And there's a bit of confusion. You always use it. OK, So sometimes, OK, this is called, this whole thing is called, this expectation is called. Here I'm using the the, the matter. There are two different notions. OK, so here today we're calling for the mark of complexity, something where the ones then are fixed deterministic quantity and the expectation over the random variables. So that what we obtain here is the expectation over the XI. And then there are the OK, but it's mostly a matter of names, then the name is changed. So I think the important thing here is to realize that there are two expectations. And then there is this super sum of Sigma I W transpose XI, OK? That's what matters, because what we want to do is not to compute it. So that's right, this is. Equal. To expectation of. OK, now I wrote it so that we have to ask anymore because this contains everything. Which is random OK. Now this is what you do. OK, first of all you realize that you can actually this is the inner product of XIW transpose of I times a whole bunch of stuff. So I can rewrite this one as W transpose times what? What you call accept divided by north and then let me collect all the sigmas into a vector, OK. And the absolute value of this, this is the inner product of vectors. I can use for Schwartz and can improve the fact that this inner product divided by the normal of this and the normal but the normal of this. So being in B. Sorry. Right, this is true and you don't make any assumptions here. You stop here and now I jump. What I want to tell you about is here you have to say something about R, OK? If you don't say anything about R, you cannot say anything about what's going on to this part, OK? What I want, if R is a norm, you can use basically instead of Cauchy sparks property of norm. And OK if R is the square norm is particularly simple because you can use Cauchy. What I want to do now is specialize in Discussion 2 RW. Equal. Norm West squared and remember that now belonging to B. Means. That W is always squared is always more than C0 over Lambda, which also means that W is always smaller. Agree. Now I can go, OK, now that I say something about RW that makes sense what it does, I can take this, use squishy shorts, OK, And I can get rid of the soup because what I'm going to get is. L and then I get the square root of C0L. OK, so this and then what I'm left is expectation of the vector X transpose L Sigma. What? I want to do is I want. To take this guy inside and do it by linearity, OK, And in this case, it's nice to take it outside and then this is just the name of. This stuff here. OK, sorry next 3 minutes I'm going to be fast and so OK, we're almost done so bear with me one second even though I'm. So here what you do is the following thing. You do one last trick just for the sink of it. The right is vector at the square square root. OK, you use Jansen and you move the square root of the side. OK, you can call it of the square root. To pull it up. Then you look at this, OK and you see what it is. OK, this is AB square and you can check and you can always write this in a very specific form which is trace of a transpose. ABA transpose ABB transpose is yet another one of those strict. This thing is equal to this. If you now use this trick, it means that now I can write equal to CLN square root C0 plus that square root. Then I have trace of X X transpose because for me a is X transpose and then I have Sigma hat Sigma hat transpose. And I forgot that there is an expectation here but I forgot it because I actually want to move it inside and the trace is linear so I can do it. OK, so let me do it. So I get expectation of Sigma X, Sigma X and here we use a thing that we haven't used yet, which is the covariance of a set of Bernoulli variables. Is there OK? So it turns out that this is just OK. So what I'm left with is this guy, the phrase of and what is this is that they make use of inner product of everybody against everybody. OK, now because it's late, bear with me. This one you can control with the Kappa square times north because you basically have you know you can put the trace of that is just holding the product. On the diagonal. So these are basically N times this. This is less or equal than N times XN times Kappa squared. But this is inside of the square root. OK At the end of the day you get twice LM 0 Lambda. You got a square root and the Kappa. And finally, finally got rid of the radical complexity and you see that what we got depends on the radius of the pole. In this case, it was really bold. And if it's constant because of the contraction principle and then N because of this magic fact. So this shows that the expectation of this guy. Goes to zero. We. Didn't really use the law of flash number, and you see that somewhat magically this symmetrization argument was showing that that thing which is somewhat you know. Is. All B is ends up being a random quantity, actually scales as one over north. OK, so again, this is the black magic part that I don't totally understand, but the final outcome is clear. We managed to get something everything depend on the radius of the ball and goes down and one was good. OK, give me two more minutes and then I'll shut up. One my plan in another world is to another 15 minutes to show you how once you're here if now instead of L. Two, you use. One and then you have this result. Then basically everything holds the same. OK here instead of question. Parts you use. Further inequality you get read immediately of you know this term, but you have to somewhat. Instead of an N2 bound here you really do something like an Infinity bound. OK, but then it can be more complicated because you want that the trace and again, you have to get a slightly more complicated result, which is the maximum of sub Gaussian. So let's say if you the pain gap, you can spend a few more minutes just to see that you know, still it would be a simple argument. We do cushy spars Yangsen race, you can do holder and then that you have to here consider the supremum of sub Gaussian random variables. And then you show that instead of this get a slightly different expression. It really depends on again, the anything you bound on the X OK, I don't think that's too much to learn there from. It is very specific to that some case, but so the machinery is the same, OK. And you see that here we didn't use in that improving the bound on the first piece. We didn't use the fact that it was as 4 as one, OK, So to some extent it's all the same, but it's very just one comment. This is the stupidest bump you can get. OK is the simplest 1 and pretty annoying. OK, there is a whole bunch of literature that says this is not the right quantity and this is not the right space because we know that we look at an algorithm, It's not trying to make things small. For example, I know that I want to try to make this small. OK, so there is a whole literature. That goes into. The localization approach that tries to replace the degree of approach with something more sophisticated distribution effect and make more assumptions, but then you get much better, OK. And so this bound should not be taken as the best or the sharpest as the beginning. So in that sense, I do think still that even though this bound of rules to me is like, you know, you study the derivative before you study the grade, then you study, you know, this is old classic stuff, OK, but we're teaching it an hour and a half and it's a bit compressed. So this is a good place to start in the sense that this is just the we're not going to do that because I don't understand, it's complicated. We're going to do the. Upgrade. We're going to use. This to start in neural networks and we're going to upgrade this morning to start.