We see that they have different different structures. So let's start again with the motivation. We have seen yesterday that for certain algorithm that we choose depends on the choice of the loss and we have identified the two losses which are OK for gradient set, the graphical graphic square loss and the logistics one. For these two loss functions either regularized you can use. There is another loss which is very often used, which is not differentiable and does not work and is is lost. I don't know if you had seen it yesterday morning. No, no, right. So it's like this here. There is one the so the the expression like this and then. The quantity part of 1 -, 3 so these line here as slow to -1 and then for one is 0 and then how do you apply to your data? So we have. If you are in supervised learning context, you have a bunch of data inside Yi and we consider this is most often used in the classification setting. So Y is either 1 or -1 and then what you use. So you compute your loss. I sorry for the abuse of notation. You compute your your one-dimensional loss on the on the product YIF of XI. OK, that makes sense. So you have seen that for classification these are the support margin based losses and the product is a good indication if I'm predicting in the correct way or not. In the special case of linear functions where we that with the just the vector and this is the linear function of XI, this becomes empirical error becomes let's write this, I can write this as Yi times W XI and I can come to the Yi inside. So at the end my empirical error will be the composition of this function which is convex with a linear operation that is this one sending the West in this color product with the Y. So this is convex. I put in this. We have seen yesterday, each summon is convex, the sum is convex and then I have a positive coefficient. So this is convex. It's convex but non smooth in the sense that every time that this point is 1 the function is not differential and then I cannot apply the gradient descent algorithm that we are seeing yesterday. So this is will be our first motivation and we will see for this. So this function so-called the subgradient set and then there is another settings that no smoothness up here is the one that we mentioned yesterday last regularization. Problem. So you put the end in front or not. But I let you skip this because it's otherwise I will forget it later where you are a least squares term empirical the data here and instead a regularization would be L1 norm. Again the L1 norm is a non differential function. It has many, it's not differential in the origin. And then you have all the There are other places where this function is not different. So the difference between the two in the fact that here you can see that your function as a structure, the sum of structure where you have the sum of one term which is elism and convex. You have seen yesterday it is good for the center this for loss plus one term which is many fracture. So in this case you would like to treat differently these two components and to split their contribution. So for this kind of problems, what are called the proximal algorithm, proximal gradient algorithms. So with these two motivations in mind, we will start our path today. In order to do that, the first thing you need to do is to define a proper subdate of the gradient. And this is the sub fraction. So it's a. It's a generalization of the derivative for convex function. In principle I could define a sub threshold for every function, but but as I was saying, this makes sense for convex functions in the sense that if the function is non convex is all very often empty. And so in order to what I've seen already prepared for later I want to introduce a class which is the class that is used in most new optimization in the convex scenario. So F as it was yesterday is from Rd. to R union class Infinity. This must be convex. And then I got the property, a property which is in a sense of the interesting, the fact that the function is proper. With the proper I mean that the function as at least one point in which is finite. So the domain that we define as the set where is less than is not. So this is proper. And then there is another request that works well with the minimization. So I would like to ask that function is continuous, but I cannot because they are and asking for continuity would be too much. On the other hand, I'm only interested in the existence of minimizers, I don't care about maximizers. So what it's not to ask is lower semi continuity here. This means that I will not be the possible definitions, but this means that this one is too much. That's continuous but not continuous. So this function here is the value here is 0. So this function is lower semi continuous. What does it mean? So we can understand if this property is true geometrically by asking that the set that is above the graph which is called graph is closed, right? So this means usually I will write it like this LSC right? So this is a subset, this pair belongs to RB times R And let's say that my function is lower, so my continuous if this set is close. So for instance this function is lower so my continuous. If I draw another one where I change the value in zero, I destroy it, destroy it over so my this function. It's not simple because here these points are in the closure of the epigraph but are not. So I need this set to be close. OK, so this is the this is the class of functions as a name. I don't know where the name came come from. So these are complex, proper and. And this is a good class where you can do no smooth optimization. I. Guess why on the left hand side is closed and on the right side? Yeah, because here there is a 0. So this outline is included in the epigraph because the epigraph is the set of pairs XT for which T is bigger than F of X. So here F of XF of 0 is 0 and then every T0T is in the sub differential. Sorry, here we are. Here I have that 0T belongs to every F for every T OK, here is that no, I have to start from 1:00. So all these points are not decided. Yeah. Like. Continuity, sorry continuity on the interior of the domain. If you are on a finite dimensional space and this function is takes justice interior of the domain because the function is upper bounded. Otherwise, in order to have continuity you have to ask that your function is real valued that does not take the the value plus Infinity. In that case all the points are in the interior of the domain and you can continue. This is just to pick the case where you have a constraint and so you can improve this inside your treatment. So this is the the class of function that we consider. And for this class they say the majority of we will not see many of them, but the majority of theorems regarding some differential work in this for this class of functions. OK, so let's define this sub differential. So what is the sub differential of the function F at the point W is a set and this set is the empty set if W does not belong to the domain and instead. Otherwise it's defined like this. So let's have a look at this definition a bit more carefully. That is the absolute value. The absolute value is the convex function. And then I want to see what is what kind of factors these numbers are in the sub differential vector at the given point. So what I should what I should look at is these quantity here on the right hand side. As you can see, this is if I fix West, this is in the real case, it's just a line that passes from the point WF of West. So I'm looking to lines passing at the given WF of West and I'm looking for slopes such that this line is always below the graph of my function F. So here I have not many possibilities. The only thing that I can do is this one. So the only line, the only slope which gives me a line which is always below the graph is 1. So here in this point some differential delta W is just one, right? If I look on that this side as well. And here I have the same, the sub differential is a line plus one line, one slope. And then let's find and look at the interesting point, which is the point of non differentiability theology. What happens at the origin? I have to look at lines that passes pass through this red point and always stay here of the graph. And here I have many possibilities I can make this one, this one, this one, this one. You see that there are many lines that are always below the graph. So in this case the and 0 is an interval and look at the possibilities that we have. We can start from here and so we can start from -1 and we can arrive to 1. Here the substantial is an interval and the second values between -1 and one. So this give us this is the idea of substantial on the realign that are defined in the realign. If we are instead in more dimensions, then we have this one will become an hyperplane and we are looking again hyperplanes that are always below the graph of A right then. Yes, so there are two two main results that we need to to we need to prove in order to already assess if this definition is a good definition or it is not is not. So first I forgot to say but this definition has been due to moral and most. Many, many ideas that are in the context of smooth comments optimization are due tomorrow, which was a French mathematician and worked on these especially in the 60s and 70s. And then really this stuff has been popularized and made, I would say more algorithmic by Terry Okefeller. So these are the two names that are attached. This is sub differential. This is usually called the Rockefeller sub differential. If you speak to French optimizer, they will call it and then and the other name that maybe should be mentioned is. OK, so these are the names. So and now we want to see if this is a really a generalization of the gradient or not. And it is in the sense that if the function is differential at the point then in the interior of the domain then some differential coincide with the gradient. The facts that's in the interior of the domain then. So this tell us that indeed this was not the case and if the function is differentiable, coincides with the gradient. So it's a Singleton and coincides with the gradient the second. The second factor which is relevant for our for our interest is the fact that here I didn't put the line but here in 0. Here at a minimizer and for convex function we have seen that convex. I told you that convex functions are nice since minimizers, global minimizers coincide with critical points. So if we have a global minimize, we have a global minimizer. The zero is the gradient is 0 and if the gradient is 0 we are in the global minimizer. This thing is true. Here I can draw this line. So 0 is a global minimizer. The zero is in substantial, and these are two equivalent definitions. This is almost a topology. If you write the definition of substantial, you will see this is not the really a big result. But tell us for sure. Tell us that looking at this kind of inclusions is the right thing to do if you're looking for global minimizers in the. So the next thing I'd like to show you is some kind of. So it's a bit tedious, but it's really useful in the sense that we like the liberties. The liberties are easy because we have a whole body of the liberty gradient calculus calculus. We have rules to that are applied in order to take the derivative of the complex functions, position sums, multiplications. Here we can prove see that stuff for the suppression and basically they everything works. Let's say that maybe they take home messages, everything works. There are some cases that are a bit pathological but may happen in some applications where one should be a bit more careful. And this is due to the fact that the function can take the value plus Infinity, so the intersection between the domains. Sometimes it's not enough if the intersection between the domain is not the reach sufficiently reach. It can happen that the usual rules, some rule chain rule not apply. So let's see what happens. So let's call it theorem. So we need the F of 0 and then we assume. We don't assume anything. Then what we have is this one. For any WERD we have that the sum differential of the sum contains the sum of the sub differentials. So this is how we compute the sub threshold of the sum, compute the sub threshold of that and then the sub threshold G and then taking the the sum. But sum every possible vectors here if the sum is that we always included in the sum not that well the other inclusion not that was equality. I need to assume, as I was telling you before, that the intersection between the domain of F and domain of G is A sufficiently and this can be written in this way so. Then we have the quality. So please that the quality always holds one of the two functions that the domain, right? So if we don't have the value plus Infinity, one of the two functions always take real values, the equality and the second sum rule. Sorry. We take these functions in the. We take them to be convex because we need to grant that these sets are not empty. Yes, I don't know what happens in the non convex setting, but basically for non convex functions is very difficult to find sub differentials if you have a known. Yeah, it was so. If you have a known convex region here, the sub financial will be empty so that it's very. It's really a concept that applies well for convex functions. There are other notions of sub differentials for functions that are non convex. They are locally sheets. You can define the class for instance, but this kind of so I don't know what would be the. I don't know the analogues for other substantial. So this is the right class to apply this kind of. This definition is tailored for convex functions. Yeah, sorry, what is the set of functions that are convex and? Other questions Yes. No, no, no, no. The nice thing is that if you have no net intersection, then the quality calls everywhere. OK, it's true for every W Yeah, it should be just. For one of the domain or. Yes, just for one of the two. So here I chose that. But of course you can reverse the yes. OK. The second that I want to prove is the chain rule, because we use it a lot when you can imagine when you have the empirical risk. We need just one function. We need a linear operator from Rd. to RN. Then we need a convex function F which is defined on RN. We want to understand how this what is the substantial of F composed with the operator. And as before, we have that if you apply the chain rule as you could do it for gradients, get the X transpose evaluated at XW and this sector is always included in the sub of the composition. And then we need a condition in order to in order to have a quality and the condition is similar to this one. So we need a point like that which is in the interior of the domain of F. So the intersection with the range of F is not empty. So there is a W such that XW is in the interior of the domain of F then it will be also. So these kind of conditions, this one and this one very often are called qualification conditions. Find that very often required when when you need to solve some kind of constrained optimization problem, you need to assume some kind of compatibility between the function that you want to minimize the constraints that and so this is what they are for the qualification. The last, the last two information I want to give you about the computation of substerentials R, how do you how to compute the substerential for a function which is defined on RF? This is particularly easy. We have seen one because you can see the function and two because it's really easy and there is a general route computed. And the second one is instead of the computation of the substerential over the specific functions, the color of a specific sound structure and they call them separable, you will see in a moment. So when we have a function that is defined on other. So you see the convex function. How can you compute the substantial of this convex function? So it's is fine. So what you can show is that the substantial at the point W in the domain is always the left derivative is almost in interval and the interval is this one. So you have to take the here the left derivative. Then you have to take the right derivative and the the substantial is the interval of close that are in the left and the right one. If this one of the two is plus, this one is plus, you cannot put it into the and the same it's minus. Then as you can see the substantial is always closed interval. So it's always closed and convex. And this is a property that is not only true for the for a function that are defined on the. So this is the computation of 1B function. And yet the second thing which is a bit related is the fact that you can easily compute the some financial functions that are like this. Here I need the. So I assume that my function can be written as the sum of, in this case 1D functions, right? This is what they call a separable sum, because each function depends only one-on-one variable. And if you think the gradient, this would be a very easy case when you have the gradient because you have to take the derivative independently with respect to the different values, the different variables of the different functions. And this is true even in this case. So the substantial FW is substantial of, 1F dot one times substantial of, two times substantial of. Is the cartesian product of the individual subtractions. Why I need this example? Because I want to take the sub differential of the L1 normal which is a special separate function. So the N1 normal can be written as the sum of the absolute values. So this is a function which is the final area, which is the absolute value we have seen before. So you know exactly how to compute the substantial and therefore substantial of the one normal. Let's take the component I. If I take this some differential, I compute it in West and I take the I component, it will be the some differential of that Y and therefore this will be -1 if that Y is less than 0 + 1 if that Y is bigger than 0, and the interval -1 one if that Y is 0. I want to be let's make an example just to see if we so this the sub one norm at one 0 -, 2 is 1 times -1 one times -1 I forgot to tell you that every element in this sub financial as a name, which is sub gradient. OK? So for instance, this implies that I know one one third and this one is the sub page. So I'll be sub page with these two components and this one OK. Let's go now we are ready to introduce our first algorithm of today, which is the subgradient descent. Yesterday forgot to tell you the very important thing, the fact that when you do gradient descent, you have a descent algorithm. So at each step you improve the value of decrease the value of your objective function. In the machinery community, sub gradient algorithm is usually called sub gradient descent. But we have to be careful because it's not a descent algorithm. OK, so I will push this one because it's not the limit. So that's right. I take W that's first date and then. Be careful here I take F which takes only the values not plus Infinity. Here I take the W 0 in AV and then I define my iterate which is this one. So what I do is that I tried to mean what we need for gradient is that I don't have a gradient but I have a of the gradient, I have a sub gradient and what I put instead of the gradient into the sub gradient and two things again we notice W 0. You can choose it as you like and here again GK you can choose it as you like. And this is a nice feature because computing the full sub gradient, the full subtraction could be difficult to be computationally expensive. And this is more difficult that then finding one sub single sub gradient. OK, so this is this can be whatever you want. And now the point is that this algorithm does not work and it is not the descent. It says that these two things. Let's start from the second F of W 1 W 2 equal to. I don't know, so let's do the. I want to consider a function which is like this. These are the level sets. So here there is W 1, here there is W 2 so. And you see this is a kind of L1 norm that is a bit. The weight of the two components is not the same. So it should be something like this. We want to understand what happens when we are here. You are here. This means that W 1 is 0, W one is 0. Let's assume that this is the level set level 1. Sorry, I am on the W1 axis, so W2 is 0 and W1 if I am on the 11 set one to be 1. And so I can compute this sub differential completely because this is again separable sum. So it's very easy one times minus right? And then here I have that if I look here and if I look at the direction given by the direction allowed by these inclusions here I can find also the directions that for which the second component is. For instance, let's take two, right? So when I take minus the direction, I will have something that looks like that. So this vector take one belongs to the subtraction minus the subtraction. So what you can see is that this vector is not does not give us a descent direction. Even if I move, if I take small, so I move a little bit from this point, I will always increase the value of my function. Because in order to decrease the value of the function, I have to enter in this level set here. And instead my direction will force me to go away. OK, So this is not the sent algorithm. And the fact that I'm not the sent algorithm does not depend on the choice of gamma. Even if I choose gamma very small, this will not work, right? Yeah. The decent algorithm means that at each step you need to decrease. Exactly at each step I need to decrease the value here, even by choosing the step size that depends on the place where I am. This is allowed, but in this case I cannot do any choice of the step size gamma that bring me inside here. There are directions in the some differential that are fine, but if I allow myself to choose whatever I want, I will not miss second observation that let us take even simple example. If I take and let us keep the computation. If I want to minimize using the sub gradient descent algorithm the absolute value, then if I start here the derivative will be 1. So when I move let us pick the gamma and move in the direction opposite to the gamma, what will happen is that either I go on this side so we move this direct gamma, I will go here, the derivative is my fun, I will go here. So the point is that I will not get closer to 0 since if the step size is fixed, the difference between WK and WK plus one is always one. Does it work? So you can use the computation but so not one about the gamma the step size since the absolute value of GK is always one. So this tell us that this algorithm will not converge because the distance between two consecutive iterates will be constant to this constant gamma. So the there is no way to fix this. But there is a way to fix this and instead of the gamma of this gamma that depends on K and will be decreasing. So the main difference, let's think a bit more about this. The main difference between this algorithm and the gradient descent algorithm that we have seen is the fact that under the regularity assumption that we chose, when you go closer to the minimizer, the gradient decreases. So I have to fix the set size, but the steps becomes smaller because the gradient becomes smaller. Here instead the step size is fixed and the gradient is constant. So that's the. So this really. And what we can see for this algorithm is a convergence. So before stating it, let's also serve one thing. When you have a algorithm which is not a descent method, it's not very useful. It's not very useful to study this quantity. But since the last iterate maybe not the best thing you have done so far. That's 0K 1231 and this is the function values that I get. Maybe my these are FW2FW1, I don't know 0. So it may happen that the value that you found on the last iterate is not the best one. So usually when you study convergence of algorithms like that that are not sent methods, you study two kind of convergence convergences, either the convergence of the best iterate. So here instead of this, this is replaced by the mean over I in 0K of FW Y the say. That's usually called the convergence of the best iterate or what you can do. What is usually started is the convergence of a on the average sequence. Can you you exploit convexity and you study define WK which is the sum of gamma I WI divided by. So you take an average convex combination of your previous iterates, where the convex combination depends on the step size that you are using along the algorithm. This is a very choice. You can be much more creative on this in order to get the better results and so on. So this is the idea and you study convergence of this sequence. The results in the common scenario, the results that you can get for this and for this are completely equivalent. So you what you can do for this one, you can prove also for that one. So the it's not really does not make the effect. In practice, very often convergence of the last iterate is observed and they there are recent results that study also convergence of the last iterate, right? These are the this is the classic analysis. OK, So what can you say on some gradient method for writing? So let's focus on the class. The class is the one of convex functions that are real value. This is needed in order to have a sub gradient which is non empty at every step. Otherwise, if I allow myself to have a proper domain, it can happen that I arrive at some point where I don't have a sub gradient. That's why I don't know. And 2nd assumptions that we needed in order to get converted is the sub differential is uniformly bounded. So we say that for every G, for every W, for every G in the sub differential of West we have. There is a constant that bounds every possible sub gradient at every possible point. We can check this. This amounts to ask that the function F is Lipschitz continuous. So we are really so the class of functions that are allowed is not so OK, these are assumptions. Then what we can do is the following. We can prove, as I will say, both convergence of these two quantities and we have a convergence rate, the one that I was showing you yesterday. And the right one is this 1. So I write it on the average iterate, but it's the same before I write it. And I've got one thing, sorry as equal, there exists a reminder. And this thing is true for this one. This thing is true for the. Some comments, yes. So say that if the step was constant then it would not be the same. Now you're saying that any way you want, even the step, it's not a decent. No, no, it's not a decent, but I still have convergence and I have convergence of these two quantities that are the best iterate that I found so far from this quantity. I can prove an upper bound like this or only other, but previously not. Only 100%, but not compared to also with a constant step. With a constant so we can so we can do the exercise. And I wanted to do it so let's I can see. So this is true for every possible choice of gamma I. So I didn't write, but this is true, correct for every gamma I. So I'm not asking any restriction and say right for any possible choice of the step size. You can prove this inequality. So this is true for any possible choice of the step size. Now let's see what happens. Let's let's try to interpret this convergence result and see what happens to instance. First put on some step size. If I put the concept step size, what I see is that. Sorry, yes. That was a bit slow so I lost the statement. Where does the G in the sub gradient comes in play? Yeah. OK. So the B square is a upper bound on the norm of GK square. So you see it only there. OK. So we have and we have second line, yeah. Yeah, yeah, these two, yes, yes, sorry. Yes, I wrote it in the. Yes, I think he's correct now. OK. So let's see what that is. So let's take this is not so clear for sure. We have two kinds of contributions. This first contribution that as you can see is very similar to what we got for gradient descent. We see even now depends on the initialization. And then I have divided by the sum of that sizes and the second term which is a new term. And it's basically due to what I was saying before. It is due to the fact that these sub differential does not necessarily go to 0 when we go towards minimizer. And so we have a kind of error due to the subgrade non normal in some gradients. And it is as this form I have the sum of gamma I square divided by the sum of gamma. So if gamma is constant gamma gamma and then what I get is that I can prove the right hand side is equal to plus square and then here I have plus one gamma square. So if I take a constant steps, I cannot claim that the algorithm is convergent from this upper bound. But I have a quantity apart term which goes to 0, this one as the iterations go on and then inside the constant term, this term here is equal to V square divided by gamma. So you see it's proportional to gamma and tell me basically that I have I can arrive within a certain confidence. So if I like accuracy. So if my if I choose if I want to reach an accuracy epsilon and I choose gamma appropriately depending on epsilon, then I will reach the constant steps like that accuracy. But in order to do that I need to fix the accuracy in advance and run my algorithm with the proper gamma. OK, and in this sense it works OK. The most common sense, usually this I don't fix the accuracy in advance and then you want to apply the algorithm and see where I can get. Usually the choice is different and the gamma is chosen decreasing along the iterations. Something like this. You never choose this one, you always put constants in order to make things work. But with this choice, which is the best one for this kind of inequalities, what you get is that this quantity, the sum will behave as a sqrt K. So now I am right hand side let's say of the order of initialization. And then we have some constants that we don't compute divided by sqrt K + 1. And here what you get is that when you sum the square, you sum it 1 / K So basically you get the logger logger of K and here again the square root. That other constants and this is the what is called often the optimal convergence rate of a gradient descent. You have a square root convergence 1 / sqrt K and then you have this logarithm that usually you start and you say that the convergence rate for a gradient descent is 1 / sqrt K + 1. This is called. This choice is called a infinite horizon choice in the sense that I choose which is decreasing at the beginning and adapted to my step sizes. What I can do instead, as I was saying before, is to choose to fix the total number of iterations that I want to do. So I say I want to do 100 iterations and then you can choose a constant gamma which gives you the same interest rate based on that number of iterations. OK Other possible choices are the ones for which this sum is finite. So this is a sequence gamma I square is summable and this is not summable. This is the usually the usual test that you do on your step size. These are called moon row conditions. So the sum of gamma I is not summable. So gamma I goes to 0 but not too fast and here the SO goes to zero. Since the sums of squares is the sumable so it goes to 0 but not too fast. So this is the last practically sub gradient method does not work very well we can say. So as you can see, we look lost from the rate we lost the with respect to the gradient. And there is one additional problem that really makes sub graded method embedded method. Every time that you see that your method depends on a decreasing sequence, it's a bad sign since we never know how to choose this sequence in practice. This is not a practical information. See is the only thing that I know is that these C these these step must decay as 1 / sqrt K + 1. But I can choose the the constant as I like. So here I can do plus C here I can put D and when I will implement my method. Basically this will give me an extreme freedom and basically I will not know what to do. So usually the step size is 2. OK, it's a bit of a trial and error work. OK, so it has been, it has been used a lot to minimize the loss. There is one famous solver for SPM which is Pigazo Pigazo Pigazo that is by senior that I don't remember and Cotter and another one. So this this is this has been used a lot and it's just a separating method, right. So it has been applied successfully, but still it's not from the numerical point of view. The next one that we see is the proof is so I didn't mention anything on the proof. The proof is easy, easy and maybe just line the first line and then you can go as an exercise, you can try to do it as an exercise you have you do this, you expand this square norm and then here you just compute the square and then you use convexity and the assumption, OK, so you can try to do it. One thing that I didn't mention, but you can, it's a It's a result which is not so common in the little bit difficult to find. But there is a known result that establishes also convergence of this sequence. So you can prove that the WK is convergent to any matter and the techniques are similar to the one that used for gradient, so also the sequence is convergent. So last chapter of today, algorithms that are used for solving the functions that have some structure with different regularity of the terms, maybe I can stop, but other questions, yeah. Basically, what do you do to compute? Like a subgrade I mean. Usually. So in the cases that I know, these are cases where it's easy to do the sub gradient, so you can use the chain ruler. This is for SPM, for instance, It's very easy because you have the inch loss composing in your operator and then you have the sum. So in that case, this is like that. But This is why calculus rules are very useful because you need something like this. Sometimes it's costly, just you don't know. If for example we use constant step size, I mean we could converge it sublinearly in a sense, but we don't have the square root so many. Yeah, you convert sublinearly without the Super root, but in a neighborhood, let's say of the within a certain level, yeah. Practically and. If you know yes, yes, yes, it is indeed yes. In practice, the choices of the step size are are 1 / sqrt K and then there is the constant one which is almost never used, but what is used is a constant step size. Then at a certain point, so there's some gradient descent that goes like that. If you implement it, it will be like that. It's a certain point. You can something like this and then what you usually do, you decrease the step size and then you try to do something like this and do something like this. So the piecewise constant step size is a practical choice. One more. Question. So basically if we are dealing with this then if in case we should choose some kind of exact sub gradient in the algorithm, we should choose it like randomly from the whole. Interval as you like, yes. For instance for range loss SPM, usually they choose the left derivative. It's not in the interval, not randomly. If you have a way to compute 1, you can choose always that one, otherwise you can choose it randomly. So this will not affect the So what I'm saying the convergence of the algorithm. So there is one special choice which is the best one and it is the minimum normal element in the sub gradient. This is a descent algorithm and it's better. But computing the minimum sub gradient in the substantial it requires them to solve an optimization problem. So maybe it is not worth to do it. This is called the lazy sub gradient. But maybe if we continue with some hasty coverage and we've been prone to do some. Sorry, I don't, I didn't understand the last part. Well, maybe if. You had to deal with the like we needed to like change, choose one kind of separated from this interval, maybe like beneficial to use some hectic algorithm for it. Yeah, I I don't know of any results, theoretical at least that he tells you that this is better than choosing a fixed one that you know how to do. But could be I mean. Yeah, yeah. I'm not sure that that to be honest it will work. So the last bit are proximal gradient algorithms. So there is a, as I was saying, an instance of an ordinary class that is the one of splitting algorithms. And that really motivated a lot of research in the community at the beginning of the 2000, around 2000, 2005 until now. So the others are like that are are developed in order to minimize things like that. So here I would call it. So I want to minimize it. It's a plus regularization. So square loss L1 norm. This is the example that we have in mind and that we motivated the. So in this case what you want to assume so is that F is convex and and smooth and instead R is just gamma Sigma. So constraints are OK here. I can put the constraint norm of norm one of West less than a certain constant. And then the algorithm looks like that. I will write the algorithm so I start again from a arbitrary point and the sequence is defined like this. So these are these algorithms are like that are splitting in the sense that you do a gradient descent step with respect to the function of which you are the gradient. And then when you do it, you treat it separately. You split it from the regularization which is activated through a proxy. It is a proxy operator which we see is a kind of is a is a generalization of the projection. So it's these two steps are really different and this one is is explicit. Sometimes it's called a forward step. And this one is that is implicit is a backward step. So another possible name is forward backward algorithm. Let's define the option. The processor is the solution of a minimization problem is this one. So as you see, I replaced the minimization of a function with an algorithm that requires a huge step, the minimization of another function. The these kind of algorithms are effective when this minimization is easy, because if it's difficult and then it's not really useful to use a. This is usually expressed by asking that the function R is proximable or prox friendly, meaning that this computation can be done one thing. That is a hidden assumption I would say. 2nd observation is the fact that this proximity is proximal point. This is called the proximal point is what they find. It's really a proximal point of West in the sense that this function here as a unique minimizer, since I'm taking my function R which is convex, and then update a strongly convex function. So the mean the sum of the strongly convex plus convex is strongly convex. And we know that we have seen yesterday that strongly convex functions as a unique minimizer. So this is uniquely defined, well defined. What I'm doing is that I'm really regularizing R around the West, the quadratic turn centered on West. And so I'm trying to balance on one hand the minimization of R and on the other hand the proximity to West. So I'm trying to minimize, to go to work to minimize R but being close to West. This is why this is called the proximity operator. There are certain cases in which this proximity operator can be computed in closed form. And these are the best possible ones. So let's say the one where we love to apply proximal gradient algorithm. So the first one, let's assume that other is the indicator function of convex convex set. The indicator function is convex and so I remind you that IC of West is 0 or plus Infinity. And so I get what? What? What is the proximal? What is proximal gamma indicator function of West? So in this case, the process does not depend on the step size and is the projection of West on the set C So if I if I have if I'm using the function here. So I'm trying to minimize F / C what I guess here is the projected gradient descent. OK, so the proximal gradient algorithm is a generalization of the projected gradient descent. So what I'm saying is that here we can find an expression is not always true that computing the projection is easy things where the projection is easy computer, you can use this algorithm the projected gradient descent. The second case that I want this task of course is L1 norm if I take R equal to L1 norm. The expression and again, this is an exercise what is called the softer shoulder operator, and it's like that. So I put the feature. So as you see the proximity operator can be computed component wise. This must not surprise us because here when I try to the 1st order conditions I will get this a differential then when norm is a separable sum. So the subtraction will be the product component by component. And what I do when I compute the proximity operator of L1 norm is that I look at the component I the R component and if it is smaller than gamma in absolute value, then the crops put it at 0. When I compute the crops of this component is 0. Otherwise, if it's bigger than gamma, then it's a bit diminished and it's a bit augmented is smaller than minus gamma. This operator has a name and it's called soft thresholding or soft shrinkage. Why this is a soft thresholding operator? Because it's a thresholding operator. If the component is less than gamma, I put it equal to 0, it sends it as 0. On the other hand, it's not a hard pressure because it's that the component is bigger than gamma and absolute value is not left unchanged, but it's a bit diminished or augmented in order to have a continuous thresholding here. OK, so they they have the thresholding one would be here. So in this sense, it's a soft pressure. So the the resulting algorithm, Sorry, yeah. There is no absolute value in the last Commission. Right, yes, otherwise not that very often. So the when I apply my algorithm to solve these kind of functions, I will get. And then here I point gamma. Gamma look like this in order to apply. I am seeing this example. My function F is general but the regularization is 1 normal and look what happens. It happens that when these vector as a component which is less than gamma, it's zero at the next iterate. So we like this kind of algorithms because they will produce let's say sparse iterates. So iterations where some components are 0 without needing of without the need of setting some thresholds. So at the end I will have my last iterate that we have certain components that are exactly 0. I don't have to choose what to put the literature in order to see if the component is 0 or not. So this is particularly relevant when I want to approximate sparse solutions because it will give me a very easy way to determine if the coordinate is in the support of the solution. This is this made the really the success of this kind of methods. The second reason is that they get super easy to implement, almost as easy as gradient descent and as costly as gradient descent, because the software operation is really right. So the main cost of the algorithm remains the computation of the gradient, not the computation of the software. Just to close, I want to write the convergence result and SO theorem. The assumptions are the one that I wrote at the beginning. I assume in addition say assumptions plus as usual, minimizers not empty. This step size. This is again a super nice thing. This step size is chosen As for gradient descent. So I just The restriction on the step size comes from the factor from the Lipschitz constant of the smooth component. The constant step size is fine, so we can use a constant step size then. And what I have is that I write it since yesterday. F of plus R of WKK is an increasing sequence and then I have a convergence rate. They write it completely and we are convergence of this sequence. So the the convergence results are a copy of the ones for gradient algorithm. And indeed this theorem covers the one gradient descent that we have seen yesterday. In the complex scenario you can write a similar one for the strongly complex setting. Then you fall back in the inner convergence case. What we can see is that yes I so I don't mean this for this type. The nice thing is that for this kind of algorithm you don't pay the non differentiability, the non smoothness, while for sub gradient descent you suffer from the non differentiability. Here you don't. So you already say pay your fee when you compute the proximity operator. Since you are able to compute the proximal, you already solve your problem with the no differential entity and you can convert the straight that are the same as the gradient 1. So, and nice thing, convergence of the. SO I, I think more or less I, I told you, I hope that I, I made an overview of what are the basic algorithms to deal with weakness. Of course. So people as you see the others are super simple, just one step 13 parameter and people worked on these algorithms for many years and especially recently in the last 30 years a lot. So there is much more to say and there have been many more developments of these kind of techniques, but these are really the pretty blocks. So I think this in these kind of results, there are the main ingredients that may be used in order to deal with more complex scenarios or more refined. There are questions. Otherwise I let you go. Yeah. If F is a strongly convex, then. The convergence basically is so either RF or R. So you can play with a strong convexity what you like. In both cases you can prove media convergence. So what? I don't know to be honest. What? So what happens if the sum is strongly convex? So you need to convexity in one of the two? OK, how do you solve the the second optimization? Problem usually. The problem usually is when the case is when it works well. It's either available in closed form or for instance requires the solution of 1 dimensional equation, polynomial equation. It happens it is used in that case. More complex cases other algorithms are used. If your function R is the composition of some kind of normal with the operator, then you need to split the operator from the norm. Usually you know how to compute the prox of the norms, that's why I'm saying this. And very often approximately an algorithm used for the regularization, the variation are regularized problems. So where are very often is a normal. Beyond that it's difficult. It has been used a lot for instance in problems involving matrices for this, for that you have a lot of proximal calculus for functions defined on matrices that depends on singular values of matrices. And that's a bit the landscape. You cannot go hardly on this, I think. Yeah. Exactly. But just do a few Yes, yes. And here we are expert. OK, that's the question. I love it was not prepared, but we love we love to do proxy to compute proxy exactly. And you can still prove convergence. So if the error decreases along the iteration, then you can prove convergence determinizer. You have convergence rates, you will have some kind of error term that accumulates along the iteration. If the error is constant and does not decrease along the iterations, you have some kind of approximation results. You cannot hope to go to the minimizer. Yeah, yes, prox does not work well with stochasticity for instance. But with the deterministic errors is fine. There are no more questions. See you you later, but the class we start again with the professor ASCO.