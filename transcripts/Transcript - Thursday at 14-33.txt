So the spoiler is that you're going to consider basically regression in the context where you actually have a kernel, OK? You can see that there is something you can more complicated, but ultimately become just the same as what you discuss, OK. And in fact, the main idea would present. You can illustrate the kind of. Dimensions. OK, so we. Want. To think about what does it mean? For the process. Easy or hard for OK SO. Remember that when you lose. These squares OK. You have the best possible factor. In the and satisfy a. Linear system. OK. You remember what was Sigma? Sigma is the true covariance operators OK and H is just the integral of the expectation of XY doesn't matter which is your linear system OK. And then what you do in practice is that you consider an approximate solution of you have Lambda, which is. This OK? And here there are a couple of things going on. One idea is that you hope that these are decent approximation. That. These two guys are decent approximation of that guy. So one is the 2 Co variant, OK, And then you remember what is this? You take the matrix and then you add the Lambda identity, OK, What is the effect of that? It's basically that if there is an eigenvalue which is stated, OK, you basically don't touch it. But if there is an eigenvalue which is small, it's effectively like filtering it out, OK. When you do this kind of operations effectively, you are depressing the importance of small eigenvalues. OK. Makes sense to see it. If you're not see it, imagine that Sigma is diagonal. Sigma hat is diagonal. It's just a set of numbers and you have plus Lambda, the diagonal, and then you infer it. If the number of the diagonal is large, you don't do much. If the number is small, you ignore it and you put Lambda. OK. Is that OK? Take a second about the following question. Which W dagger? So on the left there is the ideal world and on the left and on the right it's what you do. Can you think of situation where you know to do a simulation on a computer? Can you go and generate the left hand side? Can you imagine how you could generate problems that are easy or hard for the algorithm? On the right. Make sense? Which problem are easy and the problem are what? Do you think yes? OK, the hint was you throw away more eigen values. Right, so. The two pictures that I want, I want to think of it about is again, let's focus particular on the on the role of Sigma. OK. I think of the true one, the true 1. You can imagine that if diagonal for a minute and let's say I mean dimension 2 means that we're going to have a big eigenvalue and a small eigenvalue, OK? If you draw it, if you assume for a minute the eigenvector are the coordinate axis, let's say we look like this, OK. This is the. Image of a matrix seen as a master. Because it's not just that it doesn't not map a ball. Into a ball. But it makes ball X OK and the X side are the size of. That we're good. This is not generic because to some extent I'm assuming that the eigenvector are this guy and this. In general you might have that depending on the Sigma you get. It still looks like an egg, but the orientation might change. OK, you can have. This. Egg you can have. This egg, and of course now I'm going to show you, but the egg could be more or less squeezed in the limit case. You might even look like a line, right? Squeeze it a lot in One Direction. And lose it this. Is about the axis. This thing here does not depend on Y at all, right? Then let's give a second about Y and I think about the role of of how Y might change the picture. OK, to do that, let's consider an easy case where you have a. Function. You just have the West star, which is the true vector, OK, and Y is just this. And to make it seem simpler, let's just make the pressure. When it's positive, I count 1. When it's negative, I count -1 So you can imagine, for example, let's take the sign for example. If now I give you this vector, OK, this is W star, then OK, I can decide that basically, you know the line is going to be this one, and I can decide on this side, this is plus, and this guys is minus. OK, there's a classification. On the left you have -1 plus and on the right of the plus. You could remove the Y OK just. Making it a bit easier and you. Would have some kind of. Make sense? OK, what I'm showing you is that in this case, we're thinking that the true vector is defined by a direction, OK? And then I want to ask you which directions are best for. So what I'm trying to say is you have a vector here and now you have this thing here, and you want to understand when they're compatible, OK. So. If I am in this equation, this vector is perfectly aligned with the biggest direction. Let's consider the not dotted edge. OK, this is perfectly aligned with this. If I was to throw away Y eigen value, the smallest of the three, there's no problem at all. OK, I can still basically recognize this guy. You agree? Make sense if the true shape of Sigma was the vertical one, it's kind of the. Opposite situation. The largest eigenvalue points in this direction, and this guy is aligned with the smallest. Eigenvalue OK. What does it mean? Well, if you throw away the smallest eigenvalue, you throw away. Everything so. If you only keep the first doesn't work, you have to keep this. Does it make sense? So what I'm trying to convince you is that when you write X there is a directionality and we write W there is an alignment of this one vector with respect to the directionality. OK. So it's not all symmetric. So far we consider everything to be worst case. We said OK, everything we've done so far, Sigma could have any shape. We knew it was containing the ball and Y was also containing the ball. But now we want to think a bit more about the job and we say Sigma break the symmetry with the spectral structure and Y now can be aligned with the spectral structure or not. Does it make sense? So. Again, if you see that now I can fix Sigma which will be the marginal distribution of my data and now change the conditional distribution which is here or the other way around, it doesn't matter too much. OK so somewhat these are property of Y given X and then condition on X. You see how they are OK is the picture. Clear So what it. Asks you again, how do you build the algorithm? Doesn't have access to the 2. Sigma or the 2 W star? You only sample it, but if you ask to be the problem, which is very hard, you basically pick this covariance and it's got that perfectly means a line. OK, so you basically? We need a very small Lambda. Regularizing is not a good idea. This is not the right algorithm. Try to throw away the wrong stuff. OK. Does it make sense? So. What I'm trying to convince you there are two things that matters. One is the shape of Sigma and two the alignment. Of. W star, which we're going. To call W Dagger. And Sigma and nothing I said is precise OK. But hopefully there is an intuition there. And why if we make longer or smaller can capture the hard? Situation better. For if you put 1 equal to. Zero or close to 0, you're looking at that. And that's exactly the point. So you say, well somewhat I created a bad situation. For this algorithm. But if you can still recover, right, because as soon as the Lambda is small enough smallest the smallest eigen value so that the factor, you're still fine. You kind of we're looking for a solution and you start looking in the wrong direction and that's fine. You pick Lambda Big Gay. You're. Restricted to look. Here and you make Lambda small you can see. It OK so. This is a purely geometric tuition. OK, that's it. It turns out, and again, this is a bit of a stretch, and the mask is going to come in. If you're in finite dimension, you cannot harm it too much. Certainly in this situation you see that one problem would be harder than the other, but somehow they can manage. OK, then we can try to say can we beat them? We have the code for the algorithm on the right and we are free to build that more complicated situation. How can we make it complicated? Well, one way for example, there isn't any noise, so. That's one issue. OK, but one way to make it more complicated. To say. Well, you know, give me more directions. OK, now venturing. To do. Something like this? You can think of it, if you give me more direction and I want to get versaria, I can hide the W star in. You know, I have many more direction. I can fool you to search it before you find mine. Because the moment you give me your Sigma and your Sigma 123, I'm not going always put it in the last. One OK does. It make sense. Again, this is very intuitive. How big you can make the dimension. Well, you know, the bigger you make it, the more you imagine that you have this ratio that becomes worse. And you obviously see that some constants in your bound will explode, some N will explode, something will explode. It doesn't really explode. It just becomes very, very, very large, OK. When you start to really break, like you don't recover anymore, is when the number of direction is infinite. If you allow me to put your infinitely many direction, then I can go and hide my W star in the smallest eigen values, the one that corresponds basically to. 0. Direction eigen vector corresponding obviously direction of eigen values, which means direction. Of the eigen vector. So I go ahead and hide the direction. Then you have to spend infinite amount of time because now you have infinitely many directions. So we haven't talked about this stuff much because it does play a role in finite dimension. But you really see the full power of this argument. And we go in infinite dimension because there, that's where the no free lunch hits. OK, when the first time you study the no free lunch, you can say, oh, that's not weird. Maybe this could be super weird. Not really. When I give you an algorithm, it's basically a search in a space of solution. You tell me how you search. It's not only the case you can do it, but rich regression is easy enough. You know, Netflix a bit harder because we don't really know how they search for a solution. OK. So it's not so easy to treat them because we are still figuring out where they start. But for this algorithm he likes the eigen values. I just, you know, put my solution in the direction of the small ones. There is. OK. So this is one of the reason why I think these squares are so nice because you can we understand, you know, this kind of very momental effects like. No free lunch you. Give me an algorithm. You're going to find the problem for which this algorithm will. Yeah, we've really slow in. Converging. Duh, of course. I'll tell you that you can say, well, I can have easy problems where the algorithm will be awesome. What is the awesome problem that you start? Is the first, second vector. How many direction you need one? Effectively, this is not any dimensional problem. This is 1 dimensional problem. The moment you figure it out you should be super duper duper. Fast OK does. That make sense? So. Based on this, what you want to do is to discuss make make this work. OK, actually before I do it, let me translate these two things in mathematical terms. OK. The first one is very easy. That's why you might want to think a second about it. What does it mean? Like the shape of Sigma and describe it through the values of the eigenvalues, right? OK. And so you basically want to say that Sigma 1 is bigger than Sigma 2, bigger than Sigma 3. So typical the way you do it is that you choose the behavior. OK. So you typically say, OK, let me call Sigma I Sigma, the eigen value of that matrix. I don't remind you here that Sigma is matrix or the objective value are real and positive or 0. But they're never. And then what you can do is to say, OK, to describe this guy, I'm going to assume that they behave that. Some. They have some behavior. For example, this is basically saying they decay polynomial fast. You could also say they decay up to AY and then they're zero. You can take the exponentially fast. You can say they. Decay logarithmically fast. Depending what you do, you will have a different instance of this theory. Polynomial is the default lazy one that everybody. Just one choice. But it's just one choice. OK, we're good. I want you to invent this condition. OK? We want to describe vectors that are pointing in the right direction and to make you think how they look like, let me give you. So I'm going to give you a vector, I call it V, and you have to point it in the right direction. What could you do? So I give you. This guy and I. Throw it out at random and it say point it in the right direction and the right direction is given by this guy. What can you do? Use Sigma. Use it less than. Project we don't have to do any spectral. Decomposition. It's easier. We just want to push it many, many sets to play it once, OK, or half the time or we'll see. You just take this B, you apply Sigma, and what's basically doing is that you push it in the right direction. OK, if you put your power, because I can do it. These are all positive symmetric matrices. I can take any power when F is equal to 0. I'm doing nothing when L is very large. Basically I'm pointing in the right direction. I'm basically pushing for the direction of the biggest. If you take the singular variable composition and you write it with C right away because you have like in. Values between. Let's say that was the eigenvalue between zero and one. OK, so the. Condition here is West dagger equal. There exists some V, let's call it V dagger with some finite norm OK such that it represents my West dagger. OK. Does it make sense? So we built it, but now we assume it OK, We assume that WWW. Looks like. This usually means that we know that it's going to. Be directed OK the. Biggest is this norm. The less was pointing in direct direction. The small is the norm, the better it is. The bigger is alpha, the more is pointing in the right direction. The more less, the more. Is that OK? In a while, we'll see for. Those of you that. Don't like geometry but like analysis or statistics? That then is. Equivalent to assuming if. You take the current dimensional setting. You see that? Basically can be. Seen as a general geometric way to describe. Smooth functions. If you take, you're going to be able to take situation where Sigma is some kind of integrator where the eigenvectors are actually Fourier modes and the decay would describe smooth functions. OK, So for example, they can stop with the solar functions. And this turns out to be the way you think about non parametric statistics. The object of interest are, but for now we just keep it like to me, you know, published paper on this for 20 years and people have been complaining on this for 20 years. The geometric. Perspective the sequence. You can get to understand why this condition into the picture, forgetting about function just thinking about geometry, but there. So. Now we want to consider the setting where there is a. Kernel which means that. There is a space and then we have that. We're going to make a change of notation. You have seen that this guy for any X belong to the space, I want to call it Phi of X. You see that these are feature map blah, blah blah. I don't care. For me it's just a short term notation, OK, because I'm going to write Phi of XA few times. And the second thing you know is that now F of X using this notation, we saw this Phi of F of Phi. Is that OK? That's what you saw. Now in your head, you can now compare this expression that we used on Monday and this expression that we are introducing. Now, and you might. Appreciate how they look very similar and in fact that's the magic of reproducing converse face. You can think of them as some lines somewhere. OK. Particularly if you have some function of, say you have the normal FX. OK, so normal F if you want to compute gradients is the only difference. In the first case, you have to compute gradient in the least of partial derivative sense. In the second case you have to. Compute what are called. Fresh derivatives OK which are gradients or Infinity dimensional? Functions so. If you know what they are for you. Otherwise study what they are. It turns out that they behave exactly the same way, and so if you have to, consider least squares. OK. OK, take your. Is that equal to 0? I'm going to write exactly that expression OK, but instead of West Lambda and now I have Lambda. OK, the expression actually. Looks the same, but the meaning is different because I have to redefine things. For example, what do you think H is going to be? Well, as you see, the game here is basically a change of notation. When you see X, you should write Y of X. You see W you'll write the symbol. Of F OK, so. Here, instead of West, I'm writing F Here I'm still writing this guy, but I need to change the definition. This was just. One over. NI from 1:00 to. N. And then in the linear case was XIXI. Transpose OK. Now it's going to become, what do we say? Phi of XI? Phi of XI transpose, correct? Because X becomes Phi of X. Because in general now where infinite dimension is the transpose, this notation is used for the outer. Product, OK. But if you're in a head, that's the the symbol the tensor and you're confused. So I think of this outer product that's the same as XX transpose for instant dimensional. OK. Speaking whenever you have something like this. This means OK this. Is the mind boggling if you've never seen it. OK, this is it. Sorry, I forgot here. What about a check? A check was. I didn't remind you, but remember it was the sum empirical one with the sum of XIYIYI is still a number, but XI becomes. Five XI OK, any questions so this. Is going to be a thread. OK, lucky for you, we're going to recycle at least half or more of Monday's class. But whenever you see Sigma hat is not XI XI transpose, is Phi XI XI transpose OK? Questions by the way. This is imposing you a change of notation, OK, because the mental exercise because you find a lot of people with this. To keep that notation. And just say that X is infinite dimensional and that be. Good enough. OK, but since a lot of papers. Written this way, I'll force you to do a bit of mental gymnastics. Because whenever you see AW, whenever you see an F the West, and the other way around, Whenever you see a file of X. In fact, if you think of it. Make sense? OK, So what do we? Want to do we want to study the learning property of this stuff It means. That we want to do this. When the Infinity move in general is taken over over all measurable functions. However, when you look at the when you look at linear function, what is the best function you can find? If they give you Infinity data, you will never be able to find the function which is not linear. Correct. Here, even if it give you infinite data and let Lambda 20, you're still not. Sorry, right, SO. You just give these big sticks the biggest basket that you search for is H the entire refusing current space. One first thing that you make sense to do is to say, OK, let me compare this guy. To. OK, so I say, OK, look, anyway, I cannot shoot for anything besides my age. So let me look at age. Why did we do it the last time? Because this error was so, you know, there's there's nothing to say about this guy, OK? Because once you choose age, you have to live with it, OK? And if you take linear function, this is the difference between the best linear approximation and whatever it was. OK, Now we can say maybe something about it because age is not the space of linear function, it's a space which is an infinite dimensional space. So we might want to take a second, but what does it mean for that to be big or small? OK. Does it make sense? So let's focus a second on this. It turns out that there are choices of H which are which makes that error. 0 OK. And these are what are called universal reprodition kernel diverse spaces and what we want to just check and this correspond to a density property of this space, which I think we can talk about this more. I'm going to show you briefly so universal RKHS. Means we have to write this. 15 times so I'm. Not going to write F and I'm going to write it. Like this INF L / H for. That I say that this is equal to. INF L over MXR and the same stuff. When the 2 are the same, I say that the space is universal. Miracle here must be because H has to be. Big OK. So here the observation is that in LH minus in L is actually. Equal to. In F. Of over H of. F minus FP square rho square FPX is the conditional expectation and the normal row. So. Get the difference between the two is actually equal to a least squared. Problem OK. Where you want to do what? You have this function here which introduced maybe a couple of days ago, which is the best possible function achievable by d ^2. Over all the function in the universe all measure and you need to project it. That's what this is doing is finding the closest into H. OK, let's do a drawing. This is the spaceable measurable function. This is H and somewhere here there is FP. And what you're really doing is find an element here close to that in that to norm. Effectively. Trying to project it. Here is that OK? That was written OK. So the solution to this? Problem is. The projection the. Projection sends functioning a tool. Into the tool OK. There is a bit of math here because you are writing this dimension. When you take H in its norm, it's closed by definition. But now when you view it as a subspace of. You can close it OK. You need infinite dimension. They're all the same. OK, they coincide. They're infinite dimension. They're not. You have this. And so. This thing here. The Bucha. Outer shell is the closure. OK, so. It's literally the outer. Shell but. Are we assuming that? H is universal or is that? A. We are defining it and we're just so this is the definition, OK, and I'm just showing what it means. I'm doing another. Characterization of universal you. Can stop here and turn off your brain, OK, Or you might want to say, OK, can I get any intuition or insight of when actually I hope to have this property? OK, And here I'm going to try to get closer to that. And I'm just saying if you take the difference, you can write it this way and you see that it's just a matter of projecting on that space. OK, here there are two questions. 1 is. How do you prove it? OK, this is relatively. Easy. I guess I forgot to tell you that we're going to. Assume. Throughout Y not. Be more than a constant. Always do that. Just do it. Simple. Which means that. L2. Is just a space of function which is normal and because of that assumption is AP actually is in row observation one then you have seen that the difference. Right here going. To have the three corners. So this is going to become the. Three corner you have seen that the difference between the expected risk of anything and the target. Is always just F OK? That's what I'm using. So I'm using the fact that here there is an info over M OK, but actually this is the same as an equal. Over 2. Then I have the this difference this way. Sorry, I get everything you want to do that you want to say that. So when you do INF L. Of. FH. OK. Minus in. Over F this. Is the same as. L of P OK. This is a fact. This is what you want. But because this is a minimizer, you can write it like this. OK. P. Belong to L2. L of F equal minus L of. FP equal F of FP. Or any F in L? 2. Which I can also write. Like this OK so means that if I take it. In here I need to take the OK this is just a constant and I can drop it. Is that OK? The purpose of this is just to show that this difference is indeed a discord. Problem. And its solution is a projection. Basically you have the. This is going to be 0 when. EFP is equal to OK. Which means that the closure that your function that you know that the closure is the whole thing. OK, so this is going to be true. OK, it's doing 2 situations. Either you're lucky and you put an FT which is not everywhere and it does leave in your space, so you take a smaller space with say, bending in the function, but your function leaves in the closure. Or you take. Any function? Like, you take a space such that its closure coincides with it. So a sufficient condition is this one. Another sufficient condition is and throw in the closure. This one is the condition on the problem You need the problem that you you pick the kernel. You don't know how rich it is, but you know that your problem project inside that. And if you want to be agnostic about the nature of the problem, you ask if there is a kernel that will work for anything. And it turns out that this can be translated as the density property of the kernel in a makes sense. Yes. So the fact that. The kernel is not related to the loss function. At all. Well, I guess here I'm using it everywhere. So the question is whether the property is related or not to the loss function. Here we're speaking to this per loss. So everything is written this way. So the reason why you get this norms is. Because of this per loss. However, if we're to use another loss like the deep sheets loss. So if you don't know anything about the loss with luck. But if you know that the loss is Lipschitz, then you have a similar this where instead of the square you basically go from this to L1. Then because it's a probability measure and one is controlled by L2 and you lose a square, Geometrically you have the same thing and still relay the approximation error to a projection. Right at. Least the density reason here it's a projection, because this is a subspace of a Hilbert space. If you lose some more exotic set, then will not be characterized as a. Projection, but you're realized. It's really the projection in the end is geometric interpretation. Then this interpretation, OK, gives you everything somewhat. Again, view this in light of this discussion. Or basically say if your function is anywhere and your space is there and you can say anything, you don't know anything about your space, you're going to make an end if your function is in the closure game. But that's an assumption of the problem. If your space is dense, then basically this is just misleading because really I don't have these. Slides they can start. The only thing for which they differ is the outer shed. OK, it's like an orange and it's outer peel. There's nothing. Else the whole. Thing the RHS is the inside of the orange and the closure of the of the RHS is the. Skip, skip, skip Orange skip. Desk. All right, that's your orange, the skin, and there's not much water. OK, yes, is built as a closure of the span of. A different. Topology. OK, So you have to be careful with. Topology this morning you saw that you did the represent to every space by taking a prefer space and complete must be complete, which means that it's with disclosure. But that's in the in the the vertical norm which will give rise to this. OK. Here we are doing the N 2, which is using the weaker topology, OK. So that's why when you age, the substance of N 2 is not project OK. So it really makes sense to. Kind of because. The the Y the D OK. Also notice that if the reputation curve space is finite dimensional, all these is OK. You can say you take Poly, we take linear functional, we take polynomial of any. Arbitrary 6° OK. And you basically know that the size of the space is the size of the number of polynomials you have, which can be extremely large, but it's still a closed space. OK, and so either you leave inside or if you leave outside you project and you make that error. Which is the difference? Between whatever was your function the best? 6° polynomial OK. Approximation like you and unless you see the example one-dimensional example where you take something like kernel that I think is for multi. Quadrant you want to correspond to more normal degree. That's a simple. Exercise. You can see that representative is like taking all the possible. We said there exists. Again remember that is AW. Somewhere in the world there exists an F in H such that L of FH is min of SH. In the finite dimensional case, we saw that we could relate this to this condition. OK. And so we know we use the fact that you know H was in the range of Sigma. So if you can see that H is in the range of Sigma, you're good to group. It was sufficient. In fact this is addition to get that if H is in the range, OK, it's easy to see that in general that's not the case. In finite dimension it is. In general it is not OK. I'm going to show it to you. Peak V in the kernel of Sigma 1/2 OK, which is the square root power of Sigma, which means that if you do Sigma 1/2 D you get zero. So assuming that you can get the square norm OK, you can do the square. I can also write these as D Sigma D OK, but then remember that this guy is expectation about their product. Let me skip one second and small mass sufficient and see there is actually the expectation of BX square. But then the fact that this is 0 implied that this is 0 almost surely. So each X 0. How much is VH? Well this is V integral of XY the row. I use the linearity of the expectation. I put in a product inside and this is equal to integral Y DX0 which is OK. I do this stuff because these are more population that someone shows the difference in the finite and Infinity dimensions. OK, because in the finite dimension you know that you notice the compose H in kernel of Sigma 1/2 and the range of Sigma 1/2. However, here we are in infinite dimension and you have to worry about closures, OK. And it turns out that this is what you get. So again, it's not the entire orange. So the inner orange is not enough, you have to add the skin. OK, so we just from this argument derive that HK is in range closed of 1. OK. However, the following St. inclusion hold. OK, you would love this, but this is only containing this, which is itself containing. And the one thing we can prove is that your age, unless you make some assumption, will only be OK. In finite dimension, this is equal to. This was equal to this. OK, let's keep this infinite dimension here, at least to kind of see what's going on. So how we're going to handle this, You can do it in two ways. You can drop this assumption and see what you can do because you can just say, well, let me make this assumption, OK, so that they know that the minimum exists. I know that at that point I'm going to lose generality, not making an assumption that was not there before. But you remember that before I was looking at the fine dimensional space. Now to the dimensional space. And I do this only because the other analysis is still taking kit for you. Now finish. OK. So we're going to make that assumption. Good. In many, many possible ways, and there are a little bit more you can do. Spectrum calculus, the only complicated stuff to remember the inclusion of ranges of symmetrical. OK, so we're going to make for now 3 assumptions. Remember that we assume X to be involved. Now we assume 5 of X to be involved because five of X is the new name of XY is in an interval. And notice that we have for free that F dagger satisfy this. We know and I reminded you that you can always relay the risk to the L2 norm. But when you have two functions in an RP chest, just by using that expression, you can use the other expression that we use, which is this one. This norm is the norm in H OK, If you go back to what I explained on Tuesday morning, I guess we just did the same thing for vectors, OK, the exact same results. Hold if you have functions. We didn't use any word, OK, The fact that we're in finite dimension because this is we already seen this, OK, So I'm going to announce whenever we see something that we haven't seen. This one is the same, OK, Up to the fact that now whenever you see instead of West, I called them South, right? Then I guess what I'm going to do is we're going to spit out a piece of the bound. OK, see. Right. Is it comprehensible? OK, but it's true. What was this? This was F hat Lambda minus F Lambda. Where F Lambda was this intermediate object. Remember what was F Lambda? W Lambda was the solution of each regression with Infinity data. Here's going to be the same thing. Let's keep the place where I write the variational form and the right expression that you want, which is this one. So in the same one there where I replace the variance. OK, All right, so this was it. Or rather it was this and this. So this old thing in Norm is this, and this old thing is just Sigma 1/2 and Lambda minus. OK, remember, these were monster, but believe me, this is not. We're going to find out. OK, what did we do? You just remind you this one we just split and do the norm estimate, OK. And we figured out that this whole thing counted as a one over Lambda. So the overall contribution of this term was Lambda. So this has a negative power 1/2. This dominates gives me one over square root of Lambda. I think this square becomes one over Lambda here, this Lambda square here. So the overall thing count as Lambda. So I'm going to do a lot of this wiggle stuff today. OK, Don't keep track of constant, only write the quantity that I like here. I like Lambda. OK, here, what do you do? We have to do concentrational measure here. We have to do concentrational measure here. We have to brutalize this to realize that this norm is always like the one of the dagger. So this whole thing become roughly one of the then we have to massage this because we would like sorry, one was root of N and there is the square one was root of N. And here we have to massage this, expand it using Neumann series. And then we turn out that, you know, it was up to constant and some condition. It was as if this guy was. So this old term would contribute 1 Lambda square OK. So for reference, means that the bound was of the order one over Lambda N plus Lambda OK. This Lambda was the one coming from the approximation bias OK. And this one was the one coming from the. The constant was a bit more complicated and this bound would be hyper OK. With high probability, which means that the dependence on delta is of the kind log 2 over delta log 4 over. For sure this bound This means up to numeric constants or constant that need not depend on Lambda. OK I only want to show you the dependence on Lambda. Then I get this. If you optimize Lambda which is the best error you can get. Or you can do the derivative but the trick is to set them equal. OK so this is square root of N you get Lambda square equal 1 / n. So the best rate here is. OK, So what we want to do now is try to see if the whole discussion at the beginning of this class and head OK, and now the two trick is easier than the other. So in the one trick, we're going to make any assumption on the shape of Sigma. We know that it has a shape, but we don't know how it is. We make the assumption that F dagger is pointing that right there. OK, now the setting is the resistant GH dagger H alpha equal to 0. That dagger equal Sigma alpha H dagger H dagger. OK, That is, this is exactly what we discussed in the 1st. You may ask, we were talking about vectors here. You would just look at functions as vectors in your. OK, an assumption as a name that comes from the theory of inverse problem, which has nothing with machine learning. But it turns out that there is exactly what you're using there and it's called the source condition. So you do this name, OK, the name is weird, but it comes from the typical interpretation when you have measurement system and you have to do version, all right, now that we have this, we want to know what is the effect of this, OK? And it's actually very easy to see. You have to go and check there where I have dependence on that dagger, OK? And it's not in many cases. One it's obvious and one it's where is the obvious place where there is that dagger, right? OK, let's talk about this Lambda because we're not going to touch it. We're going to get it for free. Everything is going to be. Let's just focus on this. What did we do last time we forgot this? We just said, OK, let's keep it. Now what I want to say is no, no, no. This value is a nice property. By the way, notice that you can put your alpha equal to 0. I'm back to business. What we discussed basically. OK, It's like making more assumptions. I don't know where it is. We already discussed this. OK, 0 and I start to know where it is. OK, which means that now instead of this expression, I can consider this expression. OK, I just use the condition above. But now notice that all this stuff commute. OK, so this is actually equal to Sigma -1 Sigma alpha plus 1/2. Take the norm of everything. This is what we call our plus this one. You can do the same norm estimates we discussed 15 times and what you find out that is actually goes as alpha. So basically we have to do this minus this. So become alpha -1. OK, what? And also, I'm sloppy about constants here. Again, today's constants. OK, now if you check this computation is true until alpha is between zero and one. After one it doesn't improve anymore and it stays the same. And this is what is called the separation phenomenon of. Now on we need sanity check is equal to 0, we get Lambda one over square root of Lambda, which is. But see in a minute. I've already seen from here if alpha is big, for example, if alpha is 1, again, check this already here. If alpha is equal to 0, this is 1 root of 1, root of 1. OK, so rather than slowing down because Lambda will go to 0, essentially making it much faster. Is that OK? All right, now we can go and work here and say, OK, here this whole thing is that number squared. So I get two Lambda square times A2 alpha -1 R square just taking the square of diversity. Which means equal to 2 Lambda 2A minus 1 + 2 + 1 R Plus OK again here. We already did save it somewhere. We saved it here. When alpha is equal to 0, it means that the approximation part goes down as Lambda, but the alpha equal to 1. The approximate part goes at Lambda to the three. So I gain to but if. Lambda is that P = 1 part it looks like. 2. In the in. The Lambda alpha, but this is only a piece because I still have a Lambda square. That piece is just this, you're right, but it's still like this. In fact, you see 1/2 you get OK, yes, when you do the. So when you have to do this, OK, when you have to compute this, at some point you have to tell the derivative of the function which is Sigma to the alpha plus 1/2 divided Sigma plus Lambda. This is a real valid function of literally this computation. You just take the derivatives and equal to 0 and. Check what happens and. What happened is that this has. This form is equal to 1 and otherwise it keeps the same shape. So let's say you have. Let's say strictly speaking, you have this, the power you should put here is this up to 1. And then as if here there was always one. It doesn't improve after one, it improves after there and then it stays the same. So now make it like a bit easier in the same. You're basically blind to an improvement for any bigger alpha. So it's not that it cannot be bigger, but it gets just algorithm doesn't understand anything. OK, OK, now where is the other place where there is hidden F Lambda? Right? F Lambda which is written here is written this way. However, H is equal to this. So I put them together and I get Sigma Lambda -1 Sigma F. OK, figure out. Extremely lazy. So now I want to observe that now when F dagger satisfies the source condition, it means that here it can replace F dagger by then. Which definitely means that now the norm estimated to do is -1 Sigma Ala alpha plus one. OK, so here similar to before say, let me delete this here. So again, it's the same calculation OK, but here instead of here. So here I have -1 alpha plus 1/2, here I have Lambda -1 Sigma alpha plus one. OK, alpha plus 1 -, 1 to the alpha. Why do I notice I made a mistake? Because for alpha equal to 0? I'm back to my assumption. And I know in this case that should be order one OK. However, when alpha is smaller, actually that function goes to the smaller space. I guess that is somewhere OK. Now, as it turns out, this is not the crucial thing and it doesn't really change the rate too much. OK, you see that frankly speaking, this was a constant. This was one of root of Anna. This was one of here. There was a constant that depended on Kappa and M Here we depend on Kappa square and this guy. Now this guy is better because it goes to 0. It's not just a constant, it goes to 0. But this is somewhat not dominating compared to this. So the overall order of this doesn't really change. It's a bit better instead of like twice a constant of 1 plus something that OK, what I'm telling you here is that now the new bound we got, so we're not going to change that. So somewhat we're going to ignore this improvement. We give a rougher bound and say that now what we have is that we have two. This is the same. So one over Lambda, this is north. And if you want here again, remember that here the constant is a bit better, OK, instead of like 2, is 1 plus Lambda to the alpha something I don't care because that doesn't change the rates. So here I do this and I put another two and then what do I get? Is 2A plus one OK? So if I ignore the constant, this is the kind of bond I get, and we're going to compare it to this. This part is the same, but that's not changed, OK, And it changed this way. If you now check what happens, OK, you should get so 2A plus 1 + 1 + 2. So it should be that the best possible error is 10 to the -2 alpha plus one divided to alpha +2. Again I do this by I because I set this part to equal. So I multiply this by Lambda become two Lambda 2A plus two. I invert and then I shall because I know the So up to two I get the same. This is what I get. OK to zero, and there's nothing interesting here. And here you see, if R is equal to 0, what do I get? Which is the same as before. You're going to lose much, OK, If R is equal to 1, OK, this becomes 3 and this becomes four. OK, so I go from route 10 zero to one over north to the 3 / 4 if alpha is equal to I don't know. OK, that's kind of the stuff I was telling you. This is, you know, I don't know, I mean this just tell me that I expected improvement to be there and I see that it's there. And notice that this is just the theory for the polynomial. I could have said here that here did not. These are coefficients going to 0, and I can say that this goes to 0 exponentially fast. Then you will see that here you get 1 / n up to log term. Of course, you could also say that maybe your function, even though you work in an infinite dimensional space, your function is actually sparse, meaning we're, you know, it only lives in a finite dimensional space. Which means that you can put any rate you want because they're all true. And you can even get rid of log N in some sense, the number of nonzero components of your character. So don't stick too much to this if there were very particular meaning. What matters here is the theory, OK, And the fact that somewhat we introduce an idea and you can see the effect, OK, So what is the real thing? Like why the underlying, you know, makes it faster? Than parametric. Because this is if you have a. But this is not so. We are in infinite dimension and the number of parameters that have to be infinite, again we have to agree on terms. OK, more or less. You know, when you have finitely many parameters, you typically shoot for something which is of this form, OK, which is the number of parameters divided by, you know, the number of dimensions divided by the number of points, OK, When you go to infinite dimension, this becomes infinite. So this is not very interesting. But what you get is 1 over space. You go from D over north to one of those root of that. And this behavior saying, look, if my function is simple, I can interpolate within these two regimes. If my function leads in space is already enough to be able to say something and I can say it was just root of it. It's not to say that somewhat, you know, is well aligned to the problem. Then I'm going to, you know, get something better. And somehow what I'm pointing at this is just one form of alignment. But if I allow myself to do something stronger, more aligned, OK, for example, it only depends on the 1st 5 principal components, then I would get something like this with the 5. Whereas here I'm here, you don't see this because again, I allow myself only to look at this one of the OK, that's for not I don't know, but makes sense. You know, you are anything dimensional. Probably you can go and hide your direction anywhere, OK, but not really because you still have to be in the space, OK. And here I'm very well. If you know that it's actually more aligned, you gain. The more it's aligned, the more you gain and. One of them even if you say the the Lambda. DK double exponential, yeah. So 1 / n in regression, in classification, you can do that. So it is known now that if you have a classification, so we didn't talk about it much. But if you have probably like this, you move to classification and what you see that you can inherit this bound more or less, you more or less get the same or a square root difference. But if the data are separable for problems like one block, one block, the rates can be exponentially fast. This someone suggested regression is a much harder problem than classification. Classification can really do something like that. But for regression, I believe that I guess maybe you have noiseless data. So if you have a function and you observe it, I think if you have a function which you observe without noise on a set of randomly sample points, then you can do better. For example, I think if your function is is a exponential decay, then you have exponential rates. So somewhat I guess is similar. So you can get much better both in regression and classification. If your noise is 0, if your Y is a deterministic functional X OK, then you can do something the Y is that OK, which might be one thing is here and defensive sector because this theory is from 10 years ago, the last five years ago like it's crazy doesn't produce anything. I don't know you have to be careful because this is a box where you have to insert assign, right? So when you see something happen in practice, if the shape that this predict is not true, maybe because the theory is wrong or because the assumption that somewhat input in the theory. Again here we have freedom to keep the same theory changes like the assumption. You could say instead of just take to be bounded by M, put the noise and assume it's zero. OK, or make this super fast. Then somewhat is the same theory just the different instances, right? Like you have like in physics, you have the table and the friction and the efficient factor to be higher or smaller. It's easy. So I think that's maybe what's interesting to think about here, how here just give you an instance that I'm advertising the fact it is just one instance of many potential instances of the same exact theory, only changing the shape of the same assumptions. Let's notes or faster decay as well. OK, I want to tell you one more thing, but it's hard. So what I'm going to do is I'm going to give you a very quick summary of it, OK? Because I want to use the second quantity, which is very interesting, OK, somewhere where that I did that, I promise you that the shape of signal was important, OK, The way it enters the bound is what is now this case called effective dimension of degrees of frequency. The following point Lambda, which is the quantity very interesting, the phrase of Sigma Lambda -1 Sigma. OK, think of this to make it to be diagonal. That's roughly counting the number of eigenvalues bigger than Lambda. Not exactly, but that's the so somehow if you fix a scale Lambda, this is saying how much stuff I have bigger than Lambda. If you're something go fast, super fast or it's even 0, then you know what's going on. If you have a big tail of anyone that would not be super super, super slow, which means that your egg looks very much like a bowl, then you're going to have that that is bigger. Small egg, small effective dimension, round egg bigger. Indeed it can be proved that if Sigma satisfy the condition I told you before, meaning it's eigen value, it can. This can be shown to be order one Lambda to the D Here the range of P is 01. This being you know, hard and this being easy. What do I mean? When B is close to 1 is the slowest you can get and still get some mobility which you want to have because you want to assume that that's space plus if B is equal to 0 that goes super duper fast. OK, and here why do you take polynomial And could take an exponentially decay thing and do other stuff, but you have to start from the case and then you only develop this theory in the polynomial case. That's the first thing I want to tell you about. OK, so this is the parameter that controls that allow the composition is a Troy and horse that is going to make the shape of Sigma enter our back. OK, The second Troy horse is remember that we consider cases where we have random variables that were bounded and we take copies and we get something like some I from 1. High probability being constant divided by N and you have the log again, I'm skipping the log, blah, blah, blah, so that we have something like this that was the quantitative number or large number a couple of times, OK. It turns out that if you know more, you can do more OK, Particularly if you know this. So if not only you have something about the boundedness, but you know something about the second moment of the random variable, OK, then you can do more. What you do is that you can get the second bound that looks like this. OK, Again, in high probability, it's something else. This says if you know more, you can get more. OK. And you see that here, Sorry. Before we got this, this is what you got with balmness alone. What we get now is C over north not square root of north. And the behavior like square root of north you get associated to the variance. This is unrelated to learning, but it says if you have a random variable which maybe have a huge interval but it's very peaked, OK, random variable again, this is the interval of values, OK, this can look like this or it can look like this. Which one would be easier to concentrate around this? Anything you look here is the means, OK? So this is much easier. How can you quantify this in something like this? Because it shows that the interval is keep fast by this 1 / n whereas the spread is going to be. So if the spread is small then it's much faster. This is the second ingredient to use to get upgrades. OK. And we don't have the time to do that. Great. This takes a while. Basically you have to be busy this decomposition and then use this kind of you have to use this kind of bounds, OK, but this part is very simple. And the main difference between this kind of bounds is that this kind of bounds. It's called trusting. This is called Bernstein, and you need Bernstein. OK, That's the big difference. You use the slightly different form of numbers and you have to compute not only the order 0 moment of your random variable, but also the second order. OK, this is super high level. OK, we're going to make this assumption. We're going to must type this a bit. We're going to use this result, then something is going to happen. What's going to happen is that here instead of Lambda, you can get Lambda, which means that here instead of two blocks here you can get one plus B, OK, Which means that's it, OK? Which means that now you can play with both alpha and B, OK. In particular, you can show well, efficiently, what does it mean before we play with it, B is going to get fantastic. So let's see first, what do they mean? You have two situations, right? You can say I move into a space which is really small and a function in the space which is really. That would be the situation where B is close to 1 and outside of the equation where B is very bad. I have a huge space which is no the directional of the state. But my function is very nice only depend on the 1st. So in some sense, you can do this as a measure of the dimension of the power of your space. And this alpha is how nice is the one function you're looking for, OK, this doesn't know this condition, doesn't know the condition of probability, doesn't know the labels you're looking for. It only knows the inputs, OK? It only knows how to build the space. The condition on alpha you deleted it sends it to your function. So in some sense one can see this condition on alpha, the sparsity condition, the animal is a kind of dimensionality condition. What you can see is that as you expected and expected the discussion maybe a second ago, if I put your Pi can do a whole bunch of situation and for example, even when alpha is equal to 0, if P is itself equal to 0, then the rate comes on over a see this is 0, this is 0, this becomes one again. Is this surprising? I don't know because I just told you somewhere and I deleted and in the final dimensional case, I do expect my bound to be ordering OK, you can do all the other cases. OK. The best case I guess is this one. You get 2-3. The best alpha and the best D will give you again parametric that you're anything dimensional. Anything dimensional problem where something which is the fastest polynomial you can really take equal to 0. I guess what what today I try to try, you know, take a step back on just doing a whole bunch of computation because show you a bit more ideas. Because here the computation, the first class, they were very simple. Again, they were tedious, but they were very simple. It was instructing because that's more or less what you do here. You basically the same stuff but with more computations with the moment you have to do this for the body and say to just do a few more computations. So we shifted the attention of the computation themselves to the role. So the first class I try like the mistakes give you constants. Yeah, OK. Because really I just want to see that not perspective of assumption. And again, the old message now is that program can be easy or can be hard. There are ways to encode easy or hardness, OK. And you can expect a different theory depends on how you're encoding. These squares are the playground where you really try to somewhat people out. Some of these is the lack of noise. Is the simplicity of my function important or not? Is the size of my space important or not? Here you can really ground with this idea in something that you can control. See tomorrow and discuss neural networks. The same stuff, but much weaker and much harder. OK, questions. Whatever you want, but I'm curious.