We. Introduce. Briefly. Different. Possible architectures and then instead of the section talk will be a bit more technical and I will introduce stochastic gradient descent is I didn't do so. Far so. It will be focused on framing on our network and we see stochastic gradient descent, the convergence behavior for convex objective functions, and then the extension optimization of empirical risk with neural networks. And first there are so bad computation of the gradient bad. This is the plan in the afternoon is that you will see bounds, learning bounds as you did until now. So we start from. Is to go beyond the linear models. You have seen already a way to go beyond linear models, so we started particular. I was assuming that our function explaining the data was via at the beginning. At least it was my main example. And also in the lectures by Ernesto you analyzed very besides this setting and then other lectures by Ernesto yesterday you passed through a more general setting. Is this 1? I start from the feature map. So you be the feature map 5 that goes from the space of your data into, let's call it MA different space of M Usually M is bigger than D You're seeing that this can be even infinite, so you can pass through kernels. But let's speak to that. The idea is to represent the data with respect to new coordinates using different features and to try to explain the data in a linear fashion on these new coordinates. So the dependence I think that the master told me that he yesterday is stressed. The fact that this is a normally it's a lazy way to go beyond the linearity, because we put non linearity in yes, but still the function here that depends on the parameter W depends linearly on the parameter. So the optimization still compacts everything works. We have usually this requires when you demand the learning algorithm using this approach. 2 steps. So. First you design the features, feature design that is done by an expert, OK. And then optimization today. So it's a really it's a two stage approach, OK. Today we want to to do something different. And the first thing that we do differently is the way we we need to induce nonlinearity in the dependence of the function from. So here if we start from here, what we need in order to make this function nonlinear, these two other nonlinearity. Here what I could do is to add the nonlinearity outside the product. So I do something like this. I take the nonlinear function and I apply my linear. OK, so this is a in a way, this is very natural at this way at this step. But so if we think that the Sigma is a fixed nonlinearity, this is a very limited function class. So I can write it down. So I have a linear explanation. Then I apply a fixed nonlinearity. The function plus that I can express with this. This idea is very not very interesting. OK, So what people usually do is instead. Of. Considering just one of this function, I can consider a linear combination of this function. So instead of this which is this sum J1 to M over WJ Sigma is a function plus we are interested. So we take the linear combination of M functions like this where at each for each index Ji consider a different linear function. So here WJ is a vector capital WJ is a vector. I take this color product with my data data X and then I apply a linearity, this is a real number and then multiply it by South color. So WJW, one, WN all belong to R This is a little different with respect to the edition math scenario, the methods, since here my class is really a parameterized class. So here the function, the set of functions that I consider is just the set of function that I can generate by choosing these M vector in AD and vectors in AD and these M coefficients in AD. It's a finite dimensional set of functions. Hi so let's first at the day is a non linear function Sigma that for the moment is a. Generation in the weight of the system they run outside. I don't know, it's a some nature by. I started from the most classical one already seen in the talk by one of the most classical one is the Italian way to pronounce it. I don't know R&U rectified linear unit positive part. So Sigma of Z is just the positive part of Z. So it's like that. So when I apply this kind of linearity to these to insert the Sigma here, what do I get? So here I have a linear function, I apply a Sigma. So I still have a piecewise linear function continuous. And then I multiply this by multiply this by coefficient and so I can change it a bit. And then what I can do, I can combine these kind of functions. So at the end in any case I will have piecewise linear functions and continues. So usually here I started like this to be simple, but usually what you do is that if I still consider this class still quite limited because here I impose just a linear function. What I can do is that I can generalize the set of functions in order to get a reach space by adding a so-called bias vector. So in general we keep it. Sometimes I will delete it for simplicity, but we have to think that usually is there. So what the usually is considered is this one. VJ is enough and then if we do this is the vector of VJ. So if I do this, at this point I want just to do 2 examples. I don't know if you are familiar with this kind of networks, but it's possible looking at them at first sight, they can, they can be quite, they can seem quite as in terms of expressive powers. So one main thing that the class of function that they can generate through these operations and this that I want to show you just with the two stupid examples that you can do quite a lot. The first thing that you can do is to build the functions. So this is easy. So I take X naughty. Start from this. And then what I can show for instance which is X + -, X plus minus X plus. So this is a function which is generated in this way, right? So they take WB equal B 1 = 0, W equal 1 and then B 2 = 0 and W2 equal -1. And what I get here is I should take the positive part of XI should have the positive part of minus X. And then what they get is this function here. So starting with this, I can I can generate B modules, the modules inside the class. And then if you play a bit with the coefficients, you can you can see that you can do many things, things like that, coefficients and number of functions that will be called. Murals. But keep it for later, OK, So you can generate rounds and stuff like that clear, you know what is this stuff? Maybe another thing that you can do something like this. These are the kind of functions that are very easy to generate even with the. So just you then rectified linear unit is not the only activation function that is used, just some some comments on its pros and cons which is nice of function is the fact that is simple and it's very simple to evaluate it very cheap to evaluate this function. A better that aspect is the factor that this function is known. So you have a front of known differential unity. So you want to. Place a computed gradient of something that would be inserted, you will have the problem of non differential unity. The thing is you cannot yet see what is the so-called neuron problem. And here is the following. This will become a more relevant when we consider deep architecture, but still we can understand more or less what's happening even in this simplified setting. So let's take B negative and consider this very simple function. So what happens is that when I do this thing and I take X negative, then this function is 0 because the argument of the Sigma is negative. Yes, I can do even better. You're right. Yes, sorry, I want this to be minus, right? That's fine for sure. It was true for me, but it's true also even like that. And what happens is that del F of X is equal to 0. And when I take the derivative. So this constant, when I take the derivative with respect to B, if B is negative, this is 0, right? This is a function of B, right? And. And then? This is a problem since every negative P it will be a special point in correspondence to XI points that are smaller than minus P. So this on this case is not very maybe relevant. But if you think that maybe X comes from a previous step like this, then it may happen very often that this case happens. So every B negative will be a stationary point. I do not update my bias spectrum anymore. The optimization is stopped and stops here. So other examples of nonlinearities take this one. We have seen this as well. In this case in the talk by Andrea he was using the sigmoid. So like this super observation, this is always positive. So this quantity belongs to 01. And so let me the picture here is almost 0 and 1A right. This is very flatter starting from these in these two regions. So why should I use the sigmoid function? Because Sigma is between zero and one and so can be interpreted as a probability. So this will be its main feature and it will appear mainly when I will have to do classification. Second stuff is second point is differentiable like it and the other point is that on the negative side is the fact that. So I was trying to show it in the picture starting very soon it's becoming, it's becoming very flat and so it's derivative will be very small. OK and. Again, for the moment we have one layer, so we are looking at the. If you are more or less familiar, a shallow network, you don't have more. We are not stacking layers. So the problem is that we see later. The problem is where we put many layers. So the problem is vanishing gradients. And then is the hyperbolic tangent. Quick question like problem. Different. It's used in practice because it's very easy and because we are the sub gradients. And so instead of gradients, we use the sub gradients. And this is a very interesting point since I will discuss it later. No, not the flying. I want to one once we have the maybe the algorithm. But it's an interesting point because usually you do rainbow and you differentiate rainbow and you don't care. You take just the left area either -1 or 0. So you are applying, you will apply the, you will differentiate the composition of radius. You will apply the chain rule to compute the derivative in a setting where the chain rule does not hold the theoretical. So this is this is done in practice. It works in practice. From the theoretical point of view, it poses many challenges. There has been a group of people in particularly in Toulouse trying to vault the vault, the group, vault the powers, trying to explain why this chain rule makes still make sense even if you cannot apply. You are not in a convex setting. You don't have the convex sub differential anymore, but you're using the convex sub differential of the railroad. So there are many things that surprisingly work well, but are not well grounded theoretically. And there has been a trend in the optimization community trying to explain why this works. Another thing that you can do is modify a bit to the rail. So they use the leaky reload, the reload where you do something like this. Here you put the a small, a small. Yeah, that's slope. Yeah, sorry. Is there any specific reason as to why they develop such a function for this purpose or? Yeah, yes. So we do some I think some even historic. We inherited it historically from the original model of the neuron. Yeah. So I will try to give a biological, let's say, explanation, not very biological, but just mention why it makes sense. OK, yes. Well implemented before stopping the character. Then you take Sigma, which is. So this looks this looks. 1. One and so this is the the picture. The outputs are between -1 and one source. In this case you could use it for classification purposes and it's a smooth and trashable. So I would say that it's trashable and it has a less, less than this, but more than the same. So the comments that we can make are very similar to the previous case. OK. The other one instead that is a bit different in the sense that it can be interpreted as a smooth approximation of the rectify the linear unit. There are many possible smooth approximations that have been proposed. Again some some of them were mentioned and again in the talk by Tom Pojo Jane who I don't know even all the possibilities, but this one is an easy way to approximate it. So plus so the Sigma is algorithm of 1 plus east to the Z and so basically what you are doing is something like this. So here it has a linear behavior and here it's going to 0. So you can really see it as a kind of approximation of the positive part. So the nice thing is that smooth approximate the rail because it's easier and then it's. But on the other hand it is never 0. So here we were complaining since this function can be 0, but this has also positive effects kind of specifying effect. Here I'm complaining because this is never zero, OK, and with respect to this one is more possible. OK, so a small digression on other possible ways to induce this nonlinearity in a similar but different using a similar but different strategy since they are related to have methods that you have seen yesterday. Functions function. So instead of computing a non linearity applied to the product, you apply non linearity to a normal and you feel the functions that look like that. This is a radial basis function and here your FI never wrote it. But these are functions that depend of course on the parameters that I that I choose. And I generate a class of functions by letting this parameter body in RM and RB. Constant. So as you can see, when I will insert this function inside the empirical risk minimization problem and I will optimize with respect to the parameters, what I will do is that I will optimize with respect to the coefficients and the centers. I can interpret these as centers when I do reproducing kernel interspaces that you have seen yesterday. You have seen the representative theorem that tells you that at the end you can reduce your optimization in any dimensional setting to a finite dimensional 1. So at the end you have functions that I try to mini this notation. Let's take that which looks like this. So it's a translation. So what you do is that if you use a space approach without a kernel here and what you will do when you will solve the empirical risk minimization problems that you will optimize these coefficients. But these centers have been fixed by the approach, OK, These centers need to be the training points. OK, Do you remember the presented theorem, the solution of your empirical risk minimization problem either regularize is a convex combination of the kernels centered at the training points. So here you see what you gain by using these a more general approach you want in the sense that you have you are eneraging the space of function where you are optimizing by giving yourself the choice of the centers guys the possibility of choosing the centers. Of course you pay for this bill since your essential problem will become non complex. The typical to make things more complete need a typical choice here of Sigma is of Z is P to the minus gamma. Compare this with the Gaussian panel. So far what I needed was to consider a linear combinations of nonlinearity very often when you. But So what should I do? Is it correct or not? So, and the point is that this approach that I described so far is fine when I want to regression. If I want to do classification very often I don't use function spaces like this. But this is the same that you do in classification when you use evenly. In the linear case, use your linear classifier and then you take the sign in order to classify. So here we do the same and. The point is that for classification so you as usual. So I compute my sum J1 WJ Sigma of WJX. But then I need to I need to interpret this new classification and so usually what they do is that I add a Sigma here and there are mainly 2 different possibility but more classical ones of the following. So if I do binary classification then I take this. That we have seen before. This will give me a probability of being plus one or plus -1 and if I do instead multiclass very often this will be. It's a different than this, but in the sense that the I will need the vector as an output and then I will do soft mux. This is more to tell you that if you do classification you need an outer nonlinearity, so you cannot consider the class that I will show it to you. Yeah. Not necessarily. This is in general too. So I will always assume that the nonlinearities are the same all across the layers when we do these layers. You don't. They don't need to very often. Yeah, but they don't need to. Yeah, but we get like a richer class if we consider like different types of no, no, no, no. Sure, you change the class, so I don't know exactly what happens when you combine the different activations. I guess you can be creative, especially in this case where the last one needs to be a small. But maybe maybe decided you can use different stuff. OK, so I want to, I know more or less I introduced the thing practically with the very practical construction, the class of function we are interested to. Now I want to try to give some interpretations of motivations, interpretations of what we did. And there are different. I start from the first one and India is again going back to feature representations, so and start again from the first, the black one I wrote this morning. So this one learning data representation. What I told you at the beginning is that there is one classical way in machine learning to go to linear models. That was this one. So let's consider functions that other so and I was stressing the fact that what you do when you do something like this is a 2 stage procedure. So you first design the features and then you optimize the positions. OK, this is the thing that we need to. If you want to classify images and you are like me, you've never done it for real, the first thing that you can, you have to do is to understand what some good features for representing images and you need the next step. And then once somebody has found the right feature, the right features, I can do the optimization. But it's really a 2 step procedure. One idea would be to say OK, I don't know how to build these feature representation, can I learn it from data? So basically what I could try to do is instead of keeping this for granted and trusting my aspect, I could try to do something like this. Now here I put the minimum on W as before but also 5 so this would be. This would be not a good stage to see, but something that is called an end when. So you give me the data data, I put my file here, which I don't know for the moment is an unknown or the problem. And then I optimize on file on that. And the hope is to find the best possible representation and the best possible parameters for that representation. As you can imagine, if I write things like that, that is the problem becomes immediately very complicated and maybe not too but very complicated because here I have optimized over file. So it's not clear what this means here. It is clear that W belongs to 1D and I can take it vary on the whole Rd. maybe putting constraints on the norm. But the function class? What is a good function class for teacher maths? Not clear. And one way to instantiate this problem in a practical way is to introduce the prioritization of teacher maths. So my prioritization is. And the one we are interested to is the following. I take Phi of X. Consider the set of five such that Phi of X can be written as Sigma of da, maybe 1X. I'm doing the same thing I have done before but I'm getting a different interpretation. So one possible way to what we are doing is that we are trying to learn the best possible feature map to represent our data. And in order to do it, we fix a function class for features, which is this one, right. And of course, I can add the bias as I forget it. So you can add the bias everywhere forever. Yeah. And usually it's fixed. So you fix the architecture, right? And it's part of the architecture. So usually you fix it. So it's the width of your network, right? So it's fixed, yeah. But it's in principle I would like to learn. Hi, no, yes, it's just a different way to I think it's a nice interpretation of what we are doing. So the fact that very often you heard saying that neural network find a good representation of the data and this is a way to give at least an interpretation of why you have to look for interpretation of this class is good or not practices tell us that is good. OK. So we so as I was saying before we replaced this tissue design when you need to design fixed wave that's you know, see bag of words. I don't know all these packets was very active and still until a few years ago where there was this research to design good features for a given data with this end to end, so I don't need to do this anymore and at. This stage we lose the. Interpretability tool right of the result. Yeah, I mean, until now we have this, we have this Shiloh network where we have just one lady and maybe, yes, they they are still interpretable. This will be my future map. I can try to have a look to it and understand what it's doing. Yeah, for the moment, I would say that also interpretability is more or less preserved. OK, we lost I think more when you add the layers. So OK, yeah, this is a the one possible idea, another idea and motivation is that why you do this? And So what we want to do now is to meet a neuron to introduce finally the terminology that I was trying to avoid until now. So the next step. Off again. So I will write a graphical interpretation of what we did so far. So we consider and keep the balance again. So I want to understand, make a picture of this function, give a graphical representation of this function. And usually it's done like this. You are X which is a vector in Rd. and I write it here and then you are the one. Here I can and then what I do of these components of my back here what I'm doing is. So I think my X my component XI multiplied by W 1 here. Here I multiplied by W2 by WD here I multiply 1 by D So these lines means that I'm multiplying this X1 by W 1 and so on. And then what I'm getting is something here which gives me the sum. So the sum operation and they get the source sum, right? And then what they do after this, I apply a nonlinearity let's put and I get my let's put it what? So usually when you speak of their network and neurons, you see pictures like that and it is just a graphical interpretation of the function, the motivation of these. So usually these sets are collapsed together. So here I put them separately but. You see them? Together, and I guess that the motivation comes from a biological interpretation of the neuron as well. This is an artificial neuron. The model is due to make the Carlo can heat. It's quite, I think 1943. And what's the biological interpretation? So I will try to say something not an expert at all, but what you do is that you try to be the very simple representation of a true neuron in our in our brain. What does a neuron do? So neuron receives inputs, so from various sources. These should be maybe other neurons or inputs. So external, external excitations and the So each variable acts on the neuron arrive at the neuron with a different strength say. So the strength of the is given by these coefficients here. And then what does the neuron do? Is it mixes all these stimuli and then it is activated or not if this sum is positive. So if this sum is positive, the neuron let's say release energy, so it's activated otherwise no, right. So you have a certain threshold, you have to put it that's zero, but can be a threshold above which the neural is activated otherwise. So this is the meaning that the biological application original maybe was a zero or one would be OK, active, non active. But this can be more or less the explanation of. OK, sorry. Yeah. Why? The. Why do you introduce me? I introduce me as a. Yes, this is a so from the biological point of view. So yeah, it gives us, so it give us the different threshold over which this this neuron is activated, I guess can be a pressure when it is activated. So I, I said positive because I was I'm always thinking to this setting. But if you put the you are allowed to the pressure above which is activated. Sorry. Yeah, it's like that. I'm sorry for the temperature. I, I I understand you now. You see also why this function Sigma, they often is called activation. So this is an activation function and this is called. So what happens is that when passed from one neural, we consider linear combinations of neurons. Listen, that is used to be a kind of a model of the biological situation where we don't have just one neural, but hopefully we have more than one. And so you see these pictures to represent the function that we have we have seen before so these ones. So the inputs are always the same. I have my X1X2XD and one. But now instead of having just one neural we have many. For instance let's take 3. So here M is equal to 3 and what happens is that all my input variable act on the 1st neural like this. So here I will have my West 1 and then I will take 11. 2. One and then these input variables are not only on the 1st neuron but also on the 2nd. Here we add B2 and here we add the other weights. I don't write them not to too much and then they act also on the 3rd row, right? So here I did, I did the three functions Sigma of West One X + 1, Sigma W 2X plus T2, Sigma of West 3X plus T3. And then what I need to do is to linearly combine them and then here I add step which is this one and they get much. Out I don't. Know here there is a sum, here I will have W 1 W 2. So let's start to give names to stuff. This is for the input layer, input or input layer. the Y is the output or output layer. And then we have all these coefficients parameters that are called parameters or weights. The D are called biases I already told you, and the other one are the weights of the network. So let's put it all together. The terminology I forgot one layer which is this one which is the hidden layer. OK, this is called the hidden layer. I've already told you sometimes before is a very special neural network. This is the first neural network that we have seen since the beginning. It's a special neural network with just one layer. And this kind of networks is very often called shallow network. This is a shallow network and this will be compared. To deep. Networks fine. The second other technology, the number of neurons that I have here, which is M is called the width of the network. Activation. So the this class of functions is obtained by letting WJ small WJ capital WJBJ body on the entire space of R where they are. So they there are no restrictions. But very often is that you like to put restriction, you would like to put restriction on the positions and these are these kind of regularizations or restrictions have a specific specific names in this context. So you have this is now I put together and say the class and the non linearity. I I put them together. Usually it's like that. You don't even put the plus. So I was putting it at the beginning of the steps that we need to do. But usually they yeah. So as I was saying, sometimes you like to see the functions and I call this class of functions. I never take a name but. And then here what you do is that as I was saying, usually this WJI belongs to Rd. and BJI is enough so you don't put the restrictions on the coefficient. But sometimes, and when you do it, this is called for instance weight decay. So you take the SpaceX, but you constrain the weights. Constrain the weights normal not of the value of the weights or you do a different stuff that is a bit. I know something in the sense that you consider the same class, but you parameterize it differently. So you you write the functions like this. The class is the same but so I need to. WJ and then here I put the Sigma of Rho JDJX plus DJ and. Here. The WJ is as before belongs to other but instead rho J belongs to this positive or zero and DJ has known one. So this is called the weight normalization. So. The function class does not change because basically you can always write the vector as the product of its normal times the direction. So the space does not change. But when you, what does change is the application of the optimization procedure. When you you will apply the minimization procedure to this parameterization, you will get the trajectory which is different from the one that you applied. In this case you will obtain the wrong entity and you will follow a different path, right? This sometimes called like in practice layer normalization or. I don't know, maybe I don't know. I usually call it weight normalization, but. The important thing is that why this operation changes the class. This corresponds to any parameterization. Now more technical, that is useful for the class of these after. To. See then the more theoretical properties. So you here we constrained our class by imposing the heart constraint on the normal of the weights. What you can do instead is we set the regularization when you minimize the empirical risk as we did in the near case. So instead of considering the immunization of the empirical risk, you can see that the immunization of that's right here the empirical risk. But now, now we depend on WW and D I do the usual abuse of notation. I replace the function with these parameters and then here I will add the regularization. And the regularization that I do is this one I can do in a moment. So the point is define and what I want to do first, that is a before defining the regularization is a remark. The function is one of genius positively only one for positively homogeneous, which means that. What does it mean? It means that when I take the positive part of alpha Z for alpha positive, this is alpha the positive part of Z. OK, you agree. So it's Z multiplied by positive means -0 is positive, which is alpha Z is positive, the positive part is alpha. OK. Therefore, if I define. What does this implies? This implies that if I consider my function F and now I need to I want to explicit the depends dependence on the parameters. So if I take the function here that depends on on my vector small W say my vectors big West and the V and I evaluated the effects, this is the same function if I use the ray network so Sigma is the one. This is the same function as the following one. I would say F West and then here I want to be a bit more precise. Sorry, A1 W 1A2, W 2A and. West. I write it explicitly because I need a different coefficient for any element and then here I need to put W 1 by the A1 1 divided alpha M. And. B1 divided A1. Maybe it was easier, I don't know. If you see it, it's better to write it. And then I can write this in the following way. Here I can divide by alpha J. But if I divide by alpha J here, this will be one over alpha J that is 1 because it's positively homogeneous. And then I need to this is the same, right? So this is what I wrote here in a maybe way. Let me give a name to these things. I call this Theta all the coefficients and I call this Theta alpha for a vector alpha. And So what we get is that F Theta is equal to F Theta alpha. And So what I get is that when I compute L hat of Theta, this is equal to L hat and Theta alpha. You agree it's just a writing of what I already. Said. And then when IA very common, very common regularization is the one where you regularize all the words so. J square plus square square. So usually here usually will do like that. CWJ here is multiplied by X that will have a certain norm. Let's assume that as norm. K. And BJ is multiplied by one. So you want to put the weight balancing these two regularizations taking into account the fact that WJ may be multiplied by very big numbers, quite J knot. So usually what you do is that here you divide by a parameter that is related to the normal XI. Think you call them Ki. Try to keep the same notation we made. This should be the notation that we have seen in the previous classes for the norm of X, the individual. So I assume that this is related to the norm of X, the norm of my data. And this is the regularization that you use. And then as you see, you see that these regular. Why the implicit risk is blind to these kind of changes in the parameters? The scaling of the parameters, the regularization is not. Therefore, once I hit the parameter, I could look at all possible that the parameter is scaling and take the one that minimize the regularization possibly. So I write the problem here that is this one. Basically what I do is that I fix my parameters, fix one, I choose one and then I minimum over alpha that is A1A north of what of alpha plus Lambda of? Here I wrote down but I was wrong. Sorry, this is the older notation. I will minimize them for all the possible thetas. But first I try I start to minimize respect to alpha to avoid the redundancies. As we said this is mean over A1A north of and hat of Theta because this this does not see the alpha plus Lambda J of Theta alpha. But then I can take this minimum and put it here and I can first compute the mean with respect to alpha of my penalty of my regularization. So what I do that I take this one and I can put it because the first part is independent or not. And this is A1 dimensional optimization problem. So I will not solve. I will tell you the solution so you I can write the corresponding regularization. So J of Theta alpha is the sum of. What I should do is alpha J square, WJ square plus WJ square divided by alpha J square, DJ square, alpha J square right? So as you can see nicely, this decomposes in the all different components. So you have to solve 1 optimization problem for every J and I can find the optimal alpha J square fine. And so you find that the optimal choice should be. Write it here. I wrote it here. This one you don't need to find alpha J, but alpha J square is enough. So this is and I forgot the K. This is the optimal solution of this problem. This is the optimal alpha J. Now that I found the optimal alpha Ji can take it and insert it here to see what the regularization looks like. OK, and what you get I want to skip the computations is that you get the following. So plug in alpha J. Play. And it is true for any W and WW. And. So what? Yeah, sorry. So what 1 can do is that since we are then optimizing here, what I can do is that I can start. I can start from the Theta that I want and then I will have all the possible scaling. I want to start from the Theta for which this norm is 1. I want to When I want I look at all possible scaling. I start from a Theta that has this point equal to 1 so that and I will not change my. I will not lose anything when I need noise type as the minimization of the first problem. So I do the minimization on WWD. So let me call this big Theta of L Theta plus in the set of DWD such that this quantity is equal to 1. For every gel. So this is what I told you. So as you see magically somehow then we started from a square normal regularization and looking at exploiting the positive homogeneity of the radio application function, we derive that you need our regularization can be reinterpreted as a an L1 regularization. And since we are optimized just love L1 since we know that it's sparsity. So this WT is kind of regularization. This is a possible way to explain at least, because we don't have. I'm not. Aware of any quantitative result saying that it didn't specify. You can say how sparse would be our result, but we know that and one norm like pen induces sparsely. So we expect the solution for which many of the efficient W will be 0. So a combination of a small number of numerals and this is also the more robust with respect to the other comment that is usually made. So I'm not penalizing with the square norm, but I'm penalizing with the L1 norm, which grows less faster. OK, so this will be the starting point for the this afternoon and now in the time that it's I want to do is 1st so I want to go to go. So I got the definition. So the idea, as you can imagine, is to add the layers, add the hidden layers to compose what we already did, to have the deep representation, deep transformation of our input so that is obtained by composing different functions. We can define it in several ways. We define it in a cursive way as again we have seen the lecture, so it will be exactly the same. So let's write the definition I fix. So here I I realized yesterday that there is a crash of the notation, so of course I was trying to be careful. But I use the usual notation. So Big L in this definition is not the loss is the number of layers, right? L is suggestive of the number of layers. Then I call. I need some dimensions that will be the dimensions of each layer. I call them B0 DL plus one. So which are natural numbers, not 0 natural bigger than one. And then I need an activation function. What is a deep neural network? Is a function. Is a function F that goes from RB0 to RB plus one. Such that F of X is. And here you see, you see the recursion H of L + 1. And now I need to define. I need to explain you how I arrive here. So this H will be a function of X of course. And the definition is a recursive definition, so I start from HC. Well. H0 is X, so I initialize my dynamic recursive procedure with the point X and then I define H1 which is Sigma of. I don't want. I want to do directly HL HL which is Sigma of W L - 1, HL -1 + L - 1 and here L goes from 1:00 to L and HL plus one is. OK. So let me explain a bit better what these things are. So I need to compute F of X. How do I do? I define H0 which is X. Then I define HL which again be careful with a bit different with respect to the previous notation. This is a vector that belongs to RBL and how it is defined. So now Sigma is an activation function that acts on vectors component wise starting from the Sigma. So Sigma will apply one component. I use the same notation again abuse of notation. WL -1 is a matrix that takes the previous input and transform it in another vector. So WL -1 should be a matrix that goes from takes vectors in RTL -1 and give us vectors in RPS. So I don't write. So what should be DL times -1? It's a matrix DF now is a vector. The L -, 1 is a vector and belongs to DL of that. So what I do is that I do my linear a fine transformation and then apply the Sigma component wise and then I repeat. I repeat this procedure Big L times, Big L times, and once I am at the end, I have the last which is a fine layer, OK, Each operation is a layer, so each step of this procedure represents a layer. So Big L is the number of layers we can do the picture and then I can introduce all the location even if you already have the terminology. Picture. In a special case where the zero is 3, so yeah, X1X2X3 and so this is H10H30H30. I have my I put them here and then I think the the four. So this is D0 here. I think the D1 equals 412. 3/4. What happens is that I move an affine transformation of each one of these, so I put that one as well. Or maybe so until now it's like 4 and then what they do here I pass from T2, from cheese 3. And. I don't know if I get there are too many something like this which is. 1/2. 3/4 the picture and the output can be. In this case I have. In general, right can be as color as before. So in this case the output is a rectum. You don't care when you do them or a network, they're very good to have vector value outputs. The last thing is so these ones again input layer, output layer as before these ones becomes the hidden layers. The matrices are called the weight vectors and all these matrices are called weight vectors. This is the bias factor are the bias factors and all the parameters together are called the weight of the networks. One possible way to to introduce to to represent them more compactly is true by writing explicitly the composition. So sometimes you see the the composition of it is L in the width. That I think is the maximum of the values to tell you that some additional technology that I forgot L that then there is. So if L is bigger than one, these are called big. If L is 1 as before, this is shallow. And then I forgot to define another important thing that is the we did. So how big are the layers 1? When I put together, So this is related to the question as before, can I choose M the number of neurons? So when I put together all these parameters, the Sigma, the D1. This is the architecture of the network and is fixed. Usually when we optimize for a training error, we fix the architecture. So we fix the number of parameters, how they interact and then we optimize. This is a let's say again an expert choice. Again, the expert come into place and when learning, but still I need some interpret some choices, right? OK, so a more common few is just writing explicitly the composition. So when you back together all this stuff, you can insert into here the fact that HL is of this form, the fact that HL -1 has this form. So again, it will be, this is just a way to unroll the composition. But the F of X is the composition of several operations. It's a elementary operation. OK. So questions, this is the question of if you show this question, it's very, it's important to science because as, as we have seen, yeah, in the, there are, there have been studies that try to explain why we should consider networks instead of shallow networks, even if the approximation capabilities seem similar, are similar, can approximate a lot of functions. And so the question is why deep? And one possible explanation which I will not end because I I don't know much, is the compositionality, which has been called compositionality. So the idea is that these kind of functions, these kind of methods that are obtained as compositions of simple element, I say layers can be used well in order to approximate functions that are obtained as compositions or functions that depend on the small number of variables. So the example is just an example. So we are in 3 variables as before and I get a function that is the composition is a function of three variables. But you can write it as a composition of functions that depend only on 2 variables. And one depends on variables, have two depends on variables, have three depends on variables. And so the hope is that instead of incurring in the curve much the dependence of the dimension which is 3, I cannot forget 1 dimension. And this can happen for dimensions. And this can be, this is for sure a direction that the research has taken in order to try to understand why that can be abundant. Another no, I should stop, but that's fine. So this is the very basic. These are the very basic. This is a thing for neural network. It's the. 1st. Elementary architecture, there are others so that we skip I mean, I was not going to say a lot going to say a lot, you know, out in orders the current neural networks, convolutional neural networks. So once you start from here, you can change a bit what you do here, you place this with a convolution or change the architecture and you get the other other networks. So the zoo of neural network is very big is changing every day and depending on the application, you may choose one or the other. And so I think it's one to stop having take out an hour as a break and then we start again at 11:30. We want to be my piece with respect to all the parameters of the network in this case. So first thing I will forget about the regularization. What I'm going to say basically, if this is a constraint or if it's a regularization, you can treat it if it's smooth by inserting inside of the gradient. If it's not smooth, you can treat it as a proximal. So it's a just for simplicity that I'm keeping it, but it's not usually a problem, right. And then so we are we are left with the minimization of empirical risk. Just one thing that we could do is to write the implication the gradient of empirical risk, which is. Function. I changed the notation because OK and VR Theta is L of. So what I want to do is to. So what I want to do is what people do when you apply a, when you want to solve a machine learning problem using a neural network is one of the cons of neural network many, many parameters. So ETA, it's really very eager. So you can use a gradient first order method, but you have also many, many points. So usually you apply this on very large data sets. And so in order to do a step of gradient center, what you need to do is to compute these gradients that maybe are 1,000,000 of gradients. So what happens is that I utilize my method, then I compute 1,000,000 of variants and away, away, away, and after 1,000,000 computations, finally I do. What's that? So the idea is that can I do something in the meantime? Does it make sense to wait for 1,000,000 computation? Or instead while I'm waiting maybe I can start to move? And it is the main idea which is behind stochastic gradient descent, where what you do is that, OK, let me do this one very aggressive stuff. I forget for much, I forget about the sum and I just move once that I see the computation of one single gradient, right. So it is gradient, it is stochastic gradient cell. Very simple. So now I would like to to explain a bit how you can do it and I will show you the convergence results that are known in the combat setting. I apologize for that, but this is what we know. We can say something in the combat setting. In the non combat scenario there are some partial results, but you can say very you can start to say something. So there are some theoretical results, but let's start from basic basics. OK, so again SGD is just this iteration here. At each step I need to choose 1.1 point in my data set and there are various ways, various possible choices under which the convergence has been studied. The first one is the maybe the most natural one which is the sign. So I have my data set which is ordered somehow and at the first step I take the first point, I compute the gradient of the loss and then the step second point and so on. So if I add 4 points to be like this, right 1234 and so on, this is cyclic and this is a deterministic choice of the data that I'm going to process. Second possibility is a stochastic choice, and the most common one is the one in which you you have a finite number of data and I choose uniform. I choose one uniformly at random. And then third possibility. There are many others but these are the most random shuffling. This is the following to first step but it's in the first round. It's the first will be called echo. So I will choose my points as they are maybe so I will do 1234 and then when I finish my points I do a random permutation of them and I start again. So the second echo will be 2314 and I finish my point, I do another permutation and I start again, right? So this is something in between a cyclic order and instead a random choice because it allows me to state that I will see every point in each echo. This is called an echo. So every point will be seen every N iterations or less. Each block of N iterations I will see my point. So then then there is a convergence theory for all these three choices. In the second for sure I will focus on the easiest one which is the 2nd. So. The results are different. This. Is nice practice, but I will focus on this one which is easiest analysis the results that you get. Sorry, maybe I should say something more the results that you get in the psychic cases are over in advertising in nature. So are better because you know what for sure, but are usually worse than the one that you get in the Yeah, yes, so. For the random reshuffling, if you have four data points, you do a forward pass for one. So say you in the first epoch you do a forward pass on data .1 and then a backward pass for the. Moment we spoke about forward, backward. So I compute the gradient sound for one, then I compute the gradient for the second point, then I compute the gradient from the Third Point, then I compute the gradient for the 4th point and then I shuffle OK. But still here that propagation does not enter in the picture. This can be done even for optimization of sum of functions. This is just a sum at the moment. I still I never use the fact that I'm optimizing with right? It's just the sum of functions. Oh, so you're shuffling the data points? The data points. So this is this is. Sorry, this is just a discussion on the choice of the data points at each iteration so I can do it one after the other in a secret way. The order is fixed at the beginning and they keep this forever. Otherwise I can choose one at random at each step and then I put it back so I can choose it. I don't 10 times in a row the same point and then otherwise I can do this random shopping which is something like try. To. Do that point. If. You do it like for a very large amount of people. Aren't they all equivalent to each? Other Yeah, and that's what I was saying in the sense the yarn, in the sense they all work, but the yarn, the analysis. So the analysis for the secret method is different from the stochastic one and the convergence rates that you can get are different in general worst. But on the other end, what you get here is that the convergence rates are for sure. So I know what will happen here instead are the convergence rates I will describe are in expectation. So of course the expectation I do maybe better. And. There is a difference between the two. This one is much more recent and it's more complex to be analyzed, but in practice is better. The rate of the same but in practice is what is better. OK, so I don't want too much into the practical events because I I there is there is that I say something which is false. But one fact is that this can be not so easy in practice. He's saying, yes, I trust him, right? Because it's not when you have many, many points not swinging around the shopping of your points, right? Maybe this one? So. Why is the algorithm stochastic even in the case one that is? Determined. Yes, incremental. Maybe there are if you want to know more, but so yes, I mean they don't think the the terminology here is blood and you refer to stochastic. They sent it to every other way that chooses one someone that yeah. So we're seeing the. Same data. Points. We see the same data points but in a different order. The idea is that a given order can be detrimental sometimes for your convergence. Some orders can be not soon. And he said if we shop for you, you can. The reality is expected to be a better yes. Evaluating the gradient for a fix Theta or we are moving after each. I write the algorithm again because I think it is better. So you have this Theta 0 which is in our very big and then you have Theta Phi Theta K + 1 which is and D of Theta is the one over north sum DI of Theta where Pi. Now what case will be OK? So the point is that each step I need to choose one index that corresponds to one data and. Even in the way. Of doing is still stochastic because they are making the radiance at one point because they are. Moving right. Yeah, yes, this is true. Yes, yes, yes, yes, yes. But but I cannot take an expectation. So I agree with this observation very often these for Italian, but I agree now I, I, I still need to tell this right? Yes, yeah, yeah. OK. So just before stating so, I tell you what analogies, what will happen is that my algorithm that this one used to minimize this thing here. And So what I will need is that K we go to plus Infinity, which is an obvious conservation, of course. So I want to stress this fact because basically I want to minimize Implica risk. I have a finite number of data and I will my Implica risk by doing an infinite number of efforts. OK, Now I want to stress this because I want to give you here another view. And maybe, yes, I can tell you what you were saying, I think. So what I want to do so when I do, I solve a learning problem. I think this is a you should have learned this many times since Monday. You don't. You never want to minimize, never want to minimize if you are not interested in predicting well on the data that you observe, you want to minimize an expected risk which is I will use this P notation again. So you want to minimize this OK, where X&Y now are random variable. So this algorithm that I defined here. So I wanted the objective. The goal is to minimize the having access only to X1Y1, XN, YN which are IID to have copies of XY independent copy of XY. So basically what I can do, I can write the stochastic grid in the center also for this problem and I can write that say this is my. The same location of the blackboard of the other blackboard. What I can do is that now I can write my stochastic gradient descent step where BK is a loss, internal loss related to this step K as here and I can write this algorithm which it would be the same what I do. You see K it goes only on keep N. So I visit one point after the other and then what I do when I finish my points, I need to stop. So if I do this iteration and I let K going to plus Infinity, meaning that the number of points that I have at my disposal go to plus Infinity. So I see one data point only once then the exact I will tell you will allow to say that this algorithm is also minimizing is expected loss. OK, so this is again a way is the first motivation online of the thing that one the four that one often hear the fact that progressive gradient descent is more robust noise. This works better than gradient descent. There are many possible interpretation, but this is 1 interpretation. So I can use stochastic gradient descent in two different ways. One looking at the data many times, multiple times, doing multiple epochs, more passes on the data and here I do just one epoch. I have so many points that I cannot even dream to see the point twice, so I see them only once. And then what I do is that magically, instead of minimizing my empirical risk, which I don't care of, I'm minimizing my expected loss, which is that my 2 objective functions. So this is a this is a field of stochastic optimization which is very much needed to learning. There have been some very old papers by the interpretation. So the main point is the observation that somebody was doing before the fact that I call it stochastic for any zone, and the interpretation is this one. Why? Why is it called like this? So let's put our self as I was saying before in the case 2. So IK is a sequence of random variables that are independent and are uniformly distributed on one end. OK. And then I can take the expectation of this gradient, say. And if I take the expectation with respect to this random variable, this random variable is uniformly distributed on one end. And then what I get here is that. So IK takes the value I with probability 1 grand. So it's expectation is exactly this one. And so wow, this is the gradient of my objective function. This is a stochastic estimate of the true gradient. So I'm replacing the true gradient with a stochastic estimator time. Yes, sorry. So in practice, regarding the previous point, in practice N is not infinite, right? I. Agree. So if we even by doubling the the number of times that the model see the data, we're moving towards overfitting. I mean as long as it. So this opens the chapter, but yes, I'm trying to minimize. So if it means overfitting, yes, even with 2-2 passages of the data, I'm going towards overfitting. Do people do multiple passes to minimize this? Yes. OK. So in practice you always do multiple passes, OK, Even if theoretically you cannot say that you're going towards a minimize of this, there is there are some results also here before they read from the multiple passes over the data. So they are not so harmful. You can see that they did they work, but it's not clear they help or not. Yes, XIYI. This XIYI are independent copies of this random variable XY which has a certain distribution which I don't remember how you call that pro pro. Now this expectation be careful is on the only random variable that is here that is IK. This is the only random variable in this expression. Is IK also Theta K if we need but I'm commissioning Theta K, You can forget the A and Theta. Just take the expectation of IK. So. Here what I can say is that if I take this discrete random variable and I take the expectation, I get the gradient of the implementers. Here what I can say I will not prove it, but you can. You can try. It's not we can put the suitable assumptions nor the 12 that the gradient of the DI at the Theta is expectation over. Let's write it like this. So here the expectation is respect. Yeah, the only which is present here, that is this one. And then you can, if the you have to, as you see, you have to take the gradient of V and move it inside the expectation. So as long as you can do this exchange to move the gradient, this is an integral that depends on the parameter Theta. As long as you can exchange the integral with the gradient, you are fine. So you get that the expectation of the gradient of the loss is the expectation of your risk of the is the gradient of your risk. The gradient of your risk is the expectation of the gradient of the loss, right? So I want, I do not want to focus on this, I will abandoned this now. But all the results that we see work for this simplified setting and also for this set right here you just need this technical result. You need to pass a limit inside it. OK, yes. One more question about the. You mentioned the one of the data durations and in one pass, yes, they go up to the size of the data set, but they're still drawn. The data points are so this one, so I didn't write it, but these are, I think you said this IID in the sense that all these data points are realizations of independent copies of XY. So X1Y1 is independent from X word or the others and they are. Copies of XY, yeah, I mean. Yes, the one which I counted. Then I can choose another one, but not clear why, right other questions and then I I can go to the the theorem and as usual. We have. Assumptions that are not always satisfied. I keep the notations there. I give a name to this guy here and they call it the UK and then I write this theorem. So that needs some assumptions on the and the functions and in particular we are in the convex setting. Assume VI move from we said RB to R&R, convex and L. This is the same setting as the first class. In the first class we want to apply something that is similar to a gradient and so if the gradient is center so we need convexity and smoothness meaning that the gradient should be L. This constant is the same for every BII assume that the constant is the same for everybody. Then I assume that the second minimizers is non empty so they are mean over Theta is different from the other side. And then I assume I need an assumption on the behavior of this quantity here which is an estimator of the two gradient. The two gradient I need an assumption of an assumption on the variance. So I think in the finite dimensional setting on UK and you trust that 1 / n sum I-1 to N, this is the gradient of Theta minus Theta square. Whatever, we see how to relax these assumptions. In general, so if we want to be a bit more general, I need to consume this on the estimate of UK. So I want to use another UK. I can that's inequality satisfied. So I like it for one moment here for generally I need to assume that. So if I want to use another estimator, it is not this one and it was inequality. And of course that the expectation is the gradient. This to say that I can generalize the theorem to different choices of the estimators. Here, let's focus upon the one that we started with, and then I need the step size being sufficiently small and more precisely, then we get the inequality. I put a square. I'm not sure this square I have to say I was the comfort but maybe. No, sorry, this is not an X square. OK, so a bit of an explanation of these of this inequality that I wrote here. I was a bit, I'm reciting notation. So this B of T, the K, the result that you see is very similar to the one that we got for subgraded, for the subgraded method and indeed the stochastic gradient descent which is not the same method. So we decided the last time. So I told you that when you study the convergence of the method, which is not the same method. You. Usually study the convergence of two points. Is either the best iterate 1 so the best iterate so far and there is still something that is missing, that is the mean of the I of the B of the I and then otherwise you study the convergence on the average iterate. So T GABA K is sum gamma I die divided by the sum of gamma I and TK. So the result that I wrote here is wrong because it's easy one important thing that is the expectation and the minimum. Sorry, that was correct. So do you remember these two things, these two guys? OK. And then, so let's have a look at the result, first observation, the result that is in expectation. So I cannot prove anything more and I'll tell you more in a moment. And so I want to bound the distance of this quantity to the meaning of B and I can upper bound this distance, this difference with the 2 terms. First term depends only on the initialization. So it depends on how smart I was in guessing the position of my and. And then this initialization error decreases in with the number of iterations and the decrease depends on the choice of the step sizes. So in order to make this going to zero, I need that this sum goes to 0 so As for class grade descent, so that subgrade As for subgradient descent. This quantity must be not sample. The steps must be sufficiently in order to make this quantity. Second addition is that is a narrow term due to the variance of my estimator, the fact that I'm not using the two gradient, but I'm using an estimator of the gradient, the stochastic estimator of the gradient. And this term is multiplied by the variance of this estimator, multiplied by the sum of the steps, squared so and then divided by the sum. So again, I want the denominator to go to classicality as fast as possible, but I want the numerator to go to 0 or at least remain bounded. And so the only thing that I can, the thing that I can ask for is that either I ask that all this quality goes to zero or more simply, I guess that this is bounded and this goes to classicality. So I ask that this sequence is square sumable but not sumable. We have seen already these solutions for the sub gradient method is the same, same comments but in this context these have a name. These conditions in order to have say convergence, I need say. Infinity sum of gamma I square equal to Infinity and these are called moral conditions. Robbies just because these are the conditions that have been found in the first studies of stochastic gradient descent that were due to robbies tomorrow I think more or less around the 50s. So we are so I if you agree I would skip all the possible comments about how on how to choose the gamma i's because these are really the same as the ones that we made for stochastic sub gradient. So usually you choose this is set size 1 / sqrt K is the standard choice or use a constant one and then do whatever you want. OK, So I, I skipped this remarks and generalizations. So first remark is about convergence. So I said that we cannot make more than convergence in expectation. So these are the classical results. But recently there's been a quite strong interest in trying to prove high probability convergence results. So instead of assessing, say, convergence in expectation, I try to see if on the single run of my algorithm I can see that that probability that we go. This is much more useful as. An example SO. We are called the high probability convergence results. Then other things. I stated that the theorem for smooth convex functions, but you can do the same for stochastic subgradient. So you can use the subgradient. Subgradients are OK, but you need to add the you don't have these assumptions and these assumptions anymore. You need to add additional assumptions and usually the assumption is that the the country which is the IRP conditions and yes it must be bounded. So you need to change the assumptions but still the same. Not the same proof, but the same bounds are good. Then I think that I said in the beginning these words, but if I have a regularized problem, I can add the crops. In front. Same proof as this one, so I can add the crops and. Then the. Comments. OK, so you can. What you can do is that you can generalize the bound. The fact that the variance is bound, which is super restrictive even for the square loss exit problem. So basically you can generalize this condition here. So I assumed that this is bounded. What you can do is that you can assume much more general conditions. So you can assume that instead you put an A and then here you put B of Theta K minus the mean and then you cannot be. There are various possibilities. You can have the B dine graded B and beta K square plus C Something like this should be more or less one of the one considers. These are called the ABC conditions. OK, so it gives you more freedom in the error definition. So. It enlarges the set of functions. Yes, another thing that you can ask. So I insisted quite a bit on strongly convex problem. Say that strongly convex problems are much better and much easier to be minimized. What happens in this stochastic setting? I get a strong problem, and the point is that the error term, that is the fact that we have the error term which is multiplied by Sigma square prevents the possibility of getting linear convergence rates. And what you can derive in a strongly convex setting is just the convergence rate. That's right rate. So PK minus P which is a smaller 1 equal than log north of K + 1 K plus one. So you pass from one over log N over square root to log N over K So you get you. You are still sublinear. In a strongly convex set, you don't gain as much As for the gradient descent. Yes. So it is clearly clear, maybe not so clear from what I wrote on the blackboard, but from here the fact that this problem is due to the variance and the fact that the convergence rate that we get is due to the variance. That would be beneficial to find stochastic estimator of the gradients which have a small variance and ideally which have a variant variance that go to zero. And this motivated again a lot of research on this kind of things. For the support variance reduction methods, Just mention them because I want to do that properly. Sorry if we don't take one point, we take several. Points. Yes, yes. First, the easiest way to reduce the variance is the suggestion of your here. So instead of computing an estimator on one single point, I will I will select M points at each step, and I will compute an average of that point. Let's see the right energy. 1 / m the sum right in IK of gradient of VI of and then here IK it is made by P1K EMK which are again uniformly picked at random. My training sector say with the replacement twice the same point and the proof is that the expectation of this one is the gradient and nicely the Sigma square becomes Sigma square over right. So indeed I reduce the variance when I do this name is called mini matching and this is called a mini batch. So we have stochastic. Gradient. One point at each step, Batch gradient, The two gradient, mini batching, mini batch gradient. I take some points and points. So what I will do is that ideally I would like that today. So I just want additional comment. This is nice, but it requires more computations per step. So first step, instead of computing one single gradient, I need to compute M. But if I can do this computations in parallel and then it's a better convergence in the better estimate during the same time. OK, so this is also related to the mini batch size that I have. Then the the observation here however, is the fact that it's true. I decrease the constants, so I improve the variance estimation, but still this variance speaks along the iteration and that's not. Can I make it? Can I device a method for which the cost the potential cost more or less is the same but this variance goes to 0? Almost here is to increase this the batch size at each step, but it's not very meaningful. I mean, I'm going towards greater percent. You can still go. You can do it. Yeah, there are techniques that I've been instead are, I'm not going to describe them, but as the saga, saga, I don't know Finito Miso, these are all algorithms and there are many others. There are all algorithms that have the same idea that I'm not going to explain. But the idea is to compute a stochastic estimator of the gradient for which the variance goes to 0 on the iterations. And the way in which you do it is, for instance, maybe they just want to explain this some is that you keep in memory the gradients that you previously computed in order to that's fine. An aggregate estimator of the gradient. So the gradient I computed the 10 iterations ago are a good estimation of the gradient that they got. I call especially when I towards close to converts, it was there. I move in not so much. So I'm using an old gradient to estimate a natural gradient. But but it's but it's fine. And so this is nice because you have a memory requirement, an additional memory requirement with respect to stochastic gradient method. But you can, you can recover, you can get back to all the convergence rates that you add for the gradient side and with the stochastic constant step size. So you have constant step size convergence 1 / K in the convex setting, linear convergence in the strong convex side. Yeah. How this is? Different from a doctor. This is different about ingredients. I will show you in a moment, OK, it's a different idea. You converge back there more than say some complications in the sense that you can choose, you don't have a, you say computation because for instance, you don't have to do fine tuning of the stack size of the step size, but you fix, you can fix a constant step size. And this is a huge advantage when you implement the methods. And then then you will be faster to go towards the minimizer. What they do is that maybe this is not so elegant. So at the beginning you use STD and then you use these are the hybrid methods. So the people that device saga also propose this kind of techniques make them work better in practice. This is I don't remember the I'm sorry, I don't remember. The meter is. Equal to the. So. But if you choose for salary it will be fine. OK, now so SGD is really the algorithm of choice. But yes, sorry. So what is the maximum variant reduction that you can achieve from any method? Is it Sigma square over north or is that something? Nice question. So 1st order methods. So when you allow yourself to access to an Oracle that gives you the gradient, the best possible convergence rate is 1 / K square in the convex scenario. Right. Now this is related to what I'm going to say in the stochastic setting. So when you allow yourself to have access to a gradient, this is 1 / K. So, and I'm not even sure of that in the sense that I don't know if these BIOS reduction techniques are included in the complexity framework for stochastic methods. So usually when you do a stochastic gradient, we will get 1 / sqrt K. If you allow yourself to store this previous gradient, we will be able to get 1 / K I'm not completely sure that this is second class falls into the, let's say the scenario of complexity analysis. So I would need to check. But with this kind of methods you achieve 1 / K. You can I yeah, I doubt that you can go beyond. OK, other question. No. OK, so now, so this is 11 possible direction. But usually these techniques are not so much used to training in our network. Training error network means minimize empirical error. Usually what you do is SGD but not SGD as it is reduces some modifications of SGD that are basically and as usual in my lecture. So I will do it in a simplified setting and then in order to get the latest and more performing variant needed to study by yourself. So we describe the two main ingredients that make stochastic gradient descent a good optimizer and the two variant are acceleration techniques and these are momentum. They say the idea is momentum and the other one is other thing. So the question, let's start off the 1st, the first ingredient. So when the complexity analysis was performed and we discovered that if you use the first order Oracle, you can achieve the complexity, the number of iteration. So increase the number of iteration that is 1 / K square. And people started to be interested in devising optimal methods in the sense that the gradient descent descend as one of the case. So it's some optimal, I know that I can do better. And the techniques that have been proposed have been based on a momentum term. So in optimization, usually it's called acceleration for inertia. And the main ideas are due to next and then many other people. But was the first one to device an optimal network to minimize this function? And the algorithm works like this. That's it. Now I have to change the notation K + 1. And then? So. I wrote the algorithm and then we discussed it. The algorithm is a gradient type algorithm. As. Before, but then after these gradient descent step, I do something different with respect to the gradient descent update again in the direction of minus the gradient. So I didn't. I tried to make sure sure that. It will work. You see, this is not for stochastic gradient that this is for even for the deterministic 1 is that. I want to minimize my here I am here. What I do is that I compute I am here at ETA Ki compute my gradient gradient is less. What do I have to do? I want to show that what's better, that's what maybe here ETA K. And then I do a gradient descent that I finish here. Here I have ETA K + 1 and somewhere I had ETA K maybe. And then after every computer is greatest set, what the way to look is that I starting from ETA K + 1, I move a bit in the direction that was previously detected by the algorithm, which is ETA K + 1 minus ETA K So I here I take this direction and then I move a little bit in this direction and then this thing here is called momentum or inaction in the sense that I need my stuff and then I continue with in the previous direction. So here in the picture it works. It's not the case that you decrease even it's not. But in theory this works and you can prove that the convergence rate for the objective function is like this in the usual setting for gradient descent. So you this is an optimal. It's called an optimal method. You achieve the 1 / P square convergence. The key here, the point where people worked a lot last year is this one. So how much should I go in the previous identify the direction. This is the the moment coefficient and classical. In optimization this is 1 possible choice. As you can see, this coefficient test 1 as those those classic plus Infinity but depends on a parameter alpha. And what I wrote here is that alpha should be bigger than three because that is why this is not true. So this is true if alpha is bigger than three other possible choices maybe if you are an expert or romance an expert. You want me to mention Fista, It is the accelerated version of the software folding algorithm that we have seen in multi L1 norm. You have the accelerated version because props works and it is back in the wood and here the for physical choice is. So the this is a very nice algorithm in the sense that the computational coffee is exactly the same As for gradient descent, but you gain in the convergence rate. So you don't do extra computations but you go faster. So this is very often used. The the key point here is this choice of alpha. And it was the choice that puzzled the optimizers for many years. And he's still puzzling them in the sense that you have convergence of the iterates for this algorithm for every alpha bigger than three. You don't know what happens for our cycle 3. So I always like to say this, you want to work on an open problem, easy to understand. This is an open problem for A3. You don't know if there is convergence of the iterates or not. So what the So the momentum is important in the sense that is really used to train the real to create a network and is the end in Adam. So it is a stochastic gradient cell trainer network and the end is the end of momentum. So the momentum in Adam is a bit different than this one, but still the idea is the same is to keep the memory of the previous SO moment OK you want to. Yeah, yes, I guess that you get close to minima faster, but I. Also get. The algorithm stops, I mean because the momentum you tend to. Oscillations or oscillations also in the how the function value decreases. There are monotonic versions, there is M Vista where you make it decreasing. But still, yes, you're right, it's completely right. Even if I have to say that when you derive this discrete algorithm, discrete algorithm is derived from a continuous model by discretization as again Andrea was showing us, and you have a second order equation that is like this. Now I'm sorry for the notation, but it's so you have this. So the idea of this kind of natural methods or momentum methods come from this dynamical system. This dynamical system you have a mass, a bull with mass that moves in a in something in a fluid that has a disposable OK a disposity which is vanishing in time. So again here is that physically indeed you are not postulating a lot, but instead you are Q that the gradient method has OK. So because you are moving in hauling yeah, the ball and you have a cup and you let the ball go, but you're moving on it. But the nice thing is that this does not slow down convergence because the this coefficient is going to 0 when you're approaching the heat is going to. So this is the key idea was a by the way, yeah, so it's all the games, yeah, that is called indeed this algorithm are also called heavy ball methods. OK, but there are inferences but. I finish my yes, different disparateization of these equations are. Different. Yes. Yes, sure. And different that while this as let's say this as we will study everything is under school. Depending on the disparateization, you may get a different convergence behavior. Sometimes you accelerate, sometimes you go. I would say this is the, this is the and this is the. Yeah. What is the good question? We don't know. It's the underground. If you do the proof you see that we need but not PI1. OK I will. Sorry I passed 2 adaptive techniques. Explain the other area here is simpler. As for variance reduction, there are many many things here. The most famous one is other, but anything that start with another and some of these ideas hidden into it. And the the algorithm is the following. 1. The idea and here I'm slow but. Main difference with respect to gradient descent is that you still do gradient descent, but you allow yourself to choose a step size that depends on the current point where you are, and to be more precise, not only on the current point but also on the previous ones. That OK, so maybe a better way to write it. I write it as gamma K and then I say that gamma K depends on ETA. So what does? And here I write why this is irrelevant. The idea is that this could be relevant because we can have a functions which have. So let's take this function. We have very steep regions and very flatter regions. So the step size, the length of the step size will be. If I use a constant step size, the length of the step size will be very small because the short because the steep part I have more The steep part prevents me to choose a long step size even when I am in the flat part. So in the flat part instead I would like to go very fast because it's flat. Instead, I'm constrained to go very slowly due to the other places where the curvature is. And then he understand I can choose this text size depending on the point where I am. So on this, let's say huge literature, really huge, I just tell you what is the what does other to? And to me, I have to say it's not completely clear why to work better than us more principal choices. In my slides I prepared more another slide that will not show more principal choices. But the other norm, the other grad family is the one that is so the gamma plus one. The study define first recursively. So you as a step size you take one over the square norm of this point. That is the only hyper parameter of the algorithm. So we don't have to choose any constant anymore except for this delta. So it's almost parameter 3 and then how do you adopt the step size you take The step size will be decreasing, it increases as one over square root of the sum of all the PU square norm of the gradients, right? So for if you are finding sub gradient. This. Kind of makes sense but for the gradient case, so I don't have a good explanation why this one should work. What you can prove in practice, in theory is that you get the same convergence rates that you get for the normal one, let's say the basic 1 Manila. But in practice. History of the question. So to be more precise, this is called other norm. This is not the most used one because. You. What you would like to do with the other grader instead is to add this computation to perform this computation on the different coordinates. So instead of taking a step size which is the same for every coordinate, you want to take a step which is the vector, and each coordinate is multiplied by a different constant. So this is similar to what is called preconditioning. Want to change the geometry of your problem? And in that case, so you choose gamma K belongs to RB and gamma K + 1 I is one over BIA plus one plus delta, where BI a + 1 is BI plus the. But with respect Y the I Phi. So you can choose different constants for different coordinates. And this practice works better. This is true, yeah. Just to understand, from a practical perspective, these different algorithms are better in the sense that they are faster, yes, more than they are more precise no no. They are just faster. They are faster and here another another prop in the fact that you will need to compute for instant inches cost of the gradient computational expensive. Maybe you don't know how to precisely you're estimating yeah, so delta is fixed just avoid that. So I have a practical interpretation. What happens? One of the reasons is that one coordinate gets at a very small gradient, then these step size will be bigger and then you will move faster in the flat test directions. OK, this is a kind of intuitive explanation, so this makes sense to me. These I have more difficulties to understand why. So when you put together, let say, the adaptivity of the step and the momentum, and you apply it to stochastic gradient descent instead of gradient descent, you get the other algorithm, right? So these are the main things, yeah. Now, so we have an algorithm that works well to train convex, unfortunately optimization functions that are obtained as the sum of simpler, easiest summons. Now we want to see how, even if we don't have this kind of convergence results, how to compute the gradient of an our network. And it is not so easy because they, as you saw, the definition of the function with respect to its weights of its parameters is a recursive definition. So it's a composition F of X. It's a composition of different functions, each of which depends on some weights. So the computation of the gradient is not. So what I'd like to do is that are not enough is to give you an idea on what back propagation does. So first thing back prop does what I said, it computes the gradient of the one summon of the empirical error with respect to its weights. The weights. So now I restrict myself to the case where these VI of Theta is a loss B. Let's call it. Let's take the Li, call it loss of what F Theta of XI minus Yi square and the most classic gates is the square root for instance. OK, we can think to the square root also or not. It's not really important, does not create problem, not create problems. And so I can remember that F Theta of XI is defined as network. So now I write it ETA encompasses all the all the parameters and I do I guess. Yeah, I have to say I want to do a simplifying assumption since I think it doesn't really. We don't lose anything to understand the propagation something maybe, but these are the 2nd order things. So XI is real and all the parameters are real. So I'm trying to learn the function from R to R So now I assume that XI is not and each WBWJ is not. And I remove the biases because I don't want to add that so my my function and you just wanna I write it like this. I start to be late and I start to be and that's what my base at a certain point. OK, so this is my network and now I write it to the. The idea is that I wrote it explicitly just to show you that the when I look at my function so here it is written in the wrong way that the variable now is Theta and so when I look at my function I see that my function is the composition of MX. So the idea is that I should try to use the chain rule in order to compute the database expecting the ways. The other thing that I may notice is the fact that the weights appear one after the other, and so if I see as a composition, the last weight will appear only in the last layer and will not appear. So each new variable appears when one new layer appears. OK, I want. Since this is not very expressive, I want to rewrite the recursive definition of a neural network. I mean some of the variables that I use when I want to take the I want to apply. So let me write it now. So I'll just write again F Theta of XI. What we do is that F of XI is HL plus one. Well, we said H0 is equal to XI. And then I inserted these Auxilian variables and then I go on. But then we will use this OK, just define F of X so F of X. Is H of LS. L1L plus one and H of L + 1. You can see that the exact of a dynamic update of my H passing and arriving as the as the final output. Now I want. What I want to do is to rewrite the objective function. I rewrite here. So here I can write that V of the I of Theta is HL plus 1 -, y I square type because now I want to compute. So I want to compute. I want to compute the gradient of VI with respect to all the coefficients for the parameters and so in particularly I will choose one. I want to compute the derivative of the I with respect to the weight WL type. And I want to do this of course for N equal 0. Let me see step one. I give a name to this to the derivative of V with respect to the parameter ZL and I call this quantity delta L, maybe delta. Now what I want is to derive a recursive expression for this derivative. So I will write the ZL as the derivative of V with respect to ZL plus one times the key rule. The derivative of ZL plus one with respect to ZLZL plus one is a function of ZL, right? Because we see here when I go to the next one, it's a function of the people form. So I can write the researcher. Rule properly. And here what I can do is I can compute stuff, this one is called delta N plus one and this one I can compute it in the sense that maybe I write it here it is ZL plus one CL plus one is WL times HL and HL is Sigma of CL. OK, thank you. Otherwise it was something that you work. OK thanks. So when I derive this function with respect to the L, what I get is WL plus one times Sigma prime, OK. And then when I want so I can compute the delta L starting from delta L + 1 right? But I need to start and so I need to compute the delta again. What is that? L is the derivative of B with respect to BL. So you see that CL is 1, so it will be just a derivative of B with respect to its argument. So for the square plus, so B -1. So here I have a cursive way that goes. It says backward. I start from that time and then I can compute all the other derivatives with respect to CN. OK, 10 back to. I'm almost done. I'm sorry. I will take you. 5 minutes. I try to be faster without being. Yeah, I own that one step. That's it for me. I know that it works. In which way finally I can write and the delivery with respect? To the L and then. Take the derivative of the L respect to West one, right? So this will be the quantity in which I need that I want to compute. OK then CL. What is the CLCL is I wrote it here is WL times HL. So when I derive CL we expect WL. I get Sorry this is the second part. I need the derivative of P with respect to CL which is that time. So this is times HL. And. This tool for a differences and how I can compute? Final computation by propagation. So what I do is that I need you see I need a child and what I need is WL plus one, Sigma prime ZL. So what I need to compute is this for this quantity WL plus one and HL. So I want to compute of I write it just to be clear variant of VI and at the teacher that is W 0 W 1 WL at this point. So W this point is the point where I'm computing the gradient. So it's given, right. So what I do is that I do this. I don't maybe I don't rewrite it again, but I do this computation. This is a forward pass start from H0 to Z0 for the that I have H and I go on. So first this is the call it F So I do the follow up pass to compute all the parameters that I need. So in particular HL and Sigma prime of CL and then what I do set 1/2, I do a backward pass. And this is this one, yeah. And that's I follow back. So back propagation is this one. So basically what I do is that I back propagate. So. I propagate to the with respect to my weights. OK, this is called. This is called. Once I am graded, I can move you to the. I can do things in a different order slightly in a more maybe clever order. When I do the gradient, I can start to compute the stochastic gradient with respect to the derivative, with respect to one layer and then start to the other. But this is the idea that I do the gradient, stochastic gradient, and then I'm doing gradient. So there are what I'm not telling you because it's too late and I want to keep you here too long. So I don't have a specific reference convergence results that tell you, but people started to try to understand why so that they send words to train neural networks and there are values attacks that start to give some answers. But the convergence analysis is completely different from the convex one, which is very simple, very elementary I would say. And this that involves much more deep deeper tools. So what I said is, so the point is that you can try to prove you can think that stochastic gradient center is less keen to stop in local demises. And this point of view has been taken when you study lounge Evan dynamic. So stochastic gradient to optimize the convex function can work. There is a problem on how many computations you need to do in the presentation, let's say. Then you can try to take a different way and to consider networks that have an infinite number of neurons per layer. One layer infinite number of neurons and then you get the other results that are more are related to the previous one but that go to the fastest time that gradient flows on the space of measures. And then another direction. Instead is try to show that you have some geometric conditions so that your function is non convex but not too bad. So it satisfies some geometric conditions that usually are called some variations of these names if you have heard them before. So the work is ongoing and there's much food, but the is used in practice and in practice there's how that works. So thank you very much. Sorry for your time.