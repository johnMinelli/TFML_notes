In particular, how transformer architectures such as key or favorite sector, sorry, your favorite may be acting on on the input data and how it may evolve the input data. So transform the input data. OK, so here's the presentation of this again, let me stress the fact that this is going to be a mathematics to some extent radios, but to the expense of in a certain sense the fact that this we are going to make some assumptions that. Of course not always apply. But still, I would like to present some results or a framework that allows to build some intuitions of ideas on what happens. Yeah, by the way, I the notes of what I'm going to write backwards should be have been sent to you by e-mail by now or if not, not yet. Well, soon enough you're gonna receive the notes. So you can follow on the notes that are given not to you. But then it's like if it's not soon, then. Just after. OK, so the the structure of the talk is I would like to start talking about deep less Nets or the single networks 1st and how these in a framework to understand how these structures work on the input data. And then to extend this to transformer models and in particular to discuss how the choice of hyper parameters in these models may affect the presentations that these models feel. OK, so the main idea is we want to work at inference time and we want to understand our network elaborates the input information and this is the objective. OK. All right, so the first part of the talk, part 0, deep estimates, no worries. If you are not familiar with these words, I'm going to explain them in a second. OK, So we're going to start with these deep estimates and these. So deep estimates are a neural network structure that have been sensitively used in learning. And the way this neural network structure works is it first takes an input as you imagine input. By the way, tell me, is this big enough? Yeah, input is a data point. Data point will be embedded in the corresponding place. So I'm associating. Zero because this. Between time for this talk in Rd. and you may imagine your favorite picture of a cat being embedded using the RGB confirmation of the RGB representation of the picture into a high dimensional space part. OK, so this is what you feed to the network and you hope that the network tells you at the end. OK, right. Then we have how the network works on the data and one possible representation of how the network works on the input data, it can be done as follows. So we'll have the input, OK. And then the input is fed through a certain a certain stack of Big L layers, OK. And at each layer following up from the presentation where we have an iteration of operation at each layer which is done in the following way. I guess most people already know, but it's the the the hidden state of the network at layer L + 1 is obtained by applying the linearity to the output state of the previous layer. And this is defined as follows. So we have we have a layer update that is defined as follows. So you take inputs and then the state of the network at the next layer small L indexes layers. Here is obtained by X layer small L + a certain nonlinear function F Theta which. Is going to be. For instance, a single layer of neural network X. OK, So what this network does is it simply takes the output of the previous layer, it passes it and it combines it additively with the output, the output of the previous layer. OK, so this is our zero neural network S net preferred. I just questioned earlier. So no, no. OK, so far so good. OK, So here and here, obviously L is between 1L Big L being the depth of the network. And then at the last layer of the network, we what is going to happen is that we're going to take the output of the last layer or the last hidden state of the network, which is X Big L multiplied with a vector and then pass it through a soft map function. So this is how these networks work often to well a here is not D, it's just a vector, right? And then often people take this a small multiply this out the output of the network by a small parameter epsilon to make it make this iteration more. Stable, OK, right. So. What this, what this tells us is that we can represent. The. State of the network or how this network works on the information as as a in this case discrete time dynamical system that again evolves our the state of the network or how the network represents the data through its OK. So in this talk time would be depth of the network and we want to know how this dynamical system OK, there's not going to be at least here much focus on what this function is. It's just a very difficult. OK, great. Then what people have noticed is that one can take a limit here, which is the large element. This is obviously realistic because, well, networks are deep. So you would like to know what happens. And for those of you who are familiar with the Euler method of equation, this is this formula is speaking to you very loudly asking it's begging to be translated into ordinary differential. OK, hands up. Who is not familiar with Euler method like I'll do it. I'll do it right now. So I'll use this word and I'll rate it. So the Euler method for integration of ordinary differential equations for SO call Euler method. The idea of the Euler method is the following. You have a trajectory and an initial condition X0 OK. This trajectory solves X dot T equal to F of XT. This is a classical ordinary equation and the way you approximate this, if you want to solve this equation numerically, what you would do is you would start from the initial point. You would take FU which tells you the tangent vector at this initial point along the projectory. You would follow it for a small time step, that's enough and then iterates this idea, right? So you would do this, this, that that. The iteration here, I'm sorry, didn't manage space very well, but the iteration here is X epsilon of l + 1 equal to X epsilon L plus epsilon F epsilon. OK, So here what I'm doing is I'm taking the previous point, previous iterate of my method and then I'm adding to it the vector field times a small parameter, OK? And when the small parameter epsilon, well I said it already small, then the director will follow this projector closer, OK. And the theorem says I mean convergence theorem, the order method says that in a certain sense you would have convergence. So theorem says that you would have convergence of the family of trajectories X epsilon towards the trajectory X dot as epsilon goes to 0. OK, obviously this has to be there. There must be a time change that is done here. So if you want to be specific, what you would have is the supremum of X epsilon of L minus. X of epsilon L Right it is. Going to 0 as epsilon and here it's L between zero and T over OK. So every step of your iteration corresponds to an increase of time of epsilon of your original system because you are scaling your vector field by epsilon. OK, this is just. More. A more, you know, regular or more precise statement about what this. Yeah. You want to send the number of You want to consider it, Yeah. I'm I'm about to connect the two. Yes, this was just thank you for the question. It was just a reminder for the oil is for those who are not familiar with it. But then what people have done, sorry, is people have considered the deep limits of this neural network here and connected it with it. Because what turns out to be true is that in practice, people often take this epsilon as a function of L. And in particular they take this epsilon for large L. They choose epsilon L of the form 1 / L or at the scale of 1. OK, So what what this means is that you're scaling epsilon by a small quantity. OK. Then what you will have is then that XL here should index the set of projectors L here XL. What happens here is that XL dot converges. As L goes to Infinity. L now controls you know the small time step in my analysis. And since epsilon here it's going to be a state of 1 / L here I have I have converted to the dynamical system for all layers. L is going to converge to X dot, and X dot solves the ordinary differential equation that is induced by this luminarity. Here which is F. So what this means is that this is true solving X dot equal to F Theta X. OK, yes please. Are you assuming that the way? Yes, I'm assuming that, yes, you can obviously change it. But if you do change it, then you you have to either you have some averaging and then you have some the thing that has a little bit of noise, but the noise goes to zero. And then you only see the average of data depending on how you draw data or yeah, more interesting. So I'm simplifying things here. Absolutely. Also, if you do weight sharing in any way, then this yeah, please accept that this was defined only on some 2 points, right? So, so sorry, it's maybe not optimal to to see it on here, but you, you can there are various way where you can define this convergence. But here I'm just defining on the discrete points L that goes from zero to T over epsilon. So over there. So in this case here we would have L which is 12 depth of the network because excellent. Obviously what you can do is you can also linearly interpolate between these two points and then have convergence. The topology here it's a very strong. Does it make sense what I explained? Here. So yes, what is a Sigma? The Sigma, you could choose it if you do classification for two classes, it's just a Sigma even without. Yeah. So if you do a question, you can just I'm, I'm giving a framework just to. I'm very happy about that. OK, So what this is called, I mean this is a very well cited paper and it's a paper that is called New one ODS which I guess. To anyone who does medical math just means that recycle ideas in construction, but no, because every layer is connection. If I train this for a sequence of data. I'm serious. Well, you understand this. Like if you want to use this for regression, then the only place where you would. Do is that yeah, I'm going to say something very quickly in a second about that right. But, but the idea here is, is very simple is that you have and then it could determine, sorry, discrete time dynamical system telling you how the information flows through your metric. And this converges in a certain limit under proper retaining, which seems to be close to what people do in practice for continuous time dynamical system, which again describes just how data are worked on or elaborated by the network. OK. And this is the main idea, the main idea of this OK, So what happens in practice or what we would expect to happen? Other thing in fact what go into just ask is that inference time. What we expect to happen is the following. We would expect. That well, let me draw a big diagram here. So here this is fine. It's a depth of the network. Again, we have at the beginning some points which I belong to two classes, maybe a set of, you know, dog pictures and a set of cat pictures. This is obviously a very high dimensional space Rd. and what you would expect to happen, and this is something that is still ongoing, I mean open questions in the literature, is that at the end you would expect that or at the training time, your flow pushes all the cat pictures to a regional state space while all the top pictures go somewhere else. So that by choosing an appropriate vector A, you make the work of the classifier extremely OK. So what you will have, what you imagine will happen is that this is a start, the best this is a start. So the best vector that discriminates the classes once they have been simplified or separated well by the network, and then you have a very well performing classifier that generalizes and The idea is that the. Flow. Of your network separates or clusters based data into different regions of state space associated with different. By the way, the representations that sorry the data point at XL are often called representations of them because they should represent the data, the quality you want. To does that the. Objective would be something between this and. Why something like this? And then you minimize. Then you would have one dynamical. System and. You want to identify the flow vector that pushes the right points on the right and the left points on the right and and classes. Obviously, yes, this is this is hand waving. I'm not, I'm not saying any theorems for now. The only theorem that I'm trying to the only, the only improvement thing is this. This is an intuition, right? I mean, there are people that work on this. This is this phenomenon is called neural collapse. What people are trying to prove is that indeed you have for two classes reading 2 clusters of points at the end of training. But that's still something that are not. But yeah, the idea is is that this should in some settings you can prove it, but not yes. So does this have something to do with presentation or? Yeah, yeah, there are. There are. Not sure I'm familiar with the word. Tokenization, but yes, I mean it's. Embedding. Well, I mean that that step is happening. Here right the embedding step. But then you're you're. Kind. Of learning the nonina structure in your data, this is how you do one week OK all right so again let me stress the main idea of what happens here. We have a discrete time dynamical system telling us how the information flows through the network. This converges with the for continuous time dynamical system, again telling us how data are elaborated with the network and we expect that this builds. This dynamics builds a structure that captures the structure present in the D OK, it's a representation of. Again, D here represents the depth. What I'm going to do in the rest of the talk is try to generalize this image, this picture for transformer models. OK, so for a different structure. Here K star is the optimal classifier, I mean parameter for the classification. All right, so, OK, so let me start with moving on to the more timely or fashionable neural network structure with this Transformers. So now we want to extend this reason into deep transport the classical example such a PDE or any other. OK, in this case, let's do step by step. So we start with the input there. So what is the input in in a transformer model? Well, input is a sentence, for instance. OK, picture. Let's consider the simple setting of sentence, which may be right. Need a new paper on then your favorite subject. And obviously, because they are going to take over soon and we want to save ourselves if you finish please, because, well, you want to have a higher chance than anybody else, right? OK, So what I have here is in the sentence what it, what usually happens is this is going to be tokenized. So it's going to be separated. Every word and every configuration mark is going to be separated. And then the network does two things. The first thing is to associate a position of encoding to each word. So it what I'm going to do here is I'm just adding a number here. This is usually, I mean a more complicated task that happens in the in the transformer model. But you just add an information about the position of each word in the sentence. And then you take a very high dimensional and nonlinear map and you map this pair of word or embedded word and position encoding into a very high dimensional. And I'm not going in this talk to consider anything specific about this map. I'm just telling you that it's a very very high dimensional map which is called nonlinear embed and this will map each one each pair of word and position here into a list of elements in a high dimensional space which I'm going to. Call. X1X. 2. X 3 and so on. And here is going to be. It's going to be an important parameter and it's the length of the prompt that comes as an input. So I'm mapping this and all of these elements are going to be in high dimensional real space. You can choose this to be Rd. but in for this talk for the sake of this talk. We are going to simplify. This and think about them as being in so to normalize them so to be in the sphere of dimensional so SD -1 and since there are north of them, it's. Going to be to the top. D is the dimensional embedded space and is the length of the. Prompt square and. Typically, yeah. So the dimension of the embedding space is the same if the left of the world is different, yes, everything is mapped to the same. So why D -1? You're right, I could have this one. Because because I'm thinking about words being dimensional so I'm limiting the norms. OK. Right. So an important remark because before we move to the layer update is that I am already encoding the position of these words inside their state on the sphere. So if I switch these two words, the index here will change and therefore their map, their position on the sphere will change. OK, so this is important because this means that these points on the sphere, which from now on I would interpret as particles, they are indistinguishable. They the index of this particle or exchangeable. The index of this particles don't matter. Only their position on the sphere matters, because their position in the original sentence is encoded in their statements. OK, so these particles are exchangeable. We don't care about the vector X1X2X3X3. The same information would be encoded if I switch the position of these planets, because this nonlinear map would have changed, right? Change. Yeah, I'm sorry. So. Giving that we are carrying only about the absolute position, not about the relative position. No, OK, no, no. I'm just caring about X position. But they're not caring about a position being X1 and this composition being X2. All the information is encoded in the list or in the set of these points, not in their order, because the order in the original sentence is in stored in the in the state of the sphere. So these particles are exchanged. I don't. I can't. I can't recognize them if I call 1X1 and X2 or X2 and X1 the same information. Is there makes sense? So this step is called the high dimensional nonlinear embedding, right? And this is the first step that the transformer does to your input sentence. And then the second thing that the transformer does is takes this input, which is now a list of high dimensional vectors and works on it exactly as before, passing it through a set of nonlinear operations which are a bit more complicated than they are here. And I will now explain about it. OK. Layer of date is the following. Yeah. So the layer of date works. Suppose that for those of you who are familiar with Transformers, this is going to be use of business. For those of you who aren't, it's a bit of a complicated bit of a mouthful, but I'm going to simplify, so don't be scared. About. OK, so exactly as before we have. What happens to the input is that it goes, it's passed through a stack of ether and I can draw a similar diagram to the one I have drawn earlier, although this diagram would be a bit more complex. So it's M0. So I have my list of elements and I pass it to an Internet application of the following two operations we have instead of one, we have two operations. The first one is what is called the self attention operation, and the second one is going to be exactly as before a nonlinear neural network. And all of these are applied in a set. What I mean is that before applying this application, I take the input and then applied additive combining additively with the output of the nonlinear operation. OK, So when we focus on what this does, OK, then I will actually forget about this because they want to see look at the symbols. OK. So what this self attention operation does is that it takes the output of the previous layer and then for each particle XI it updates the position of the particle XI at layer plus one in a estimate way. So it takes the position of the previous layer like L, then combines it with again a small parameter that just here times a complicated function which is given by this word J equal 1 and. E to the beta QXILKXJ L * X JL times. So here this is a matrix. So this is BXJ. OK, so. This is a bit of a complicated formula. Let me you know, parse it for you again. Combining with the state of the corresponding particle at the previous layer a small parameter times a normalization factor of Z which is given by simply sum of east to the beta. UXILKXJL and that's it, J equal to 1. This, this is just a normalization factor. We want to play a very big role in what I'm about to say. Actually, I'm going to kill it in the next thing. So. OK, the important part is this formula. So here UK and V are some parameters, UK and V are some parameter matrices, query key and value matrices and also about to kill them in a second. OK, but these are parameters that are usually present in the model and beta is so-called inverse temperature parameter and it's just a scalar graph. So beta is a scalar and it's not OK. So it's a complicated formula. I don't expect you to have any intuition about this. I'm going to try to compare some intuition. This is what's usually done in this step. And then when you're recombine the input to the output of the self attention module, which is an operation that is represented here, what you will actually do is you will normalize the output because as you may remember, every particle XI lives on the sphere. So you want the output of the update of every position to be again. So you remap everything on back on the sphere where N here of Y is equal to Y over. OK, what I wrote here is the update that you get from this part of the matter. Then you would in principle apply a linear function exactly as before. But I think this is complicated enough. I guess you guys agree with me, so I'm going to completely forget about this part. For the rest, just focus on the action of the state attention model on the statement. OK, yes. But I understand kind of like represent small words related to. What we all understand is why is the initial input layer combined with the object? So why is XI present? I mean, from an operational point of view, people say for stability. In practice, I don't know people do it. And I'm not so sure. Yeah. It makes the flow of information more continuous, so it preserves all that remind remembers information that you fed through the to the nonlinearity. There are a lot of ways you can think about it and all like as what you would say they right and that that would be a way to kind of build memory or have a spasm not you would just do it. Yeah, exactly. So here we have again exactly like before our continuous A discrete time dynamical system updating the position of each of the particles. An important thing is that why in the previous case a particle, so a point X was evolving independently on anything else that was just a particle evolving to the to the network with step. What will happen here is that the position of XI at the next layer depends on the position of all the other particles. So there is interaction between this is going to be important, Yes, the normal addition term that you are using is to make the point under sphere. But in practice, I guess we are using different kind of normalization, right? Yeah, but this makes the difference in the analysis or it's just arbitrary. So that's a very good question. So the first and this allows me also to refer to the original autos of this. So this idea and the convergence of them about to explain came from a paper by and Sander. So in that case they keep X in RN. So there is no there is no normalization. We don't think about normalization people from MIT. So people and other people have then thought about what happens with the normalization and things get. So the dynamics is quite particularly different. So I hope that you can change. You have. Different dynamics and you can ask yourself what happens. This is post layer normalization. OK, so exactly like before, I have now erased the picture. But we have a system. Let's take the infinite again. This is integrated to obtain the representation of each of the data points at the output of the network in the deep limit, exactly as before. We have converted to this continuous time that's three times and I'm in the system to a continuous one if we take epsilon L. Here epsilon L of the. And what you obtain here is a continuous time dynamical system which takes X both the position of the particle I according to the field induced by this non enough. OK, so it's that before this place the role of except that now if that contains interaction. So here we'll have I leaving some space here, one over ZI beta of sum of east to the beta QXITKXJT times VXJ. Again, I'm not giving enough information about this. I'm going to simplify this dynamics in a second to allow you to develop. But again, this is exactly the same thing as before. You take the nonlinearity, you have yet set the depth. Use. On the right hand side and that gives you a continuous time dynamical system approximating the dynamics of that is the time dynamical system in OK here again, this is that. This tells us what there is. Token representations evolve through the depth OK here. This is what I just did. The important difference with earlier is that here I have a normalization factor, so everything the dynamics is projected to live on the surface of the sphere. So because this is not satisfying this condition, I need to project this thing on the octogonal space on the tangent space of the space. Octogonal is just a projection of this vector field that is Rd. into a tangent space. OK, so if that PX is projection on TXS, sorry, yeah, it's. Like the classical stupid, but what is that? It's equal to 0? You can't normalize. It can happen, but it's very well no if you if it's equal to 0 you just add it. So possible Q&K? We use Q&K usually. Can't we just use one method? Yes. I'm, I'm, I'm just using this so that if you go to the Wikipedia page and you look at everything, you, you say, oh, it's actually the same. So it was nice. But yeah, for mathematician, this is right. But it's not, not, not exactly. Because, you know, if you draw Q&K at random, then you, you can, you can push this on the other side and then the product of your K changes the structure of the matrix. So it's not. OK, great. So, OK, so here we have finally state of the system we want to investigate. And again just summarizing, we have a continuous dynamic, the system approximating A discrete dynamic, the system telling us how the token representation evolved through the depth of the transportation model. This is also called self attention model because we are completely forgetting about the neural network fact. This part here which is expected to do some heavy lifting for you. So here I'm starting to simplify already my model to make it more understandable from the mathematical point. OK, so the important thing here and here start. I finally can explain the word mean field. There is an important point here which is that particles through this this interaction is called the mean field interaction because you are taking the average over different contributions coming from different path. So this is a sum divided by a weighting factor. So this is an average over the path OK and mean field means that every particle acts on the particle XI in a mean way. So this is the field. The particle XI field is a mean from generated by all the other. OK, so now finally we get to trying to build some intuition. So we want to simplify this obnoxious formula, right? And so simplifying assumptions, we are going to simplify this formula to make be able to make some general statements. So I'm going to make 2 further assumptions. The first one was to kill the only have a network part which already would have kind of have any anyone interested in application completely disconnect from this talk. If there is anyone left, they will now absolutely disconnect because the second assumption I would like to make is that I would like to portion out the contribution by the matrixes QK and V and set them to something simple. Simplest matrix. Well, the zero matrix simple, but the simplest non zero matrix you can think about is probably Yeah, I will show you in a second that even for realistic parameter QKNVS, the effect that I want to describe in this talk is going to be reserved here. This is obviously a very heavy assumption, but in order to pull something. We are going to have to simplify a. Little bit then we are going to set. Also further works that try to explain this assumption. And the second assumption that I'm going to do is that I'm going to set set I beta as NN being the length of them. So I'm I'm going to set all the normalization factor for all of the particles the same. This automatically simplifies the representation. Again, there are some motivation for this. This again, if you do experiments kind of preserves quantitatively at least the effect that I am about to explain. And this part here is you can interpret Zeta IP as time practical dependent time change. But you can check that the fixed points of the system that I introduced above don't depend on the CI because the fixed points correspond to this guy being equal to 0 for all the particles. And this is just something which is never seen, right? So this guy is going to be irrelevant for the fixed points. OK, so with these two simplifications I can finally write an intelligible model which is the following. The time derivative of particle XI is equal to the autogonal projection at XI of following Formula One over north sum of E to the beta XI as we have stated earlier. But these brackets here are an inner product. OK, so the. Inner Product. XI, XJ. Times XJ. And this sum goes. J equal to IJ equal to. One and. This is going to be the set of ordinary differential equations I would like to investigate, so we want to look at this one. OK I just took that formula and change QK in V1 and killed one of us and changed it with one. Sorry, yeah, yeah. But if you set all those QA and B values from transformer architecture from the attention architecture to I then are are just removing that information like position on encoding stuff like that. But what I would like to extract from this framework is qualitatively what the effect of the self attention operation is on the on the on the token of presentations. So exactly more of more focused on the architecture than on the on the real values of parameters. And extending this to moralistic values is still open. So it's a very I. Mean would be very. There are some papers, OK. And then now let me let me talk a little bit about the structure here. I managed my space very quickly, but I have I want to make a statement about this and the statement about this right hand side is the following. This right hand side, let me try to make some space downstairs. You can express this as this formula as a gradient. Of another formula in particular. This can be expressed as PXIP times gradient of X in X of certain energy function that depends on beta XXJ. 1J1 N. So this formula is the gradient of another another quantity and this quantity is given by. Energy is given by this formula. Or maybe I could as well as well just use this. Here energy beta of XJ is given by 1 / 2 and sum of E to the beta XIXJJ and equal to I going to. OK, so you can check that if you take a derivative of this term in XI, you will get an XJ downstairs. So here maybe I'm missing a beta. Actually you can get an XJ up downstairs to some remains the same and therefore you get exactly this term. Yes. Right there. Yes, yeah, Yeah, you're right. OK, So I have this is a gradient fluid, right? So in order to understand what this does, I might just as well look at the energy and look at the at the critical points of the energy. And this would tell me where I expect this to stop or? Right. There is a lot of structure in this, right? And So what what this dynamics is trying to do is it's a ascent. There's no minus here. So I'm trying to maximize this energy and this energy is maximized. It's pretty easy to. See where this? Energy is going to be maximized because all of the XIS are on the sphere, so the norm is bounded. It's equal to 1. So this energy is going to be maximized when XI is equal to XJ for all I and J. So the energy is maximized. This dynamics is going to push me to have all the particles sit at the same. I'm trying to cluster particles together. OK, this is the first important remark. So this energy, this gradient flow dynamics is trying to maximize this energy trying to cluster particles together. In fact, you could think about this expression here and see that basically what the dynamics is trying to do is looking at particles nearby. Near particles will have a larger inner product and therefore will have stronger weight in this average, in this average. And then it's going to move them towards because when you talk, that's what this dynamics is trying to do. And then let me know some. Another important thing is that the energy only depends on the relative position of the particles here. So if I do a global rotation of the initial state of the dynamics, the dynamics would be variant on this global rotation. So if I reapplied the rotation after I let the dynamics flow, I get. So in particular, the consequence of this fact is that if I have initial state that is an initial state that is symmetric under certain group of rotations, this symmetric will be preserved by. So what I mean? Maybe I can explain this without simple example. So. So what I mean here is that you can imagine what happens if I have situation in D equal to 2. So on the circle, OK, I have particles sitting here here, here here here, here and here here. So this is a symmetric initial condition. What will happen to this is that basically this particles will be drawn together, this particles 2, this particles 2 and this particles anyway end up in a symmetric configuration which will be preserved that the dynamics. This is going to be a fixed point of the dynamics because this particle feels an equal force from yes. Please do we have any other additional assumptions about this XI? For the moment, I'm making up. Is this true for all except that they are? You don't know. OK. Yeah. Any other questions? Sorry, is the energy also dependent on I? Sorry, is the energy also dependent on I? Oh, sorry here. It's probably because it's a bit far, but this is the sum on J unequal to I. So this is. This doesn't contain anything. So this is a 2 index J equal to Y&J equal to 1 to north, I equal to 1 to north and J Oh, this is just the the list. Of all of them, OK. So let me just write. Down what they. Yeah. OK. So the note is that. East beta is maximized when X1 is equal. To X. So, you know, just pushing me there. The natural question now, if you like dynamics as I do, is whether you do end up there right? Because that's the most convenient point in your dynamics to push it. But it's not so clear whether you see there are other fixed points that are not right, that are not a synchronous. This is called a synchronized state. Because all the, all the. Points are in the same position you can see up there. That's also a big point. You can convince yourself about that because every particle feels force from symmetric force from, right? So the question is whether these fixed points are locally stable or not. So there is a positive basing of a caption for symmetric fixed points without not. So this brings us to the next question, clustering. Can we say something more about the dynamics of these dynamics here in general for how we have initial conditions? Unfortunately for us, or unfortunately because it would have been a nice proof, somebody else has already proven that for N larger equals one almost every initial condition X0IN R. Sorry in SI made a mistake on the notes here SE -1 and we have the XT converges X. * which is given by which? Is a synchronized state or total cluster state, so it's given by. So in other words, what I'm trying to say is that those fixed points are in the fixed points that are status. They are not stable. There's always an unstable direction. This is exactly how this is proven. You characterize fully all the fixed points of your system that are suboptimal. Then you prove that there is an unstable direction for all of them. And this implies that the measure of the set of all of all of the suboptimal fixed points then become saddles. This means that the attractive of the basin of attraction of all of these points has measured, right? So almost every initial condition you have convergence to a totally stable point which is only stable. The only two point that is totally stable is how we doing no, because it because it's a gradient flow so the energy is non convex. But it's not. There is no all this, all this all the only going to be inside the Yeah. Maximizing energy. Short, no problem. It's because here I'm I'm. This is a gradient, but there is no mind. So X dot goes in the direction of ascent of this gradient. Descent dynamics is X dot equal to minus gradient. You have a plus you're maximum. OK, so this was proven, I have left the references on the notes. Did you get? Yeah. So what this tells us is that we are pushing things. In the end these dynamics almost show of the whatever input you give you give to them. With cluster the dynamics for single. Great, so this is good news mathematically, but not so good news from the. More. More practical point of view because, well, it's a bit of a sad picture. Right, you give your. Sentence to and then it just builds a cluster, right? So it's a. It's a very synthetic situation, so I don't. If. You're thinking about prediction, right? This means that the the network will will predict. So as it goes through the depth of the network you will have, you can think about the distribution of X as the presentation of the next word that the network will predict. Or you know your output, the output of the sentence you have given to it. And this would be a deterministic. Word. So something quite. And we know that these models are not the next prediction. And second thing, if you're thinking about just influence so you know, transformer building a presentation of your data, then this is a pretty. Poor representation. This means that the transformer has a very single point idea of what you have given. Which of course I mean I would expect to be something. OK, so your observation now if you go back to the dynamics is the following maybe? I will go. Here is that this doesn't really happen, or better to to state it more faithfully, there is something that happens between this the initialization and this large time limit. But maybe more a better explanation of what happens in. That's all my models, at least in the simplified setting. And the observation is the problem. Let's keep dimension lower just for the intuition here. So the observation here is that if you start your initial condition with a large, for instance a large number of tokens, and you know here in the absence of a figure direction in this model, because I have said everything. To be. Just take the uniform a set of iid initial conditions probably. And then I plot the evolution in time. So this is T equal to 0 and I plot the evolution in time under under this dynamics. OK and look at what happens computation. So what happens computationally is as predicted by the theorem. But in the end of the simulation, for a very long time all particles would have concentrated in a single point. But you would notice that in the meantime something there was an intermediate phase. Well, what happened is that particles have first clustered, yes right and position would have expected, but first have clustered into subclusters, something like this. And then these clusters then sequentially merge for the output cluster that this OK, so there is a meta stable cluster in state where you can observe the update time. In a way, this meta stable cluster in state could be at least a better explanation of what happens these models, because this representation of the data is a much richer representation. You have, yes, a synthetic sparse representation of your data, but at the same time it's a bit richer than the single representation, OK. And what we want to characterize in this talk is what is this structure here as how? How is this structure emerging from the model that we have just introduced? And we would like to do this when. So let me show you a few simulations to convince you that this is actually happening in practice. Practice. For the model that I have for that of the simplified model. OK, so so this is a time step simulation on a sphere of dimension 2, and you see that at the beginning you have a uniform distribution of data points. Then they tend to cluster into smaller clusters, but then ultimately merge into the unique cluster predicted by the theorem. If you do something with a large number of points. Again, this is the initial condition with points uniformly distributed atmosphere. And if I now let the evolution run, I have to wait a little bit, but then I see that the points merge into a set of clusters that are, at least on the time scale of this experiment, kind of fixed. And then after a while, every one of these clusters will start to merge with an equal to 1 to. Ultimately we didn't run it until the end, but ultimately for. The ultimate class. OK, so. This is the effect we want to characterize. We want to study how this phenomenon happens and from a uniform initial distribution. And we also would like to characterize this as a function of the unique parameter that is left in our model which is better. And this is the objective of the last half. OK, is this is this clear? I mean then next result like you cannot like it, but this is the main idea of what it would like to present. OK, you have just a second I, you have a dynamics that tells you how talking evolved through the network of a very, very, very simplified, but you may want to know, I mean that represents how the architecture acts on data. You observe you have this mental stable clustering phenomenon forming which may you? Know. Suggest that the network forms a sparse representation of your data through instead, right? And we would like to characterize it as a function of the only. Remaining parameter Yes, the initial points are each word. Sorry, the initial point is each word. For one or two or two. Yes. So the density of the cluster is related to some probability of the. Yeah. So the. Heavier cluster is the heavier the model they have AI mean the more importance the model is given to a certain class. Again, let me stress this is not a real transport. So what happens in reality is a bit different probably, but the effect, the interacting particle nature of the self attention model is something that is absolutely right. There's just a lot of more parameters that is hard to capture. Yeah, yes. How do you observe that this? The original words I can say yes, very good. Question great, great. You're driving me through the presentation. I mean, I hope I'm understanding what what you're asking here, but this is but OK. This is the distribution of token to token similarities, so inner products between tokens that I think through birth this is this was this is taken from a paper by. But basically what they do is they, they, they look at the evolution layer by layer, layer 252627464748 invert of the inner product between the tokens at each of the levels. And you see that you have a build up of cluster, cluster points which is inner products equal to one. And also here in the intermediate regime you still you see that there are some peaks for it which. I mean. Obviously it's not proof of what I'm saying, but qualitatively the phenomenon is seems to be happening. There is a sparse representation of the data. This is for actual parameters and somebody at the beginning asked whether the parameters are fixed through the network. They are fixed so. So this is actually very similar. The clustering collapse with the self attention or even with the fit forward layer. So that's a good question. So if you add a fit forward layer, the you could still go through a continuous time limit through a so-called so-called no, it doesn't matter how it's called. The only important part is that here you have a non interaction term. So every particle would would feel an external interaction. So yes, the the interactive force would still be there. The only thing is that it would interact with a complicated non interactive term which you don't know what it is. So we are just leaving it out for the moment. But the interaction with you 2 would be very. The problem is adding a potential that does something if it's strong enough to kind of kills this effect. So it's not so clear what the what the generic choice of this interruption. So it's another question, but yes, yeah, yeah, I know that this model is simplified. Are there relation the semantics of the words which after or this information is lost for the moment that this information is? But it seems to me that one thing is lost is that whenever you do neuron. So let's say if you have good vectors gainer product, you just keep things that. Close it if. You are angle is more if you are. Semantic. Here we basically assume that they are. So you don't do neuron, you just you got everything. So that's the only. Simplification. Which is huge because you're assuming. The angles, yeah, very semantic. Meaning and that the tokens are not symbol, but they're already nuclear vectors with semantic meaning but. Let's say from the. Beginning everything else, I guess everything here is the Z. Yeah. But the Z, again, if you add the Z, it's just a, it's a change of time independently for each fact. So it doesn't really. And other than that, this means that it's about the vector. We like to move them so that already the angles were meaningful, but now it's even closer for. The moment it's the same so I didn't want to. Interrupt but I think it's just posterior, but I think. You do work too back and you get amazing. Thing then on top of it you will squeeze it a bit. More right although of. Course, usually the process is the representation. AB and the other Q as the accuser Rd. yeah so we have this some hopefully appearing soon results but that they didn't do something with you can be something next episode I saw. OK, great. So, so we want to characterize this thing. We have 20. Minutes 1515. OK, great. So I'll switch gears. The setting we want to characterize this is the setting where N goes to Infinity. So I want to consider the large context length. The reason is that if I don't take any limit, I won't see it. OK, so, so this happens when I take N large and I approximate my initial condition with the uniform. This is, I mean arguably realistically OK. So in this case, what we want to do is following, we want to consider accepting of our, of our representation of framework that allows us to take this limit in a reasonable way. OK, so here I'm looking at the evolution of vectors. How can I take the limit of this right? So instead I want to consider a PDE or a PDE formulation of the problem of the problem, which allows me to take the limit and goes to Infinity in a reasonable way. OK, don't be scared, I'm not going to do any weird. Things in this. Position. OK, I'm just going to write differential equation which can completely ignore. OK, so the important thing is that now I have to consider an object that makes sense and for which I can take a limit in a reasonable way as I said. So I have to start from a vector XT which is X1T all the way to XNT and this vector is a bad object. Because. The dimension changes. It's not doesn't even factor in a natural way the symmetry between exchange of these laws, which I highlight. So this is bad. Instead, what they would consider is empirical measure associated with this. This vector which is given by one over north sum of delta functions sit in that position 1 OK, so this object was sitting in SE -1 to the north. So. It's bad because N increases this object. On the other hand, it's simply on the set of probability measure on the D -1 dimensional sphere, so it's a perfectly fine object. Furthermore, this object when I hit the limit as N goes to Infinity atomic converges for something nice, but it also converges to something nice independently on the indexing of the particle factors in all the permutation variance of my dynamics that I highlighted. This is nice. Now another nice thing is that because this dynamics is mean field. So this is. Written as a sum one over north times the sum of some non linear interaction term. This dynamics can be expressed as an integral against this. So I can express the vector field moving particles here. Here I can write this as a vector field V evaluated the position PSI, but that depends on all the other particles in the following way, so the vector field moving. Is given by. I can express this vector field V of MU as a function indeed. Of MU so. I can rewrite that vector field which is given by T orthogonal. X. Of one over north sum of P to the beta in product XXJ time J. This is just the vector field that I have. J equal to. One, and this is the vector field up there, I can rewrite this as an integral of this nonlinearity against the. So this is equal to beta P orthogonal X integral of E to the beta XY times Y MU where MU is the. So this is nice because this allows me then to write the equation that transports. So to rewrite the dynamic transports articles here as a dynamical equation that transports transports. Sorry, points in. I can rewrite equation moving MU MU X into an equation MU moving MU, right? So because the vector field moving the point is expressible as a function of the. So I can just leave this because it's. I'm rewriting the whole thing as a question that involves. So what I'm doing here is I'm looking at the equation that evolves beyond and it does so in the way. So by solving this. So again, don't be scared, I'm not going to go into details about this equation, OK? The only thing I would like to carry on from this is that this partial differential equation is the equivalent formulation of this equation in terms of the empirical measure. So if I'm moving points like this, I'm picking the vector fields that I'm moving the points and putting it here. And the this equation is the equation that's moving the empirical measure associated that's in other words, I have my points here space. They are they have a certain differential, they're following the lines of a certain of a certain flow. And this equation is what is moving the empirical measure associated with those points along those lines. Transport of continuity. OK, no need to go into. This it's just OK here, obviously nonlinear. Equation because you have dependency on the view itself inside OK and then according to what you have said earlier, this equation moves the same object as before. So it should satisfy some similar properties. In particular, what satisfies is that the fixed points of this equation, the totally stable fixed points of this equation are totally clustered state because we have seen earlier that this is where the the OG dynamics is driving from. OK, so the delta X star is a stable fixed point. Of the PDE of this. Dynamics. OK, this is a total synchronized state. All the points are in the same. Point X Star. And MU bar 0, which is the best measure, is totally unstable. So what it means is that the the uniform measure is going to be stable because of symmetry, but every small perturbation of this uniform measure will drive the dynamics. Of weight, OK. So the question that we want to ask is consistently with the picture I have shown earlier, what happens? The question is what if the initial condition I consider a sequence of initial conditions MU N 0 and I sample them uniformly from the So I sample part because IID sequence of initial conditions without uniformly distributed in the sense that they are drawn uniformly. This is exactly by the way, the simulation that I've shown earlier, right? The one of the nice one on the sphere. I have particles distributed uniformly on the sphere and then I let it flow and then what happens is that the end of I want to explain this. So the way I would like to do it is I would like to explain in a way the emergence of symmetry in the system through an initial condition that has a much stronger notion of symmetry. It's totally uniform measure. But that as we have seen there is a, there is a build up of structure, there is a build up of clusters and it would like to understand that and emergence of. Or I'm going to Infinity better piece. How do we do that? Let me explain to you how what I plan to do in finite dimensional system. OK, this system, this PD dynamics is infinite dimensional, but I'm going to do something very similar to a finite dimensional analysis to understand how the structure builds up from this sequence. OK, So what happens when you are close to totally unstable fixed point of a dynamical system? What you want to do is you want to linearize the dynamics because they have a sequence of initial condition converging to this. So when a finite time interval nothing will happen. So the picture. I would like to. Think about is this is nu bar. 0. This is a fixed point. It's unstable, but it's a fixed point if I have a sequence of initial condition converging that well over finite time intervals. Nothing. OK. Because it is staying close to the OK, so if. You leave it enough time, so a long time intervals away and we want to. OK, so I want to discuss an analogy in RM. So we consider sequence of initial conditions or space in RM. So YN converges Y star in RM. OK, and I like to consider such a sequence of initial conditions which are exactly the same definite dimensions here and assume that Y star is a fixed point, unstable fixed point. OK, So what will happen now? Well, I have to define a dynamics. So let's consider the dynamics given by this equation. This is a set of equations and since Y star here is a fixed point of this dynamics, F of Y star is equal to 0. OK, So what do we do? If we are good. Mathematicians this point to study how the nonlinearity evolves. We linearize these things right around this. OK, So what we will do is that we take sequence of initial conditions converging to Y star. This is Y. And then we look at the equation telling us how the difference between YT and Y * evolves in time, right? This is equal to that. The equation gives us that this is F of YT minus, F of Y *, F of Y star is equal to 0, and I linearize F of YP around Y *, so this gives me F of Y *. You know, F of Y star plus DF of Y star times YP minus. Sorry for those of. You who are in the back but the same computation. Is so. I'm approximating with the linear. This is equal to 0 like we said before. So I'm solving the linear differential equation because this is the F. And in order to solve this differential equation, what I have to do is I have to linearize. Sorry, I have to deagonalize. This right so I have to. Deagonalize it. Decompose it in a set of eigenvectors and eigenvalues, which here I assume to be the axis of this picture. And then what I will observe is that this solution of the solution of the perturbations will grow exponentially in all the directions of the values associated to the various eigenvectors and eigenvalues. So what I'm saying here is that a diagonalization of this matrix will allow me to characterize the dynamics in a neighbor of the totally unstable. Fixed point. And the solution to this dynamics will grow independently in all of the eigen direction. So in all of the growth modes are the corresponding growth rates. So we have and those give me the growth modes and growth rates, right. So what I'm trying to say here is that we run the equation for. We write now the equation for YT minus Y. So I can solve this because now this has become if I neglect higher of the term a linear system and this becomes YT minus Y star equal to ET DFY star times Y 0 -, y star and this is equal to using the eigen composition. Of. The Fe to the P Lambda M times sorry Lambda JBJ times Y 0 -, y star times. I have done an eigen position of the Growth Dynamics and this will be dominated by what if T is large, which needs to be because otherwise we will not observe any of this point. If T is large, this is going to be approximated by E to the T Lambda Max, the dominant largest eigen value of these dynamics times D Max times Y 0 -, y. So all I'm saying here is that if you consider such a system which has naturally directions of growth that grow faster associated to larger the direction of growth that goes lower, what you will observe is that at the exit of a certain neighborhood, your dynamics will exit in the direction of the fastest growing. OK, So it will be a when I observe A fluctuation around the initial condition. This fluctuation will be to leading order dominated by the eigenvector or the growth mode associated with the fastest growing. Eigen value OK. So all I have to do in order to characterize the growth of this solution and I'm almost done will be the following. I have to simply consider Lambda Max equal to Max K Lambda K and then I will. So this is what happens. Here and then what I will observe is a fluctuation that is dominated by this associate that the eigen vector associated with the fastest scoring and this is an idea to this analysis. This generalizes perfectly well the infinite dimensional case above. The only substitution that the right hand side here is given by a function of MU T or by a vector. So we are in the infinite dimensional setting, but we have exactly the same structure as I have here, only yd is given by. So I'm doing exactly the same computation and by analogy what I get in the end what you get in the end I will present the results only for D equal to 2 is that I will have MU and converging to MU 0 bar which is instead of probability measures. Between 0 and. Five and I have a linearized dynamics, so I'm looking at the evolution of MU T minus MU bar 0, which is equal to F of MU T and top. OK, I generalize MU N 0 sorry. Yeah, N 0 plus MU T minus MU zero times the differential of the flow, and you solve the same thing, you solve exactly the same equation. And what ends up happening is that in this case of this you can identify explicitly the eigenvalues and the eigenvectors, what the eigenvalues and the eigenvectors of this semantics are. This in dimension D equal to 2 are simply the Fourier modes and some eigen values that they can represent. So here I have a generalization given by BK. Here now are eigen functions and these are given by cosine of two PK beta. So this is the Fourier basis. So this operator diagonalizes the Fourier. So no problem. The conceptual level and the eigen values associated with this guy's is simply B beta to the -1 IK beta. IK beta is the modified bested function of the second. OK, So what what this analysis is telling me? This. I have essentially concluded is that at the exit of a small ball for induce delta. For delta sufficiently small, what will happen with my dynamics is that leading order the perturbation I will observe exiting from that ball, where T is going to be the exit diagram. Also, I'm looking at the dynamics exiting. Here so when I notice a microscopic fluctuation from the original condition is E to the Lambda Max Theta times P times cosine. Of. 2P K Max. So what I'm telling you is this. At the exit of a small ball, around the totally unstable initial condition, I will observe A macroscopic fluctuation. This macroscopic fluctuation will have a determined structure A. Structure. That comes from the diagonalization of my operator around the totally unstable fixed. Point. And because I can compute explicitly the eigenvalues of this diagonalization as a function of beta, I can choose I can identify the leading eigenvalue of the function of beta, and as such the periodicity of the mode of the emergent structure from the diagrams. OK, So what what I'm telling you is this. Give me a beta. I can find the dominant eigenvalue corresponding to a dominant mode exiting from the local proportion analysis that I would find, and this will give me the structure of the perturbation because this perturbation has a certain periodicity. Remember that my dynamics preserves, so that structure will be preserved by the dynamics. So in particular, I expect that my clustering phase will have the same structure that has emerged from this. So now I show you just to conclude some results of some simulations that corroborate this message that they're just. Sorry for. So. Let's see if you close or not. I can also just. Draw them. It's. OK. So the idea is to start from a from a initial condition that is uniformly distributed and then you start from an initial condition that is IAD uniformly distributed on the circle. Then what happens is that that's sufficient enough, sufficient time pass and then you will have perturbation of this initial condition that has a certain variability, right? This is very well. It's. Very poorly drawn, but it's let's say 5. Here. OK, and this will eventually build 5E space clusters. Can count 5E space clusters that we only need 5E space cluster with respect to the same structure and this amount of clusters can be characterized directly because of the organization I have done there as a function of. So the experiment that I wanted to show was running basically the dynamics that I have learned above for two or three values of beta. Inferring by this formula here what was the maximum eigen value and therefore what was the number of clusters that one would expect to observe? And based on that was running the dynamics and confirming that that was exactly the number of classes. So that's what's essentially confirming what. So let me just very briefly conclude. I think the message, the main message of the talk here was that there is formulation which is making a lot of assumption, but representing the evolution of token, token dynamics to the depth of a transformer or. Simplified. Performer model as the system and this system seems to display the qualitatively at least a clustering behavior when false. The sparsity of the representation of the developed by the network throughout itself and using some very basic considerations, one can characterize the structure of these intermediate phases that seem to explain better the range representations still start with reach representations built by networks as a function of data. This, however, is asking for a lot of assumptions that are, to say the least, a little bit of a stretch and of course would be very interesting. And I think it's still very open which one of these assumptions want to drop and how the model of this representation will evolve, you know, improve by properties. So. This thank you all for your attention and happy to take me perfectly interpreted with the TSM analyst. So questions please. I have a question. I understand the argument that with this structure of the perturbation you're going to have invariant manifold for the dynamics that preserve the periodicity. But I'm not sure why the final number of cluster is exactly what there is, because you still have a points with the multiplicity of periodicity which are stable in the sense. That's absolutely totally perfect. Let me try to understand that I'm not going to be able to because we are not able to prove that. So unfortunately I was running a bit tight on time. I was trying to like would like to give a bit of a list of what is still open. That's definitely one of the the the reason I think. This is true is the problem. What you can do here is you can have so. There are. 2 answers here. The first one is that this is not obviously not what happens on the long time because here I'm I'm neglecting some correction. So what will happen is that you will end up, I mean, assuming close to this manipulation, but then you would still fluctuate away from it until the final unit. So that's one part. The other part of the story is that the reason I believe this is true, and I would be extremely interested in proving that, but that's proven to be disturbingly, is that what you can use the following. You can look at one periodicity set and then map to this set back to the sphere, basically by unfolding it, right? This gives you an initial condition which is of this, right? And then what you can show and then what you do know, sorry, what you know is that for almost every initial condition, the dynamics of every, almost every initial condition converges to a single depth. That's what I proved for I discussed. This means that for almost every initial condition, you know that the dynamics converge and then automatically then unfold. And it means that for almost every initial condition, The thing is, and it's still very unclear to us, could the initial condition that we get, so the cosine percolation be one of the almost non initial condition that does not converge here. So you could have something like this, which is I guess what you were just about. This is probably stable, but something we don't. The answer is we don't know, but almost almost surely it's not the case on the initial. So it would be very. Surprising, I hope that. Sorry, the future that I'm getting is in a low dimensional sense, but if you do some experiments where D is the dimension of the token is high. I don't know, maybe the? Convergence, I'm so happy because all the things I couldn't stop. Yes, that's great. And yeah, so higher dimensions, it's much more interesting and also somewhat difficult because the it's not so clear, but it needs to be played on the sphere. I mean, on the circle short, but on the sphere, it's not so clear. So what happens in that case is that you can still do all of this. This goes through exactly the same way. So you linearize the operator. The operator is linearized, but not in Fourier, but it becomes linearized in the spherical minds. OK, but what remains true is that the leading eigenvalue term only depends on the degree of the spherical. So it doesn't depend on the index. So what it means is that in higher dimensions you will have a multiplicity of modes that are excited all at the same rate and will compete when they get out of the of the linearized dynamics. So basically what you will have, this is very probably, I mean I would imagine possibly interesting for someone who does random geometry is what happens when you do leave that region. You will have a random superposition of spherical monics and how that then collapses into clusters. I mean we. Observe that we collapse into clusters. But we don't know what is the cluster configuration that converges. So the the the presentation becomes much richer. We know. I mean, we know it converges to clusters because we know that the fixed points of the PD of this PD, the only fixed point that has that is absolutely continuous with respect to the bag is the bag itself, which is totally unstable. It's a fixed point, but you that can be but you don't know which cluster states this random super position of state. So. That I mean if you have any, if anyone has any ideas about how to you know, do this, I would be extremely. Interested. Great question. So from this analysis, it looks like the interesting feature of the self attention soft master attention is the intermediate aggregation rather than the finite class. Would it make sense to change the potential design so that that's a bunch of locally rather than a simple one? Yes, also great question. As it turns out what I presented here is I see, I didn't mention it, but it's a single head, right. So I have only one hand. If you do multi hand, then it appears that that's a generically what happens that you have multiple clusters, locally stable clusters. But then again the question is here we have made some reference choice of parameters and we have studied what happens there. If you if you look at the other model which is the multi had attention model, you have locally stable cluster states, but it's not so clear for some choices of what for other choices you don't. So it's not so clear to what you you can open existence of, but it's not so clear what if then that is actually what happened. So the answer is yes. What? Probably that's what happened.