By the algorithm on some data screen with respect to the sequential risk of the best model of the best predictor in the modern class, the algorithm is making its predictors. OK, so we have this bag of predictors. We are, we're getting a stream. The algorithm is picking every time it's picking a possibly different predictor from the bag suffering some sequential risk of time and we look back after some capital T time steps, we look back and say, oh, how could I, you know, how would have done had I chosen the best predictor in the bag for this particular data stream. OK, again, we are not making any assumptions. So this is everything here is deterministic, there is no problem. OK. Position is can you ever? Can you ever say anything with respect to this modification? Can you can you come up with any reasonable bound? OK, first of all notice that the algorithm must. So if you want to bound that this difference, then the algorithm must perform well whenever there is a guy some some age in your mother class that performs well. So is the second, is the second term small? This term is small and this term has to be small too because otherwise you can bound the difference. OK, so this is obvious requirement and if you think of it is a little bit similar is a is a empirical counterpart of the estimation and statistical learning where you actually comparing that the statistical risk of the model that your algorithm comes up with with respect to this statistical risk of the best model in the class that the algorithm is operating on. OK. So it's you can, you can think of it as this empirical counterpart of estimation of error or variance. And the idea here is that provided some boundedness assumptions, natural boundedness assumptions for the loss, meaning that at every round that this difference cannot grow unboundedly. Otherwise you're kind of hopeless because you have you have no assumptions in the data like the on average this quantity to go to 0. So as this goes Infinity. In other words, the this difference should should grow sub linearly with time so that the average sequential risk of the sequence of models produced by the algorithm will converge to the average sequential risk of the best model in the class. Everything. OK. So that's the kind of game we are playing here and we're not going to see algorithms that actually achieve this goal. And we can provide the precise rates for a convergence to zero, OK. And this rates will be pretty, pretty accurate. OK Any questions so far? Yeah, I was. Wondering because you're updating your model like for each new data server, but it makes sense to update it by, yeah? Yeah, yeah, yeah, absolutely. So the question is whether you should really have to every step in in in practice people like may actually use mini batches and do updates every, you know, every once in a while because of different reasons. No, because you will react less rest speaking, maybe there is a trade off, you know, but definitely you know, this is the most you may just ignore the data, maybe store it in little cache and similar talk. This is generally not typically the algorithm will will do aggressive updates. But you're right in practice people may not trust to do aggressive updates because otherwise the model drift somewhere switch. OK, OK, cool. Yeah. No, no. I mean, this is just a comparative in statistical learning. You do this. We're not going back because we have no measure. We have no probabilities. Forget about. Yes, no, I'm it will change the time. So you know if you change the capital T over here, the infimum will be possibly achieved on the different model. I'm competing against the single best model. OK, you may object. There is no if there is no single best model that performs well on the data sequence, then the problem is easy. You are you are right. So there is some notion of that then this, if you know this is did, this is what I mean by here. If there is some single model, some good model from that stream, I want to find it. OK. If there is none, I don't care. Because I don't like, Because we want to see. Because we want to see what you can do without stochastic assumptions. Can you do machine learning without statistics? It turns out that you can. And why should you do that? Because probably do not exist in reality unless you go down. But yeah, could you please tell me that about regret things, what you want to do? Yeah, That what you want to do. You want to bound, you want to bound that quantity as a function. You want to make it, you want to show that it doesn't grow linearly. All right, so good. So now let's make the model a little bit more formal. OK, so I'm just going through the same side as before but I'm changing a little bit just to make it more more abstract or not less abstract depending on your viewpoint. So now my model space is going to be some boundary that comes close to no empty subset of Rd. So it is a parameter set. OK, I'm assuming my pictures are parameterized by D numbers in belonging to some to some subset of Rd. and then I am assuming that right? So now HDI can call it the WT because it's a parameter vector. So it's whatever you want to think of it. Any predictor that you can parameterize with the real numbers. There's lots. And then I have my and I have a loss function and T and the idea here, yes. And the idea here is that that I am and you see here now watch this. I am absorbing the data points inside the loss. So instead of writing explicitly, XTYTI am just putting a subscript T in the loss. So now I have a sequence of loss vectors that are convex. And you can think of them as as these objects here if you like. But otherwise, I'm not more up success just to see if you're not affordable with that, then think of this. And then and the update is going to be typically, you know, from WT, I go to WT plus one and I yeah. So what kind of information do I need? Typically the algorithms are going to use in this in this tutorial will be based on the 1st order Oracle. So they will have access to the gradient of the current loss or the current loss. And this is the kind of information. So they will be yeah, you know, I still there would be variance of online variance of which makes a lot of sense because we are assuming complex losses and we are assuming so. Yeah. OK, very well. So now you can write your beautiful regret as this. And actually you can compete against any you instead. Before you were competing against any predictor in a script page. Now you're competing against any vector which parameterizes the predictor in the in the D and you can parameterize your regret by you and this is assistant or you can go like I did before. So you are competing against the best in your in your set. OK, it's the same thing. So this is doesn't happen because I'm taking infinite here. So you're competing with the best, but in general, I could be competing with any new. OK, so same, same thing as before. No new concepts, just the just the writing and here we go. So the first time when I want to come up when I want to suggest you is kind of a stupid algorithm because I'm doing online learning, but it's also the most kind of natural. So what do you do in statistical learning difficulty? Oh, the first thing they tell you is empirical risk communication. So you take your train set that you have your class of models and you want to pick out the model that minimizes the empirical risk, also known as training error. So maybe this is so this idea kind of kind of sucks here because it's really define the purpose of being online. So if you want to pick the model that minimizes the sum of past losses, you somehow have to remember all these past losses. So it's as if you were, you remember it all the past data in the stream. So you're not not really being online, you're just assimilating online using a standard algorithm that minimizes empirical risk every time having a new data point to the to the training set. So it's kind of stupid, but we won't understand that, you know, to understand that, you know, whether I get anything out of this idea. And then we will see that this idea can be massage that is something something nicer, less stupid and more and even implementable in mind sense. OK, so that's the definition. That's the minimizer over here, the practice of my stream. Now my stream is the secrets of Congress losses, and now I'm minimizing over the first T convex losses and a cool depth minimizer WT plus one. OK, so that's my current model. Yeah. And so we're going to prove a very simple lemma, and that will help us control the regret. OK, so this is my definition of regret. And so I start from a division regret and what I do, I want to remember, I want to control the regret. So that's my goal. I want to regret for this specific algorithm. So now by definition, by this definition, I can write this minimizer as WT plus one. Look at this definition and you match it over here and you realize that the guy that and the guy minimizing this quantity is going to be we just, we assume that is achieved in D is we call it WD plus one. OK, So that's easy. We just say, OK, we call that minimizer. By definition, we call it WD plus one. OK, now we can say introduce this notation with capital Lt. is the sum of the first T losses in the stream. OK? So L0 will be 0 because I'm summing with 0 losses. Otherwise, this is the function which is the sum of the first T losses. All right, so now I can write this guy. I can write it as a difference of a sum. So every individual guy here will be a difference of the sum of the losses. This kind of obvious and the sum of the minus. The guy with the minus here will be just a capital L because I'm summing the losses with the minus in front. So that's kind of trivial. And then I can just observe the following fact. Oh, but the first summon here for t = 1, I get N 0 here, but N 0 is 0. So this guy is actually 0 when t = 1. So I can shift the entire sum ahead and incorporate this last guy over here and this. And then it becomes this. This is only quality, OK, yeah. So what I I just did that. I observed that for t = 1 and 0 is 0. So this so the first, the first in the first term, the minus, the minus guy will be 0 and I can shift that. So I get this very well. And then what happens? OK. And then I have this identity here which tells me how I can bound, I can bound the regret in terms of this difference between commutative losses. So let me see now how can I can I get the handle here? And this is something that that is actually I will need some assumption to control this difference. So this difference is the regret is a rewiring of the regret. So in order to control the regret I want to control this difference. And to do that I'm making some assumptions on losses. So I'm now assuming that my loss, all my losses are going to be strongly convex with respect to some form. And this means that they they have a non negative curvature on any point of their domain in any direction. OK, if you like the smallest eigen value of the Hessian is lower is bounded away from zero if they are, but otherwise this is the definition. So right And this is what I just said they need not be twice definition but if they are, this is equivalent to that. So there are many losses that are strongly convex and for instance, well, this query deutilian norm is 1 strongly convex with respect to deutilian norm if you like that as a loss. The negative entropy which is a convex function also is one story convex with respect to the one norm over the probability simplex and so on so forth. So there are things that are strongly convex, fine. So we are assuming our losses are strongly complex. And another thing we are going to use a lot is the 1st order optimality for convex functions. You'll be doing optimization. I understand that. So you know what it is. It is that if I mean if I do a constrained minimization of the convex objective, then at the at the minimizer in the in the in the in the physical domain, I have no descent directions inside that my physical region. Otherwise I could minimize more OK. And that then it comes to this condition here where that will start with the minimizer. OK. So this is some standard that first of the optimality for convex functions constraining convex optimization, right? So now the other thing that we assume that this is again it's a sort of a mountainous of the loss. So we need to assume that the losses besides being a mutual economics also the Lipschitz with respect to some more OK, so they they don't they don't go wide, but they can grow by bounded demand as as prescribed by the Lipschitz constant. OK, so now what happens here? So now we are applying this empirical risking miser online over this and we are using a strongly convex losses that are also Lipschitz. Now what what kind of confusions we can draw. But you know, if you sum up that many strongly convex functions, the strong convexity is just to go up because they are bound, you know the curve, they have positive curvature in that direction. So the curvature adds up. So you get a MU T strongly convex. If you sum the T MU strongly convex functions with respect to the same norm, obviously. And now we can exploit this factor together with the first of the optimality. So let's go back at the follow the leader prediction and the online minimizer that minimizing the sum of the past process in my model set. Now what happens? Yeah. So this is just the first order optimality condition, OK. This is the first inequality is the stronger convexity. So the sum of the losses, remember St. is the sum of the losses is the MU T strongly convex function. So by definition this is true. And MU T is a stronger convexity, strongly convex coefficient. And now, because WT plus one is the minimizer of Lt., you see here by definition I can use first order optimality and drop this guy that is going to be positive and end up with this lower bound here. OK, I use the first inequality, I use the muti strongly convex of Lt. Second inequality. I use first of the optimality of because WD plus one is the minimizer of Lt. very well. So now you can just do some simple manipulation. You can look at this difference here and you can write it just like the same difference. One thing one step earlier and then you add up the losses that were missing. Remember that it is the sum of of the losses. So this is just to be the algebra. And now because the WT minimizes NT -1, you know, again here I know that that this difference is going to be negative, OK, again, because WT minimizes NT -1 so this difference must be negative, must be normal positive. Therefore I can upper bound that this by that. And furthermore, by using Lipschitzness of the losses, I just that over here, yeah, I have this upper bound. So now I I can put together lower bound for this difference, upper bound for this difference. I solve and I get that. So now I know that this quantity is up mostly this much. I plug it back in here and I get this upper bound. But remember that this difference through the the Lambda that we proved before was just equivalent to regret for some of the time, OK? So now what we can reduce is the following code regret. I can rewrite it that this because I proved the lemma that gives you that. And yeah, but I just proved that this difference is of most this. Oh, cool. But that everything is a constant. And I left it with a harmonic sum which is the sum of Ouch, the sum of yeah of 1 / t So everything else is a constant. So this is the parameter strong convexity parameter and this is the Lipschitz constant. So what I get is that my regret is upper bounded by a logarithmic function of time. So it's definitely sub linear. OK so basically I am converging. So if I use this algorithm, I'm converging to the best predictor in the class at the rate of LNT over T, which is a reasonable rate and is not improvable under these assumptions. OK, so this algorithm is working. It's kind of a stupid algorithm because it's not really online. But under this strong convexity assumption, it is giving me some nice upper bound on the regret. The regret goes and grows only like a log with respect to time. So I am definitely learning because I, I may assume that if I, you know, if I pick their models at random, I'm just doing random conditions. I my regret that we typically grow linearly with time. So this grows logarithmically with time. And again, it's the optimum weight without any further assumption. OK, so now, but we have a strong assumption. The strong assumptions is that losses are strongly complex. OK, it's true that many losses are from the convex, but many other losses, interesting losses, are not from the. So now what we can. So what is the problem? What if we apply our algorithm to a loss that is not strongly convex, maybe to a linear loss? Let's take the least known strongly convex loss, which is still convex, that is a linear OK. So now, so let's assume that we try our algorithm on linear losses, OK? And we are just our model space is 1 dimensional and it's just the the segment between -1 and 1. So our models, our predictors are parameterized are linear predictors parameterized by a single scalar. OK, so suppose we start from zero, that's our initial predictor 0 because that, you know, it's kind of a reasonable startup. And then we get the first, the first loss, it's a linear loss. So what is our prediction? OK, our prediction, you know, what is the value of the model 0 on that loss is zero. Look at the value of that loss at 0 is going to be 0. So the, our, the first loss of our algorithm is going to be 0. Now we apply the algorithm and we upgrade. What do we do? We want to minimize the past loss. What do we, how can we minimize the past losses? Just this in order to minimize it, we have to take a model all the way to the left, OK, because that's where the minimum of this loss is. So that's our next model. And now a new loss comes, comes along and oh, this is it. And now we suffer loss of one. OK, we suffer loss of 1. And now, OK, fine. And now we are minimizing the sum of the past two losses. And if you sum the past two losses, which are two segments, you get another straight line with the coefficient minus minus 1/2. So your next model to minimize minus 1/2 of West, you have to predict all the way to the right. So that's your next model and that's your next loss. Bang, you're just get another one loss. Now the sum of the past three losses is the West halves, OK. And now, so again, you predict all the way to the left and then the game goes on and off. So except for the first round where you pay 0, for all the next rounds you're going to pay and you're going to pay one, OK. But you know, and the losses will be all, you know will be all like this. We will alternate between this and that, OK, But you're just off scene and you're going to pay 111 because you're being, you're being slammed on each side on each extreme of your decision domain. So the problem, OK, so, and of course there is a very, a very good model in your class, which is 0, the zero, I will have a zero loss among this among these linear losses. So if you if you predicted 0 at every single step, you would get a loss of 0 at every single step. So the best model of loss 0, your algorithm which is which predicts with the smallest the model minimizing the sum of past losses and lost one at every step, which means that the regret goes linearly with time. So what's the problem here? Well, the problem is that this algorithm is definitely, you know, is unstable, OK? I, my mother, you know, keeps on going on opposite the size of my mother's face because because there is no curvature in the losses and the curvature helps to keep the model stable. And this is something that you guys know now you know, that is statistical learning stability is important. And it turns out that even in online learning, stability is important. So you have to stabilize your algorithm to make robust two situations in which your losses lack curvature, OK, which is a kind of like a hinge loss or or situations in which your losses is not doesn't have the positive curvature, OK. So we want to fix the algorithm and make it so that it make it would be a robust that to wow, yes, robust that to known no strongly convex losses. We still need convexity, but we want to at least learn when losses are linear. OK, So how do we fix the algorithm? So yeah, yeah. So what is the standard trick that we use in statistical learning? If our algorithm is unstable, we stabilize it by adding a regularizer. We know this recipe. It's a kind of a standard regularizer, the algorithm by adding a function that is in our case, it will be a strongly convex function. It is some auxiliary function that we just put there in order to make our predictions more stable. OK, so and hopefully control the regret by by doing that. So now here's the follow the leader with this additional stabilizer which is we call it regularizer and we call it the follow the regularized leader. This is technology invented by which yeah, I'm not responsible for so don't working on it. Yeah, right. So this is nice. So now The thing is, yeah, oops, right. The thing is we are interested now in two things. So first of all, that doesn't work. If we tend to realizer follow the leader by adding a function that sort of a dumps down these oscillations, we get actually control over the regret. And also can we compute this function in this minimizer in an online setting? So maybe the local adjustments and without remembering everything from the past, we will discover this in the next episode which is coming up now. So this is like a picture watching. So we are watching many episodes. If you are watching OK, second episode. So we have a little what's happened so far? What happened so far? So you have a question of 1. And yeah, so now here in this second episode that we will look at how we can compute such a thing. And in the course of you know, this reverse of that, we will discover some interesting properties of this function. OK, so we have two. Problems First, we want to compute this in a reasonable way that is also line OK without remembering the plus losses. And 2nd, we want to boundary OK, we know we know how to do it when all the functions are are. So in convex, we don't know how to do it when all the functions all the functions are are just OK. So OK the first three, can we use the the fact that the function is the losses are complex. So if losses are complex, it's by using Taylor's theorem I can just upper bound this difference with the 1st order Taylor expansion. OK because I know that the second the 2nd order term in Taylor's theorem will be negative in this case because because the function is is complex, so the action is positive definite and blah blah blah blah. So this is just a simple consequence of convexity of Lt. OK. So remember, WT is the model produced by the algorithm, U is a comparator is some best guy in our class we are competing with. So the regret that is just the sum of these two difference and you will pick it the arbitrary in general would be the minimizer of the sum of the losses for the first two time steps. OK, so that's the first 3. And now we can look at this surrogate of regret, which is an upper bound of the regret. So we if you minimize this surrogate, we will minimize the regret. Cool. So we can just pretend our losses are linear now, because these are linear losses and we have this thing which in which we look at the same algorithm as before, but instead of the original losses, we replace them with the linear losses. Now you feel a little nauseous because this, you know, this is a linear loss whose coefficients depend on the prediction of the algorithm. What is that? But this is totally cool because we make no assumptions on the way our losses are generated. OK, so it's fine. The losses can be generated, you know, and depending on how the algorithm works and but still they will properly upper bound that the real positive interest, which is the regret. So this is a dirty trick that we can use because we are working in a non stochastic setting and so we can define losses anywhere we are OK and this is a linear so it's fine. It's definitely complex. So now the algorithm looks like this. It's like the algorithm we had before, but now losses are linear. So this looks nicer, but we still have a constrained optimization to a problem to solve because we are minimizing W with this here and here. So let's look at the solution of that. So again, we are interested in in having some properties of this regularizer and one if the regularizer is strictly convex, we know we can define and differentiable, sorry, and differentiable. We know, I know we can. We know we can define a quantity which is called the Bregman divergent and the Bregman divergent. If you look at it, it's just the error term in the second order Taylor expansion of the function. OK, so this is the same measure of how bad there is a linear approximation of PSI with respect to the true, the true function PSI. OK, so it's now I yeah, it's an error in first relative expression of PSI around West. And if the function is sorry, if PSI is new strongly convex with respect to some norm, then the Bregman divergent is always at least a quadratic in that form. OK, and with some yeah here. And if it is twice differentiable, then it is literally the sorry direction is literally the 2nd order error. Turn the tailor expansion in the 1st order, expansion error in the 1st order expansion, which is just the 2nd order. And you know, for instance, if if we use a Euclidian squared Euclidian norm as a regularizer, then the Bregman divergent is going to be a Euclidian distance squared. If we use the entropic regularizer and our decision set is in the simplex, we end up with the K divergent. OK, so this is the sort of generalized notion of distance which is not symmetric and has different special cases according to each strongly. Convex. Strictly convex function sign. OK, so we want to use this tool here to make some reasoning. So the idea is that you know the important thing is this theory. This theory is there in the following. So if you want to minimize a function which is strictly converse a differentiable over some domain value, what you can do is first you do an unconstrained optimization. OK, so this is your minimizing the unconstrained the same function and then you do a Bregman projection where the Bregman is induced by the functionality and you do the Bregman projection over the physical domain. OK, So essentially this test you that you can do constraint optimization by doing unconstrained optimization plus projection. OK, this is some factor of life of optimization. I'm not here to say anything about it. So now we can do F we can in our original problem, we can do F as the sum of these two things, where I'm just let you know, I'm just summing up all these gradients in a in a big G. So now this this sums become AW, transpose big G and this is our FW. You better with a regular visor. So now I want to minimize this guy. OK, I have to take the Bregman diverge. I have to minimize unconstrained this, this thing and then do a Bregman projection with respect to F. But actually things are even better because the Bregman diverging with respect to F, which is the sum of the two, is actually the Bregman diverging respective side, because Bregman diverges invariant with respect to the addition of the linear function. So it is the projection I have to perform. It's only the terminate by the regularizer. They can use it, not by anything else. So now I can and my algorithm is going to do like this. I'm going to do a unconstrained optimization of the complex function plus a Bregman projection according to the strongly complex regularizer that I have. OK, so now I have a path to to come up with a nice and efficient category. OK, yeah, the Bregman projection, I have to do it efficiently. I have a way of doing it efficiently. So I have to pick V and the PSI so that I can project onto PSI onto V according to PSI in an efficient way. And this is doable for natural regularizer functions and natural choices of V OK, so let me go one step further further. Now we are asking something. OK, so this is unconstrained optimization. Isn't that the case? I can get the closest form solution for this guy so that I can just have just have to worry about the vegma projection and everything is taken care of. OK, let's take a look how we can get the closest form of this guy. OK, so we will assume we need at least convexity of our regularizer, but actually we will use a strong convexity. OK, so this is the thing we want to minimize and restrain it to to find before the Bregman projection. Now I can flip the sign and I can call the TTA which is a dual variable that the sum of the gradients of the losses. So the admin because I flip the sign becomes an art Max. OK, it's the same thing. Just change it the change it sign and change it the name of the OK. This thing is as a name is the the maximizer of this difference is called the convex conjugate of the regularizer sum. It's a something that you do that you're studying convex analysis. So every convex function as a convex conjugate which is defined by in the following way. OK. And Theta is a dual variable with respect to that. OK, no, sorry. Oh yes, yeah, now good. So now we can we can now do this on Max and and we know. That the maximizer is a dual conjugate. But I don't need the maximizer, I need the Argonauts. And because that's going to be my the solution of my unconstrained optimization problem. It turns out that there is a nice theorem that tells you that if the regularizer is strongly complex, then the dual, the convex conjugate function is differentiable and the maximum here is achieved at the gradient of the dual of the regularizer. Oh my. OK, this is a little complicated, but this is actually the closest form I was looking for because that's what you see. I can W prime is actually equal to the gradient of the Phi star. PSI star is the convex conjugate of my regularizer evaluated at the sum of the gradients of past. Oh, let's, let's wrap this up. Yeah, this is what I just said. Let's wrap this up. So now the algorithm looks really nice. OK. So basically on one hand that I'm just doing a gradient update, which means that I'm accumulating the gradients of my losses. So I see the data besides some loss, I get the gradient of the loss and I'm accumulating the gradients of these losses. Now I have to make an update, OK, I have the sum of the negative gradients here. This is the sum of the negative gradients. I start from zero. Now in order to get the prediction, in order to get the model, I get through, which is called the mirror mapping. So I apply the gradient of the complex conjugate to the current sum of the pass gradient of process. And this will get me the solution of the unconstrained optimization problem because of what I saw and because what I said before and then bang and do my beautiful Regman projection. Hopefully, you know, fingers crossed I can do it efficiently. And then I get the final thing, which is the solution of the constrained optimization problem. So again I use this trick, I use first linearization, then I use this convex duality to get the closest form for the solution of the unconstrained optimization problem. And then I use that theorem I showed you before. Or if I want to solve the constrained optimization, I can solve the unconstrained optimization and then project using the. And that's what I end up. So now this algorithm to see it's an online because basically what I'm doing is that I'm keeping a single variable which stores the sum of past gradients. This is a single variable that is the sum of gradients and everything. Every time I need to make a prediction, I just do this mirror mapping iteration called mirror mapping iteration followed by a fragment projection, and then I get the prediction. Yeah, yeah. So any I know I went a bit fast. I'm sorry. I'm sorry. So maybe you may have questions, complaints. Yeah, question I was. Wondering about the convergence and you were saying OK, the losses should be like if these conditions are compatible. Maybe the bottleneck I'm thinking about like smooth functions. OK, Yeah, bound the domain, bound the domain. No, actually, actually no. Yeah, Yeah, this is, this is true. This is true. Yeah, you will definitely depend on the on the on the issues, some way of bounding uniformly the issues from the function otherwise because yeah, she said make no assumptions otherwise I can, I can take you very far. The function can, can. OK, Yeah, there was this reasonable and yeah, over there. Question about the. Oh yes, OK, we see exactly. For projection we don't have closed work solution exception, so there's either going to be. Is it OK if this is not exact argument of the Bregman projection? Yeah, yeah, yeah, it is OK. It will add some contribution to the great. So, yeah, so actually you can do it in some natural cases, which are typically Euclidean and entropic, which will be the the leading. Yeah, it's V is crazy is V is like some crazy polytope. And then OK, examples, these are important. So first example of the case and the other is called lazy online gradient descent. Why lazy? It's a long story legacy. So now we want to do online learning. We have convex functions but not necessarily strong convex. And we use Euclidean regularization with the learning rate parameter, which will be the stronger convexity parameter with respect to Euclidean. So this is our regularizer and ETA. Think of it as a learning rate, right? So now the dual of Euclidean is is everything is self dual. So the dual is going to be the same thing. It's just that the strongly convex parameter flips over and yeah, then you take the gradient of this, you can see that, you know, take a die is going to be that. So the update that is trivial because we are just, yeah, we're just accumulating gradients. So we know how to accumulate. Gradients. It's just we sum sum them up and the gradients will have the and the. Yeah, the because of the Bregman divergent of the square Euclidean is the Euclidean distance. This is the standard Euclidean projection or projection onto the onto D OK. And yeah and that's it. So the algorithm is is very easy. You just it's literally projected online gradient descent. So you're just doing your other line, you're just doing great projected gradient descent, but on the sequence of functions that is changing overtime what we have, OK, the entropic case. So your decision set is now the probability simplex because you're doing probability assignment that you want to send probability to events to a finite set of events. You want to manage a portfolio of finite number of stocks. So this is all the cases in which your decision set is the probability simplex, right? So now reasonable regularizer is the negative entropy because you want to be convex set. In particular, you will be in this case one 1 / 8 strongly convex with respect to the one. Now you can have fun and computed the convex conjugate and which is the log exponential. Yeah, but you get computed, it's fun and you can compute the gradient of this thing which is you can see it again by naked eye, which is this thing which is actually already projected in the simplex because you are normalizing. So you in this case, you know once you computed the this is self projection. Once you computed the at the gradient of the dual, now you get something which is already projected in the simplest. So you don't have to do the the projection, the break bound protection in this case. And what the algorithm is this? So your basic is called exponentiated gradient because you are predicting with the exponent of the accumulated negative gradient. OK, this is an algorithm that has been studied in well, an example. It is an example of mirror of the same thing comments of musician, but the online version of it was independently discovered by other people. So we can have these natural examples that we can use to do our, we can run and this will be general, generally online algorithms because all you can do to run these algorithms is just to keep a variable with the sum of the negative gradients and then perform these projections, which in these cases are relatively simple. Very well. Now we go for the third installments and we have 1/2 an hour. So once we know how to compute these things, we want to we want to. Now we want to analyze the regret. Yeah. So the last episode of this short series is the analysis of this algorithm. OK, So we will analyze this algorithm is the same. We don't care at this point when the losses are linear or linearized or the original losses, Everything I'm going to say will hold in both cases. So we can just think of doing the analysis for the original losses, OK? In practice, what you do, you run the algorithm with the linearized losses in order to avoid the computational problem, OK? So W start is going to be the best model for the First Capital T type steps. So this is the guy that we typically measure regret against, OK? And we're going to make our prediction in the following way. OK, and in practice, alien will be linear, but conceptually with deep of alias complex, right. OK, So what is actually a all in angry with some regularization? Well, it is like a standard. It's like, but with some original laws. OK, so I can view regularizer as some initial loss. I am born. So yeah, you know, some people are born with the original scene. I'm sorry for them. And so those people can think of this zero as their original scene. So all the algorithms, all regularized algorithms are born with some something they they they seen, they've committed and they have somehow now atom with respect to that. So now, OK, why this is important? Because now I can see follow the regularized leader as a follow the leader because I can deal with the regularization and some close that I get from the beginning I got from the last Adam and Eve got me this. So now the algorithm will predict that it's like follow the leader at the beginning predicts, predicts by sorry, by minimizing the original scene, which amounts to minimize the regularizer and then goes on by minimizing the sum of past losses, which now includes the original scene, also known as regularizer. So it is the kind of job it is true that the original scene regularizes your behavior because it makes you more fearful what you do now. I thought it was a good job. I think it's true. You already start out with the scene, so you want to be careful. So it's true it becomes a lot more stable. Yeah. So basically now we can revisit the proof of the stability land. Now we had for all the leader follow. The leader has a strongly complex losses. Here we don't have strongly complex losses, but the regularizers will be strongly complex. So we have some strong complexity in the sequence of losses about only only in the first loss in the very first one in the original scene. So now is the proof. Again we want to control regret by some kind of stability, but this time we go by induction over capital T. So we want to prove that this thing, We want to prove this inequality by induction. And from this inequality we derive the bound on the. So this would be very easy. Base case equals 0, so this becomes this inequality. But this is true because W 1 is the minimizer of N 0 by definition West one is the minimizer of the zero. So yes, it is true that L0 of W 1 is actually equal to the infimum over D of L0 of U. OK, cool inductions step. We assume this is true up to T -, 1 and we want to prove it for T standard induction. So we call this guy. OK, this is we pick this WD plus one as the guy over here which is the the minimizer over the entire series. This minimizer over here WT plus one is W star which is this minimizer by definition previous slide. So this is true for any U in particular for the infimum by that assumption. So I'm taking a particular U which is WT plus one, which is the minimizer of the sum up to T, not the sum up to t -, 1. OK, so I can do that. Now I'm adding this on both sides. So this goes up by 1 and this what happens here. Oh, here I'm adding the same thing. So this also goes up by 1. So yes, so both, both quantities goes up, the sum go up by 1:00 because now, yeah, you can see it by yourself. OK, but now I'm noticing that by my choice of WP plus one that this is the minimizer of this. And this proves what I want Now that I prove the I proven this by induction, I can massage this this inequality. And by massaging this inequality, I essentially sorry. Yeah. No, sorry. What? What I'm doing here at the moment, I am yeah. Yeah. I am using this inequality to take the regret this the definition of regret. And then by using that inequality, I can upper bound the regret in the following way. This is actually some algebra. It's nothing, believe me, there's nothing really deep in here. It's just this is definition of the vector and this is a massaging of the terms which uses this input. So nothing strange going on here. Notice that this thing start from zero but the regret starts from one. That's the reason why I get this N 0 over here. But do remember that N 0 is the original scene which is the regularizer. So this is the constant term which is just the difference in the regularizer function, OK? And this is the real thing that depends on time that they need to control, OK? So the last step really will be controlling that this difference over here and this will be done by just using the same thing we used in the case of. So first order optimality is from convexity and so on. OK, so I need that my regularization is new, strongly convex with respect to some norm. And what happens now I pick some learning rate and now my if I introduce the learning rate in the regularizer, this function becomes numerator from the convex. Nothing strange. I'm assuming that my losses are Lipschitz with respect to the same norm here. And here is my algorithm which is regularized. Now this is a strong convexity. Notice that the sub convexity is MU operator which appears over here. And this is a first order optimality because WT is minimizing the Lt. -1. OK. So same thing as before, I do the same inequality by just the shift time. So it's exactly the same argument, but now time is shifted. OK, OK. And so change here, there's a change. And here it doesn't matter because I have absolute values. And now I sum up these two inequalities. And this minus that is this guy, because these are sum of losses. And this minus that, sorry, this minus that is this guy. And this minus that is this guy. And now, yeah, I summit the distributed qualities. I sum also the other side. And I get that, OK, because it was in the same way, OK, very easy. Now I use Lipschitzness of the loss. Now I can resolve. So I from here and here I get that this is at most that, but now I know there's this difference. By substituting this again, this difference is at most g ^2 ETA over MU. OK, now this is what I have done. Remember the regret? I know the regret was controlling. It was exactly controlled by this difference. So the great is controlled by this term, which is constant and the sum of this difference. And now I just got a bound on the sum of those differences, which is this bound over here. So now I can't get my bound on the regret. These are the constant stuff that depends on the learning rate. And then I have this thing over here. But now I know this thing over here is just bounded by XI squared Lipschitz over ETA S of sorry over MU which is the strongly convex set of the visor times Theta which is the learning rate that also appears here. So now I can trade off the 2 terms by choosing the learning rate and the OK. So this difference is essentially the diameter of the decision space according to the visorizer. You can do this as something that diameter of the decision space measure of the visorizer. So it's some constant quantity that depends on the D side. And then by choosing the learning rate, I get the bound on the regret of this form. So now you see the bound on the regret depends on the Lipschitz constant, on the diameter of the decision space and measure it according to the revisor, and on the square root of time. So before we had the log of time, because we had the next assumption, the function was strongly convex. Now we have the square root of time, so the function is growing. So the regret is growing sublinearly, but faster but still sublinearly. In particular, we have convergence to optimal to the optimal predictor in the model space at rate 1 / sqrt t, which is optimal, as we will argue in a minute. Yeah, OK, now let's look at this analysis in the two special cases you saw before. So in order to do the workout, the bounds for the Euclidian regularization and the entropical regularization, we have to revisit the few things, the notion of dual norm. But I'm sure you everybody knows what is dual norm. Yeah, some examples dual if you have P norms and there's a relationship with convex quality or then there's as beautiful inequalities, additive and multiplicative. But this is just for fun. But they are not being used here. But just to remind you that there are beautiful things that have to do with the dual norms and convex conjugates. Now the only thing that we really need is this one. If you have a loss which is a differentiable convex function, then the loss will be Lipschitz with respect to the dual normal of its gradient over with respect. Yeah, with respect to any norm. So if I pick a norm, then the function will be Lipschitz in the dual normal of its gradient. OK, yeah, it's kind of a sink in the way this is phrased. In other words, if you have, you can view the Lipschitz constant as the normal as the normal of the gradient. OK, I don't know why there is a normal there, but let me see how I'm going to use it. So now suppose that you have a loss and you have some bound on the components of the gradient. OK, so your loss is such that over the domain, over your decision space, the components of the gradients are bounded by a constant, OK, right. So this means under this assumption I can say that the function the loss is square root of D Lipschitz with respect to the Euclidean norm and Oh yes, sorry, Lipschitz with respect to the norm. I'm just so yeah, I want to say that the function is Lipschitz with respect to some norm and the Lipschitz constant that is defined that with respect to the gradient and the dual norm of the norm with respect to the fact with respect to which the function is Lipschitz. OK, it was a little bit weird, but it is absolutely OK. So now I have that the loss under this assumption, any loss is square root of any loss that satisfied this assumption is going to be square root of Lipschitz with respect to the Euclidean norm and constantly with respect to the one. Any loss convex that satisfied this assumption has these two properties. Thanks to this, yeah. So let's go back to our two algorithms and projected lady in the centre. So we take B to be the closest depleted ball of radius D. We take the one that's the same as before. We use Euclidian regularization. The diameter of the space is just the according to this, to the square of the Euclidian norm. Sorry, the square of the Euclidian norm is just the square of the radius. Just trivial. OK, because the PSI is just the square root of naughty. So that's our diameter. Now the Lipschitz constant that is square root of D because we are using the Euclidean regularization. So the bound we had before translates to this. I'm assuming MU is some constant. I don't care. So D is D which corresponds to the diameter of Euclidian ball, and G is the square root of D so I get something like that. So now in the case of online gradient descent, my regret depends on the diameter of the decision space. Makes sense according to Euclidian distance and the square root of time times the dimension of by the decision space between the dimension of. Very nice, very clean. Now what happens when I do entropic entropic regularization and then I'm later done. OK, we call this exponentiated gradient. Now decision space is the probability simplex. The regularizer is the negative entropies which is 1 strongly convex with respect to one more. The diameter of my of the simplex according to the entropy is just a log of the dimension number of dimension of the simplex. The image, the measure of the space in which the simplex leaves the Lipschitz constant is constant. Because I remember that's what happens when I use the because of the argument we did before in this case. Yeah, OK, because the function is 1 struggle in the river. The one struggle comes with respect to the one norm. The dual of the one norm is the Infinity norm. So I'm looking at the Infinity norm of the gradient. But we said that the gradient has the constant components. So the Infinity of the gradient is constant. So that's why we have a constant here. So now the same bound as before D actually, yeah, this should be d ^2 because this should be described as the time force. So I get that in D and comes inside the square root and then G is just a constant. So I get the boundary like this. Now you see there is a dependence on the number of dimensions. So what happens now if I run on Lagradian descent in the simplex? If I run on Lagradian descent in the simplex, I get the boundary before that I had before. Now the Euclidean diameter of the syntax is 1, so D goes away. But instead of having a logarithmic dependence on the number of dimension, I get the linear dependence on the number of dimension. So the bottom line message I cannot push anything more. I think is that essentially the geometry of the decision space. So there are two things at play here. First of all, if you want to do online learning, you have to be stable. In order to be stable, you have to regularize. In order to regularize, you have to pick a strongly convex function because you have nice computational properties that allow you to compute your online algorithm. Now the question is how which is strongly convex regularizer? Do I take? Oh well, that depends on the geometry of the decision space that you're working with. I gave you 2 very basic examples, but there are more complex theories that take this into account. The simple example is this. If you have a Euclidean domain, you should take a Euclidean regularizer. If you have a because of, that's the natural geometry of your domain. If you're, if you ever, if you're working on some new nuclear domain like the simplex, then you should, you should match the natural geometry of your domain, the simplex feedback of the regularizer. So in this case, the matching regularizer is the entropic regularizer that that we give you. You see, the dependence of the time doesn't change. You still square root of the, but what the grammatically changes is that the scaling of the regret with respect to the parameters of the problem. You have an exponential worsening of of the scaling factor from L&D to D if you mismatch the regularizer with respect to the geometry of the domain. OK, these bounds, these bounds, I'm not going to tell you anything, but there are a few slides here in which I can prove that these lower bounds are tight. So in the Euclidean domain, in the Euclidean domain, you have to pay this kind of bound, where this is the diameter of the domain is the Euclidean diameter of the domain. And in the simplex you have to pay this bound, you know, up to constant. So those bounds are tight, which means that the regularizer are the best possible for the geometry of those domains. And the general question is, if I give you a decision space, they find it in a certain way. What is the best? What the regularizers should I use? There are different complex theories that we try to answer to this question that have to do with self comportant optimization and more. Of course, there's lots of computational problems that arise, but I would I would like to stop here because already it's not too much. And so this is the season finale and we will. Yeah, I can, I have time to take a few more questions. Thank you for your patience and patience.