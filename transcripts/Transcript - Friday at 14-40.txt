The same space. So you can figure out what's going on and what you are dealing with is not. So this is. I'll point out when. It comes in. Because I think it's a camera next point of view. And the way I was driving here, we're thinking maybe, you know, two years, it would be maybe a better way to. Produce Today we speak to. What you see in the morning. So I'm going to. Consider a function for X that depends on some set of parameters you check. With beta I use this notation. I decided things. The teacher was using a little bit different, so you should be realized what you see in the. Morning correct? I called the outer parameters B. Because I don't have too many W&W&W because you might get confused and we're not going to go D just stick to shadow. OK, I don't want to be fancy. If you see, there are a lot of different. Activation function stick to the rail. And actually when I. Use it OK. I'm going to call PITA the set of all possible parameter which are all these BS, all these BS and all these. WS just that because again, in this course we never put the vector over a vector but. These are numbers and these are vectors. OK, big. Theta is this weird space which is a concatenation of this. So it will be the product of RRD and Rd. OK. As you mentioned in this morning, we're going to consider here specifically a set of constraints which can be justified. And I think this morning on the proof. That maybe this is probably fixed. And the idea is that we're going to consider the following. Bounds OK, so. On West, I'm going to. Consider the one norm and assume that one norm is bounded by some parameter tika which becomes a parameter to be determined OK if we call it one. Over Lambda with this strange parameter of all matching and if you want to enforce here, I will take this. And the second thing, we're going. To do then instead, we're going to assume. Normalization. Here, as you say, this is the user setting X OK, OK. Right. Because we have to write a. Whole bunch of times what? Yes, it's that d ^2 over Kappa squared. Where does that E come? From all of them, they're all satisfied. You should have seen this morning that this is somewhat related to considering different set of different constraints, which is more of the form sum. Square division if you consider the second. Constraint, which is just the sum of the squared norm. All the coefficients. And the first constraint is more like fixing the scaling of these inner weights. Once and for all, and then just consider the. Discussed in the morning, I just. Mentioned this that you're going to stick to this kind of set. And we want to consider not a penalized problem like a constraint problem. So I want to introduce more efficient. OK, so I'm going to denote by D OK, which notice that I should indicate as B ETA and. For sure notation are denoted by D because I have to write it a whole bunch of times, but B is actually a set of vectors or a set. Of it depends on the radius I consider it and the number of neurons. OK, here here it is just the set of parameters that's this L1 norm. It is some to 1 constraint. OK, OK. I want to. Define solution. I'm going to take the minimum rule functions. Theta. Belongs to B OK? Is that OK? You can contrast this to a penalized version where I use this as a constraint so I sum the L1 norm. For example time indicator. Function of this constraint OK. There will be another option here. I don't do it. We. Get to both. First of all, it's a little bit different from the one you saw something. New in that also simplify. So use of notation, I use I guess notation, yes. OK, speaking, you notice that you know F hat B is actually F of ETA hat B, OK. So the solution of this problem is a set of coefficients. OK. Then I just plug it in the back, but I don't want to carry. Over all this stuff so I write it OK, so you just. But why it's indexed by J? So it's. Not. Raised to the M is the sequence J12M. So if you. Count. Them it is. If you want, you can also so you can view them as. M number, vector number or you can do it as one vector, 1 vector, one matrix. OK, so this beta is the. Collection of vectors, matrices, vectors. Thanks. OK, look good. All right, in the same spirit. What we did already? Last time. We going to introduce sometimes for the. Population version of the algorithm, which is the version. Of access with the data, but it utilizes this subset of possible choices of the parameters OK and I'm. Going to note. Corresponding solution by BI guess. OK, B you could replace it with ETA and the number of neurons you're using and the bound on the. Here there is the exact same abuse of notation here, meaning that I write FB. What that actually mean is that I have the Theta B. And I plug it into the OK, so when it's useful maybe I'll write it back. Right at this point, a whole bunch of times, the goal of today is to try. To figure out the behavior of the. Error that I made when I compare the test error, the gloss of. The empirical solution to the best of the best. OK. We'll begin our journey by introducing the middle. At this point you should look. This is not the first time you see stuff like this. We have an object. The difference between these two terms that don't depend on the data anyway. You all have access. They both have access. To Infinity data is restrain the solution. The search of a solution for both this one instead is just the exact same thing. On one hand this is restricted the data in the same board. The same board. That once it's finite and OK. So starting this will be basically every cup of what we did Tuesday. OK, want to introduce our the micro complexity, symmetrization and all that kind of stuff. In fact, you know one could have done more general theory than the useful. In the special cases. We went down from the merchant. What I'm going to do next? Familiar should. Remember we start to add the subtract stuff. So these guys here, these guys at the end, I added and. Subtracted the empirical risk of the empirical solution and the empirical risk of the population. OK, you can get rid of 1. Guy right away. Another one almost right away. So. 1. 2 3. All right, Remember we. Can get rid of this one right away. You get the middle one which is. The maximum value of. The middle one at most zero. Mostly zero, mostly zero. Why it's a little bit? Simpler to what we did last time, because in the last time we actually have the penalty, there's not even the penalty. So this is the minimum on its domain. And it's just. Something else? So. This number. Cannot be better than this. Even though this guy has seen all the data, not asking me to predict all the data but just to predict value training. Said in the view OK. I guess I were. Complicated our life a little bit on Tuesday because actually we can at least an expectation actually can be the third one. OK, so you do things in probability that I didn't think about, so let's write. Down. And say the expectation of the LFD. Minus. 3 is actually 0 OK. Let's check that it is. Complete through the expectation. Of one over NI from one to NL of Y. IF of XIDA minus. The expectation. Acts on what is over what. Expectation the only thing at random here. Is the data which is YIXI. This expectation you can think of as N distinct integrals. OK, whatever is that this random variable or identically independent and FB does not depend on the data. OK, so actually I can move in this expectation inside the sum. Because of linearity and what they. Obtain Is N time the same thing? And what is this thing I get inside? Is expectation of NYSBXI? What is that? So how much is this the expected risk about B and it's the same for all of them, OK. So this whole thing, OK, is N times the expected risk divided by N So this is just. OK. I mean that M is over all the data, right? That's the expected over all of our data. But there's also an M of the definition of the pole. So the expectation is over all the data. Y1X1Y. 2X2 and so on. FB or the marriage that you could remove it because it's a fixed. Function could be people for what is there. I take this expectation. I write this as the sum. I can move the expectation inside and take this term. These are all the same. All the expected risk summing times divided by N. OK. I already said this. OK, I didn't want to. Too much, you can kill it right the way. You can also. Treat it as just. Another guy that you know you can deal with in this case if we're. Doing things improbability and it's a little. They were just. We already discussed this, but is do you remember? OK, So what he's saying, I hope that somebody knows the answer here. Can we do this? Do whatever you want. Sure you can do this. Sure, you can put this inside. OK, Why not an? Expectation is thinner. You can put it inside and then they are not independent because either in the head. There is all the Y. And the OTX. So you actually do not know that this is anything. OK, so certainly is not just. So the key point is that this is not a fixed deterministic function. Is a random variable and it depends at once all the data. So this we call it. DI. Actually. C1 is not. Dependent on C2 because they both depend on So no you OK, so this is not a good idea. What can you do you? Can do something. Pretty brutal, but this one is less equal than 0. This actually is equal to an expectation. So we only have to focus on term 1. The middle guy doesn't matter. It's. Zero. Probably 1. You don't use the expectation the last guy. We use the expectation. So now we focus on this. We already know that for. Sure, this is smaller than what the? Expectation of. The soup F. Of ETA in the bowl of L. The order matters here because I only want to do 1. I don't want to. Put any absolute value now. And I'm going to do this OK on Tuesday. I did not make this observation. I So what I did. Is that I wanted to use this one with expected. Empirical, but also with the empirical expected, and so we put an absolute value. Do we make the lot a bit easier? And we don't have no. Absolute value. You see that at some point actually, OK. So notice that here. There is an order that I'm going to use. OK, how would we do? Turn the crank because we already learned how. To deal with this, OK, remember. The whole idea of symmetrization. What do we do? There's a whole series of tricks. Rather than by by 1 as a recap of what you did, but it's the. Same. OK, I mean no, no. So step one is the observation that you already. Made. That I can always write this as. OK, this is the same game within two time times already. We just use this. OK, now we'll do it backward, by the way, but you're pressing. The magic is happening here, right? Because. You see this is now deterministic function here. This is a random viable OK. This is now deterministic function, but I had Infinity magnitude, so instead of controlling. One local large number. Morally, I have to somewhat control all of them at once. But inside this is a fixed. Function. So now if I do look at these. I can use the same. Religion I just show you. Is actually the sum and times. Of. This and another thing here is that I invented a ghost training set that doesn't. Exist. It's a purely. Theoretical object. OK, so I say OK I can just write an expectation is the sum and times over sum iid. So the empirical risk is exactly this form or the sample that I call X&Y. OK, expected risk can be written in this way. Notice that these expectations would only skip on this. But this expectation is over what the prime stuff? OK, so I can also have it on this because. This just doesn't. This is a constant so I can pull. It out so that I can put everything under the same. Moreover, we saw, and we're not going to prove again that the soup of the expectation is actually less or equal than the expectation. Of the soup. OK, So what I'm going to do is. That I'm going to move this expectation outside and. With probabilities, we don't put 2 because we always just. Shove everything to the same OK, but you want to stop one second to realize that first of all this becomes. Lesser equal because the soup expectation is the soup and then. You can ask yourself is expectation even though. It just looks just. How many random? Variables and which ones? How many? 1-2 NY because they have. N here and N here and the guy XIY I-1 to 1 and XI prime Yi prime I from 1:00 to 1:00. This is 2 N integrals. OK. So do you remember what we did next? This is trick that could come to mind. The next one does not come to mind. This stuff is more like an expectation. It's more human. The next one. It's. Alien and it's the one that you say look there is symmetry in this stochastic process this guy OK, these are the same I can change them so if I. We're not going to do the whole. Thing again, we're just saying, oh, if I call this CIW, then if I take the expectation of CIW, again CIW is a function of random variable that depends on whoever random variable its expectation is 0. And if you do ZI or minus ZI, they have exactly the same law, OK, and in particular the same mean. This remains true if you sum them up, and this remains true if you take the suit OK. The trick here is that not only you want to use what I just said, but you can exploit another problem if Sigma IA random. Variable whose value is. Are just plus one or -1 and these values are taken to probability 1/2. Then the law of ziw this. Is West. Is the same as the law of Sigma. I ziw OK. If this makes sense and it takes me to check. If you have a random. Body which is symmetric and you randomly multiplied by plus one or -1. With the same probability, you know, you get the same stuff. You flip the sign you have the same way. OK, what? Yes, this is the. This is totally different. They said the same stuff. One as the eye, you have a whole. Bunch OK, so this is true. And you play the same game, say OK, this is true for CI, is true for the sum. It's true for the soup, OK. Which means that I can now. Write that this is less or equal than expectation over soup ETA in PD. One over North Sigma I. So actually I don't want to do this so. Cheat. Put the Sigma here. This is what we just saw. We can put Sigma I right there, OK, because they have the. Same expectation so that now I can just keep one step and take the triangular in a coordinate I have the same quantity. Plus. What I can write so thing is less or equal than two or this OK? So here I cannot the Sigma I because of the symmetry of the ZI Theta OK. And it's just the same. Then I have two terms, the triangular inequality. Because there's nothing. New. So. Far it's just a. Recap. OK, it's just a Tuesday. We have here W transpose X and now we have F ETA X. But we have not used the shape of these functions. At all. OK. All right. What did we do with the next step? We use the magic result, and it's for the contraction principle that says that when you have a composite class, OK, well, we gave a name to this thing. We said, oh, we call this the empirical. Micro. Complexity and this is the expected version. And the complexity? Of the class L composite delta and it turns. Out that you can relay the complexity of this class to adjust that ETA as long as that is. So what we do that this is actually less of them CL. OK, so I pull out the content so we're. Working under the same exact assumption of the other day SO. Let me also. This one. Away so. We are going to assume. That for all Y and prime R Y a -, n Y OK, so it was usually central. Is that what I use here? OK. Just to remind you that you're not sure that the loss. So now we want to write here the expression of this. OK. So we're going to focus and. I'm going to. Focus for a bit on this. Use this for a second sort of the right soup. OK, one second because I don't want to write soup beta. Over. BI want to write what B is, but they want to replace that beta by expression. So here instead of XI, let's try what F ETA is. What is F ETA sum J from 1M? DJ WJ transpose XI plus DJ correct? But to write that right to bound to do that bound. Which one? Take the last one. This one. And to take out this constant of that, we also need that zero when a 0. No. The loss. Why is this true? Yeah. It's because of that, because of that. Because of the Lipschitz OK. But the loss is 0 when they. No. No, it was. We just checked the. Because it turned out no in fact. This is true. On the top the reason why it's. True that he's speaking because if you look at. It So what he's saying that sometimes I look at the construction. Piece pull out of the book. It says that this L. Has to be 0 and 0, but here what you can do. That our answer is. Not zero and zero, OK, but you can, you know, you can talk about this value, OK. What you could do here that you could add it and subtract it for everybody, OK, then you have that the center. Version is in the the one from. The book, but you have you also have this extra term. Which is of this form. OK, but this term doesn't depend on it anymore. The soup doesn't exist. The expectation enters C Sigma I. And so this is the one. Place where the lack of the. Presence of absolute value is exploited you. Just. Apply the construction basically then though this is not zero and zero. OK, so if you know what I'm talking about, the same question, this is the answer. If you don't know what I'm talking about, you can ignore. Make sense? That should be one line. Now we're just going. To start from. Here that's why I was keeping a second so done. OK, check now let's use it. What we do from? Here to here for now is just replace it F ETA and ETA and B with what the hell is F ETA and what the hell is B? So F ETA is this stuff, not that I have two summation 1 is over the data point, that's over I-1 is over the parameter and that's over J. OK. Then I have. 2 sups. Right, I have one Sup. Is the vector B as norm one more than ETA? OK and then I have another soup too and I write the condition here. And what is the other soup? Well, all the WJ and BJ satisfy this. OK, all of them. Satisfy this by definition of what? B is OK. I did not OK. Now what we want to do is that we want to look at the whole thing instead of the parenthesis, instead of the expectation, and view it as an inner product of something and something else, and then use Cauchy spots, or rather OK. In particular, what I want to do is I want to write this whole thing as. V transpose something. What is that something so I'm. Going. To do some black magic check because it's really simple. You see that I have two summation. I can swap that, correct? So I'll do it ready because everybody. Takes up an hour to. Just swap sums OK? So this one become J from one to north, this one become I from 1 to north. And let me put this so north, OK. Swap them, then I have the Sigma I over North OK, where do I want them? This here and I want the Sigma I. Here, because that's where the sum is. BJ, it's clear I can actually put it outside, correct? OK, now this is the sum of this number times this number and times. OK, so this is a vector. This. So this is V transpose U and what is a component of U is UJ J is equal to. One over North sum I and 1M OK Sigma iwj transpose XI plus BJ OK so the vector. U has these components you can just check OK, it's better for me than for you OK. OK, but if I have something like this, I have that the product is really smaller than the absolute value of the product and the absolute value of the product. But your is always smaller than both. Instead of the product of the two norm and one norm times the an Infinity norm. OK OK but U has a definition V instead of. Releasing this value right so this. Product and now put this. OK, because it's so. Smaller and this is more the L1 norm of this times that stuff in norm Infinity. But what is the maximum value of the L1 norm of this? ETA. OK, because I'm working on that which means this whole. Thing. Is going to be expectation. Of I don't have the Super. Vert. ETA I use it so I use this. So let me use everything. I want to use. So. This is the first thing I use. OK, I'm going to use in a second that diesel is more than ETA. Before that, let me also notice that U Infinity is Max UJJ from one to M by definition. OK, it's just the soup of the entries. So at this point I can write. OK, so let's see if I wrote it correctly. Here I use this soup. OK, actually I swap the order of the soup. Use the soup on A1. So this product, OK, this is always smaller than this and this stuff I can only drive there, OK. So I take this stuff, I view it as an inner product on this on V and the vector defined component twice like this. Use further. I replace B by its maximum value. Sorry, your whole bunch of constant that comes up, that one norm and I'm left inside with definition of the Infinity norm of this, which is Max over components. OK. And then this guy is on this? Constraint. OK, so I shape these super D10D using order inequality. I shaped out the part that pertains V, that one norm. OK. Anyway, this is OK, It's mostly notation. It's an inner product of 2 vectors. One of them is that one normal and then the other one. I look at it and I arrive the soup normal. As a Max express along the way somewhere. Is that when you have two? Soups you can change. And we're almost done because from here we're. Almost at linear models. Almost. OK. The first observation is this. Max does not because this is M times the same problem. OK, so literally I can remove this and consider just one fixed vector of that. Is just, you know, I. Have to take the maximum and times of the exact same problem. So just and. Then I'm left with this. OK, if you now look at this, we already seen it. OK, it's very similar to something we've seen because this is that the micro complexity of a linear class or if you want an affine class, that composite and other function turns out that this function is lip sheets. But this lip sheets like the absolute value lip sheets, which is the case of what you have in an expansive. OK, so the Lipschitz constant is exactly. .1 and now the good. News is that you still have the contraction principle, but in the specific case of the absolute value, you get an extra 2. OK, Long story short, you can get rid of this plus and replace with parentheses, but you're going to get the 2. OK, so if you check up to my typos. It should. Be the same the difference that the rail or if you is replaced by parenthesis, but you have to. So using a contraction they will give you a tool OK if I. Remember what to do next I want. To lower the six steps right if you go next rate you want to use it twice so. I want to look at this and recognize it in their product, because if I can provide it in their product, I can then write an absolute value and then I can use. In this case not further, but Cauchy Schwartz inequality that 2 version. Because that is constraint. OK, so I want to view this. As an inner product, the claim is that the inner. Product here is between two vectors. And I'm going to write. Here for a minute here. West bar is simply W B and I forgot that. No I did not forget. I want to add Kappa divided by Kappa. OK. Because I need to use this. Constraint. So I better have a Kappa there, otherwise it. Doesn't work. But I added and divided. So 1 so. W bar is going to be W. Like that guy, right So. B over X bar OK X bar is a bit more I'm. Going to write X bar is. One over north I and 1 N Sigma I XI. So you see basically. What I'm doing is this for a minute. I take this West and I pull it out of the parentheses. OK, and out of the sun. So I have this then similarly, I now have this and I put it in front of this stuff. OK, pulling out The Cave. So I'm left with the one over north some I from 1 NI Kappa. OK. Seems correct. So the. 1st 1X bar is made of two parts. The first one is a sum of vector that looks like XI, so it does the same size of XI. The second one is just the sum of numbers, so it's just a number. OK, all right. But now again you see what I want to do next. I want to say OK now. You have to do. That U bar transpose X bar value smaller or equal than norm of West bar X bar OK, but the norm W bar is made of these two components. It's squared norm. OK. Is norm. Is the square root of the square norm. But the square? Root the square. Norm is equal to 1, so this root is also equal to 1. So this whole thing is just smaller than XR. So he usually add tonight write down what the hell is that? And then we're going. To see enough time. I'm almost done. All right, so. Your thing is lesser equal than. 2. ETA expectation again, I'll do the same. Right, let me write this as the square root of the square. So I have expectation of square root of. Norm I have to do the norm of this plus the norm of this square. This is just enough. So I get. One over NI from one to North Sigma Ixi. Squared. Plus suite value one over N1 N, Sigma I and Kappa right hand side. So before this is really the end of it. OK, before I do it, let me remind you. So let's take this is. Really the end of it. Before I kill everything, let's look at the whole proof up to this point, because really, it's this diamond. It's basically the same as the dinner case. So let's start from the right place. You have to control this term. This is what the transition got OK, expected in Kill it to the soup. It's just one sided, so no absolute value. You use the ghost sample trick and there are the micro complexity. This is called symmetrization. OK. Or actually. Once you do the triangle inequality, that's what is called symmetrization. That leads you to this. You get the two because of the triangle. Inequality, then you. Use contraction. Principle. So. Symmetrization we did by hand, and you only relied on symmetry and linearity of expectation. This. Is a result that requires an advanced result, which is. The construction principle. We use it and you get this equation. And you see that the loss function and the YS by the way. These are OK. If there was noise in the Yi, don't know. Go on, YS are gone. OK, you might lose something there perhaps. And then what we do, we just have to massage this into the specific state of F Again, we're not doing something in general, but what's on this left board holds for anything. OK, there's nothing here Tuesday you look like it depended on neuron on leader models Today you might were distracted by marriage and thinking that depend on neural network on this. Board. Anything on a set? You would call this. Pietro OK, And then this is the Super all the Pietro's. There we have the Super version. Now this. There is not a single line which is general. Everything is specific. Replace the. Super what it use the Super? What it is we write down the the neural nectar and then we use the fact that we have an L1 constraints and an L2 constraints. So first. We sweat the beat to write. These as an inner product. Then we use elder inequality and use the one constraint. Then we're left with this. We have to use again contraction. It's time for absolute value which vies at the next OK, then we get one thing which is a sum. Of products. We view it again as an inner product. Same trick as before, but right now I have an L2 constraint. So I don't use earlier, I use earlier. But in this case we go support Cauchy points I get. Read of that part and I would like to reduce the terms OK. That's the guy. One thing that perhaps you should have. This story is is the festival of inequalities. But in fact, we keep on using OK, and here we might want to use one of the two, but we've been using. Further. Up to now, so maybe we should get OK and look we have the expectation. Of the square root. And it happens that the screw is concave. And if you withdrawing and remember then you can see that you can pull it down. So actually we can do this. And why do we love Jansen? Because whenever you have an expectation trapped in something, OK, if you can use Jansen and make it see a square, then magic happens, OK. And so here is expectation sees this square and it sees this square. OK. Let's look at this. This is really stupid, OK? This is just the square, the absolute values there just to confuse you, OK? You just take the product, but it is the product of variable that are independent, OK? They're added value 1 or -1 and their mean is zero, OK? So whenever there is their Sigma I, Sigma I is 0, whenever there is Sigma I Sigma J, they're independent. So it's zero. OK, so this term. Sum IJ Sigma I, Sigma J, Kappa square and square whenever Sigma I differ from Sigma J. The. Expectation enters and that's zero. Whenever you have Sigma I, I equal to J, the expectation enters, you get the square OK that becomes 1. So it means that you sum this stuff N times the. Expectation of this is equal to North. So this whole thing. Contributes. Kappa divided by naughty N square. So. This contribution is Kappa square divided by north. OK, let's look at this guy again. I'm going a little. Bit fast at this point. We already did it Tuesday. We did the exact same thing here. What you want to do? Is that perhaps one way to see that this is actually XX transpose divided by north, the vector of Sigma? OK, you stop all the sigmas into big vector. You can check that they're just a linear map X applied to it. So when you do. The product you can rewrite this as again N squared. So this term is going to be the trace of XX transpose divided by north squared that 10. But everything is linear. The expectation is going to go and see there are the micro random variables and their covariance is data. So they also go away OK. This is an north by north matrix, which is the matrix of inner products on the diagonal. It has just X one times X1 X one times XN, but they're all smaller than Kappa. When you do the trace your sum in Kappa and times, but you're lucky because there is an N square. Here so this whole. Thing is again going to be Kappa square divided by north. So these all crap. Under the parenthesis it's going to be twice Kappa square divided by N which in the end of the day. Gives us This inequality is 2 sqrt 2 ETA Kappa divided square root. OK, so at this point there is a mistake. It's extremely, but I don't see it proceed under the delusion. Let's keep the score because I think I have two here. And four. Sqrt 2. CL. ETA Kappa. You know it's glory. OK. Which is the glorified version, the pumped up version of what we had for linear models. OK, you have to sweat a bit more because once you hit the loss function, we were done. Here we have the parameter V, We have to kill the loss function. Then we have the parameter V We have to kill V then we are left with the no linearity. Then we have to kill the no linearity. Then we're left with the parameter. WB and we pick those. So we have two rounds OK of the kind of stuff that. OK, brutal. But we can take some look at something. How does this depend on the size of the net? If you have an extremely over parametric model, what happens? How does this affect? So that thing there depends on the numerical constant. It depends on the Lipschitz property, OK, it depends on the bound of the inputs. It depends on the L1 constraint on the outer coefficients of the neural network. Depends on the data, the size of the network. Is nowhere to be found. OK. So this result says that if you look at the generalization gap, OK, if some of you have seen the literature of statistical in the last 5-10 years, you've seen there is a paper that said that. We all have to throw away everything we've done before. Because there are a lot of mysteries in there. The number of parameters appear here if you do another proof or another model. If you don't put that one. Constraints and you do parameter counting, which is known since whatever extremely your model 2 are not well described that the number of parameters you do, they were described by whatever is the complexity measure that you use. To control OK what's going on and usually parameters. Something is not. It's a very rough OK also. Here we can make the parameter counting up here just count OK. So this bound the size of the network is nowhere. To be found. And so this. Network was M&B. Of course you could say, well it is going to be different and whatever. Sure, OK, usually the. Size of the network is actual number does not right? But this was just the story, right? That perhaps is going to be short. Because somewhat the only we're not going to do really computation because they're a little hard. OK, so this guy is more or equal than this. That is more equal than that then. Put the other. Guy the other guy is LOFD minus info L over everything. Over. Now I feel like maybe we should think a second about the fact that this B OK depend on M and ETA. So I. Still the size of the network and this is still a parametric model, finitely many parameters. So if you ask me, OK, you know, how does it compare to the best of the best? Well, it sucks. How does it compare? It's like you have only M neurons and you like to approximate. Everything OK, Very good, OK. You could let ETA go to Infinity, you say? I don't put any constraints on my network. It's still, you know, size M. OK. So it's still a parametric. One idea which is very natural is actually 2 left the number of. Neurons will be OK. And so the hope is to produce an object where when you get the number of neurons going to infinite, you have some other well behaved function space. Then you can try to figure out how far you are OK if you remember in the case of for example integration or in the case. Of kernels and so on. It is another little guy which was the best possible solution you can get with your algorithm. For example, in RHS was the RHS OK? If you regularize instead of an RHS, the best you can do is the RHS. The RHS might or not be large enough to invade and recover disease. That's. What people do. Here we are missing these largest set you will explore with older fancy OK you want to. Figure out what it is. What it is turns out to be actually an interesting one. Yeah, it's been around for a while, but perhaps it's been a bit. It is the following. I'll show you how the function look like. I'll define a norm and then give a name to the space. OK, maybe I should run? Somewhere. X sum J. Now the special what we consider is called F MU X and it's going to be. Integral of. West. Transpose X + B + B MU. OK, so. Remember that here I was happy constraining these two. Parameters to be a ball I want to somewhat interview I got to. Really I want to somewhat rescale this so that I have to. Worry. OK, so. Instead of AB, I always consider B. Over K the parameters. This means that these vectors. Always satisfy my constraint OK. So that's the choice I made from the beginning. OK, but I want to reabsorb this in the notation, so I call this B. It might be. Now the next 10 minutes you've got to be renormalized by constant Kappa from the. I could have done the final bound. OK, So what is this? Well, it's an integral of the activation function over some measure. OK. I change this measure. I change the function. OK, what is this measure? It's a super friendly concept. It's called a random measure. So if this was a probability distribution, this would be a measure that is always positive and it sums up to one another. Measure is not always positive and it doesn't sum up to 1 the difference of two measures that they are both positive. So she's not positive, but she's the difference of 2 positive stuff and both of them. And also the total measure is finite, which means that it doesn't give you 1, but it gives you 53 and 44. But you can normalize them. OK? OK, It's not a nice one, but it's not too bad. OK, And why do we like it? It's very gentle. OK. If you put their probability measure, you can explain so much. If you put their project measure, then you can describe much. In particular you can choose as MU. If I give you a finite set WJBJ, you can put, you know, the counting measure on them. That's a special case of randomness. OK, so actually this can be seen as a particular choice of. So F ETA is F of MU M with that MU. OK. So this says that all the network that we use are special case of this. But the intuition is that when you let them go to Infinity, you get much more stuff. And the hope is that you actually get what you get when you let them go to OK. Turns out this space is J norms and you can endow this space with a norm. OK. A classical way to put the norm over measure is to take integrals of the measure over some test function and then restrain the test function some way. So if you actually take the test function to be bounded and measurable, that suit defines a norm. Given a measure, there might be multiple ways. So given a function dividing multiple measure, it gives you the same. Function so it's not one to one OK, but you can. Define the norm. So you're going to take V for variation norm to be the inference. Over MU in the radon measures such all what I have got to tell you this. Such that. It's a bit short here, but basically. I'm saying if you give me an S that give me the same S, compute their norm and among all of them pick the one with the smallest norm. OK, turns out that I take any this and this is called the variation. This is a standard object that arrives for 10s of years to describe what happens when you let the number of neurons with. Infinity. OK now what you should appreciate is now this defines the space, right? So you now. Pay base of functions define by other measures with this norm. OK, is actually functional space OK, which is. So this norm so. If you take function of this form with this norm, it turns out you can prove that it's actually a complete binary space OK, and actually have some form of. You have no time to do this at all. So this would define, you know, in the future. Version This will have a whole hour for itself because it's a very interesting. Object OK and actually the theory of space goes out of the door. But the point is that it turns out they want to make full observation. First of all, we have not time to prove it. But imagine you can. I think you can imagine that there might be hope to show that this space is large and maybe is large enough that we can recover. This in this is the. Space that you obtain replacing you know finite neurons with measures can be shown to be dense. If this of continuous function we are at the time to. Do that. And in some sense, what I'm going to let's let's give a name to this guy, let's call it H because the full fledge hypothesis base that I use when I consider Infinity neurons with no constraints whatsoever, OK, I just let it go and I'm saying that. Here I do the game within before. I replace this with the info over H OK, If the info over H is equal to this I'm universal and otherwise I have some OK. And we want to worry about this guy state. Theorem which? Is essentially due. To result by Mori. That goes by. Morris lemma, and this is not Morris lemma, but it's a corollary Morris. Lemma instead of following the element of SB minus. OK, so we assume for now. As we did in the past that the input. H you know is achieved by the functioning side. This may not be true, but. You make this assumption procedure. Then what you can show. Is that this is less than or equal then OK the one I show you. The. One that the so it turns out. That when you take the difference between. Two integrals. Because of the heap sheets property you close with some. L1 norm and control some F2 norm. OK. And so if you actually look at the classical form of results here, you have the norm. But you can also replace it with it. So this is not the important important part here, is that you say? If it's true. That the minimum over this ginormous space is achieved OK and I call this function F star. Then there exists. Remember that this is an F. OK, which has. So assume that the meaning was achieved, call the normal star of the guy you know that achieved the norm, and pick ETA in this space to be exactly that norm. OK, then you can prove that the approximation error that you're making this is comparing neural net with finitely many neurons, and if you want the best neural network unit, many neurons decreases with the square root of the number of neurons and it depends on the norm. Of the function that you have to approximate. The function is very. Complicated. OK. These results can be put together with that one and you see that. Here ETA start. I need to put this equal to a start. And then if you do it, you get the final bound. Which is up to constant and start 1 / sqrt n. Plus. 1 / sqrt n. OK. So basically shows you that under this regime. OK, we could. Make the network arbitrary. Large and we'd only gain. But the smallest network. We can do to actually get a decent. Result is to be roughly the size of. The data. This does not show you any problem making the network. Large. I want to show you that you can actually make it smaller. All right. All right. I think that's it. OK, so. I spend a bit more time on the statistical part because again, it's a, it's a as you see, and it contains a couple of lemmas, the contraction principle. The rest is just do that, you know, and you can really break it apart and figure it out. OK, this part is a bit. More complicated both of because of the definition of this space with this guy so. That's it. OK. So I'll take all the questions of fine. If you. Hold the question. OK, fine. Again, if you have any feedback here, don't ask her for laughs. This was not the course about. Laughs. This was about from the beginning. Pencil and papers and then and then. So.