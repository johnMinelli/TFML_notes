Really that you'd like to understand basic principles of intelligence, if they exist, and I think we have to bet that they exist. Principles like conservation laws in physics, I don't expect will be a very complete theory of intelligence in the sense of the theory of relative magnetic, But there may be some important principle and it's becoming urgent for us to understand it because it may take a couple of decades. So maybe less, maybe a bit more for successor of language models to replace other additions first and then other scientists. And when this happens, it's not clear what they will be able to understand, what the machines will understand. So that's an urgent need for other reasons. So of course, I'm going to speak about principles in deep learning architectures today. So things like multilayer perceptions, which are being successful by themselves, very successful for image analysis and recognition as convolutional networks, and they're also the key and part of today's Transformers. Multilayer have an input, you have an output. You can look at the sequence of what happens in this multiple layers as the composition of the functions that each layer computes and each layer does something very simple. Neuron in each layer is examinating incoming from the neurons below. Those are weighted by the weight of matrix in in in the layer. And then the weighted sun is just passed through a nonlinearity that can be almost anything, but most of the time has been in RLU. So the rectifier and some smooth variations of it like OK. And of course the key is how to set the weights. And there are in the transformer there are many billions of weights. Weight you do it is using. The simplest way you do it is using the training set. So a set of inputs and outputs, for instance images and say they are labeled and you try to minimize a loss function like a classification error square loss on the training side by changing the weights appropriate. And the standard way to do this is using to use some version of gradient descent. And one of the implementations of gradient descent is that propagation one of the entries over the last 40 years, it's basically gradient descent and what is many user action stochastic gradient descent. So you're not using all set to compute any set random subsets of a batch. OK, that's just to give you the picture. And now in the Transformers the the architecture includes some additional parts. The most interesting one is something called attention or self attention. But there is a part which is a two layer multilayer perceptor and since this is just one block of the transformer, this many times 20 or more blocks. OK, So what I want to think about is first of all how to deal with theory point of view with this kind of. Then introduce composite compositionality. Compositionality, simple components tell you quite a bit about boolean functions. Motivation being that for function that can be computed, simulated by the computer, and yet in the computer the function is represented approximated by. So all right, so let's start on this. So you probably heard of already, but this is the main conceptual framework of machine learning these days. It's not the only possible one. 99% of what people are doing and and the conceptual framework is that you you want to represent an unknown function that describes the relation between your input and problems speaking. Now say every mind the convolutional network images, classified images, set of many examples of images and the labels of the image. So you want to have. There is a function, maybe probabilistic mapping between the output. But for now imagine it's a deterministic. So you have a mapping between the output. You don't know this function, you want to be able to approximate it from the input output. So you have to find to choose a space of parametric functions which now could be kernel machines. Linear networks could be deep nonlinear networks. The parameters for instance in the deep nonlinear networks would be the weights for each layers. You have to choose the space of function with which you are trying to approximate their known function and and you have two requirements on this space of function, like to be or put enough to approximate most nonlinear mapping. For instance, linear metals out because they can only approximate linear ones and they like to be able to do this approximation with a number of parameters that is not large. Not too large will mean does not grow exponentially with the dimension of the. All right, if you find a space of function like this, then you can minimize the loss of the training set, find the best parameters and then use this to approximate this mapping and do other things you want. So this would be like approximation and the optimization, and then you could study how well the function you have found predicts new data Generalization. Now turns out that good way to approach this problem is to find a space of parametric function that is composition. So what functions? There are functional functions and functions. Each function is simple, depends on a small number of parameters and or call those constituent functions. I want to give you a brief review about the idea of compositionalities old. Not only mathematics, but it's in the study of recursive functions that are very based on future science. Compositionality is one of the key way we should generate new function, new operation. But it's also important in language. And people have been using. Look at the literature. You see that they use this term compositionality in somewhat not always consistent ways, but training in general means you're both building complex systems out of simpler paths and and so for instance, in semantics you find that area of compositionality saying that the meaning of a sentence is determined by its parts and the relation between them, how they are combined. And compositionality is what allows to create an infinite number of sentences from a finite number of meetings. So this is just your similar and yeah, syntax and discuss later that Transformers exploit compositionality and and if you study how children require language, you require the difficult first what I said because people function simple parts and then later they will be able to learn how to combine the complex and of course in computer size of the engineering, you know, programs are being out of subtleties. So we like the function and components of it can always write the complex program in terms of sub routines and sub programs. And so functional programming and also related related tools exploit this. Modularized and composed, simplified. Now there is much that's going on, yeah. But I wanted just to give your feeling that competition has appeared in language, computer science, in other areas of science that's appeared in machine learning. Learning is part of what happens in this idea that you can reuse modules that you're learning for different counts and click on. Learning is learning parts of the fact you may want to use. So OK so I use the term sparse compositionality because if I just say compositionality it's does not mean very much because if going back to function, every function is compositional in the sense that they can compose it with the identity, nothing changes. So composition by itself does not say very much. Sparse compositionality says it's composition of singular parts or not. When I say compositionality should be all right. Now, if you think of the function as being compositional, you can always think of the directed. A secret graph can always represent the function, and in fact a program with a couple of conventions that is a graph in which edges represent inputs, The dots represent functions. So in terms of describing for all these red dots are constituent functions and the dot here at the end is the full function. And as you can see, the full function has in this case 9 inputs, but none of the constituent function has more than 4 inputs. If you want, you can already represent the director, the single graph as In terms of layers, you may have to need a that is just identity, you know, just. OK, now I want to speak about as I said, for the reason is that at the end we are when we compute, simulate, analyze, use a function in the computer, we are always at the end running boolean approximation. You know, you can imagine even multilayer network are in use, it's really going to happen is the discretization of the function assuming that in as inputs numbers that are, you know, so-called real real numbers, real numbers finite. Actually exist maybe that mathematics based on your number mathematics which really is useful for like rational in fact. Anyway, that's. Has to do with the question of whether we can or cannot. OK, so the Boolean class P, like the way you have certainly heard about the P equal of NP polynomial versus polynomial. That's one of the greatest of problems with the science. P is the class of cooling faction that can be computed by the Turing machine in a time which is reasonable. Reasonable means polynomial in the at most. In practice, this means any computation that can be done in a time that is shorter than the life of and instead of Turing machine, you can say any computer. So in corresponds to a circuit which is a circuit where there are and or not. And that there are a lot of results about the structure of this circuited of, but we don't need to get too much in detail. The key part is that the class of bullion function that are computable, efficient, close and operation like composition, variable reading application. So it's an interesting class that most important problem for us is the composition of. So the other part is that essentially the functions in P are boolean functions that depends on a relatively small number of variables. Boolean variables functions in P. And like X1X2 has value for X1 for the variable -1 + 1. It's like polynomial transfer mixed without. You don't have that like X1 square because X1 square will be one no matter what, so you don't have those. And you have typically adult OK. So you can see that in general, the fuller function of this type will will have a very large number of polynomial 25 or 30Â°. So, so that's that's why you may have a lot of but in terms of either of the some function that depends on small number of small, correct? And then you have the rest. The rest is not sparse, but it turns out that these composition, the composition tell you a bit more about about this. So this is just a repeating. Number of variables, so each one can be computed in a reasonable time. And then you have the composition of those fractions of fractions like like in this, this thing here. Maybe I have another example. Let's see. Yeah, this is an example that I like. Is a function like this a very a very node, a very G is a function of just two variables. It's a binary tree. If you make it deep enough, you can make as many inputs as you want. So now it's 8. Here this example that is going to be 1,000,000. I can still do this with a functional tool, so let me go back so. So the first step result is that as I mentioned that you can represent a very function as the composition of sparse function computed efficiently. So as I said, P consists of of sparse function and composition and sparse. OK, now what does this imply? So this is computability, nothing to do with learning, but. But it turns out that suppose I have a function that is during computer, then this is. This one says that if I have a function that is efficiently that is computable for a number of time, then there exists A neural network which set the restriction on that that will approximate dysfunction uniformly. And so the idea basically is that you can think that if each as if each layer in a neural network, which if you use instead of, they are a huge. Suppose you use a threshold function instead of a rectifier of just and there should be the threshold of a legal combination to the data that one can approximate this pass. So now you have the layer approximate one of the spots constituent function of the overall function and having the sequence of the composing which will give you the overall function. So we should tell you that your neighbor can approximate what to say and and this also says the part of the story. You have a factor that is computable at the most polynomial time. Then the function layers on the limit overall gives you this approximation. So this tells you that efficient computability for this property implies compositionality. Compositionality and that implies good approximation by the going back to the initial slide. The good class functions that this property being powerful and having a number of articles that not explore is the class. What I said now it's just that, you know, a kind of existence. I did not tell you how to find, for instance, the directed the secret. So the statement of this results that something like that exist. But in order to do that, you need to know how many layers, how many units per layer. This result don't tell you. Furthermore, they don't tell you. You have to know the weights. They don't tell you whether that's possible or not. These results are for approximation. They are not in somewhat indirectly also for generalization. The approximation result that you can get good approximation using a number of parameters. For instance, with kernel machines the number of parameters. And so this means that you can keep the number of parameters pretty low, which of course is good for generalization because then you would need a fewer examples to do well. In a sense, you're good for generalization because you're having your representation which is compact. It's small in terms of. Approximating well the data like the training set but having small number of parameters so. So it turns out in fact that if you do some calculations using I think you some torturing by the way, so so just support to the. It turns out that if you compute the automatic complexity of a network in which that is composition is sparse in the sense that the nodes in the network in each layer receive only small number. I think what that means means that in the weight matrix each row will have a lot of this example. And so in those those that case see that the standard calculation that have been done for the last 20 years macro complexity of different networks. If you assume this sparsity of the weight matrix gives you a much lighter power, much means like 1010 thousand times smaller. The reason is simply that this in the case of nonlinear multilayer, your second terms for each layer that contains something like this. And the standard way to bound this is to say, OK, the norm of this is less than the product, which is true very often. And if you take into account this sparsity problem, instead of having this bound, we'll have this one for that part of the calculation where small W is not a full matrix but just one row, it will be usually much smaller. You can look at the details. The point is now more exactly. You know this. The norm of gives you a much. Typically from you can get a bound on the expected. So so and you expect something like number between zero and one. Now if you use the standard bounds in a different network, you obtain something that is closed PC bound. Basically number of. In the case networks you get numbers that are like a medium 100,000 instead of 0. It's a very, very good so much that many people have said, but if you take into account this, but this bound, have you got the bound that is not this is the correct number. It comes up in the experiment. We get something that is the right one. It's loose, but it's less than one. So the theory, yeah, the bound should be could be, but you know, that's OK. No, OK. This is just to make the point that is important. So what about optimization? And. Yeah. The general argument is that you cannot guarantee to be able to learn from data in the sense of optimizing the in general. And the reason is that if you could do this, you could break encryption. Many examples of message that is encrypted version like say the technique and you know if you guarantee learning for normal time of of this, you'd say it would break encryption and breaking encryption means P whatever which is very OK. So in general you cannot guarantee in the sense of learning from data, optimizing the parameters. But instead of learning the old, you have a multilayer network, you have input output. Now this works a lot of time. As I said, you're not guaranteed that this will always work. One thing that is guaranteed orders work is if you have not just input output examples, but input output for each layer. OK and you could learn each layer separately. Of course usually you don't have both data, but if you have those data then you could any function in the constituent. The constituent function has pass function can be. This is just repeating this this kind of two approaches to learning a composition of 1 is the union one and one XI. FYI, the other one is training set for each of the. This is guaranteed to be. This is in fact. There is this theorem that says that if you have available, suppose the only Internet has been written by two image and suppose that you would have each input and output transition in the two. It should be like having layer in a multilayer. Instead of having this Turing machine writing works on the Internet, you have Turing machine writing on the input and the output of each step in the Turing machine. Then on this data set, you could learn, and repeating what that said, you'd be learning all of the constituents. And it's interesting. This is how to regress. You have an input and then you have an output and then you have the next input. And it's like, it's like what Transformers are doing, learning the next. In the examples for the sequences, instead of word of the state of a tutoring machine, then when you you did that, you'd be able to learn words are not the state in the tutoring machine because then from a few it's like you don't have the output of which network but you have a few. OK, so these are I think 2 of things that make the magic the transformer which is 1 is ready this out of a classic framework. And the other one is you know these two aspects are closely related to each other because because compositional sparsity means that you where this structure of the function function composition of simple steps. We can exploit this now to regressive which you have sequence of words. So this you know, it's just some insights what is going on this open question in activation. So that there are a lot of questions. I'm interested in working on theory about how transformer work and how constituent functions are learned during pre training and the training predict the next word. What happens in post training? You're using enforcement learning and other approaches to make the transformer becoming for instance chatbot and with some preliminary experiments on on simple Transformers suggest that the reinforcement learning is just making stronger some of what is learned in free training, but not not generating new capabilities. OK. So just to mention that one. First of all, you know this idea of the Transformers is learning compositional function and it's exploiting this consistently. What has been happening in the last chain of force, It's really saying, you know, do this and then do this. It's like saying computer one and then computer 2. So it's really exploiting the other example. Trying to do is learn and combine smaller and smaller. Yeah, that is interesting. Is curriculum learning how you train your machine? Yeah. You know, similar to children, easier to teach language by teaching for the simple parts of a sentence, complex ones together. And so, so this is quite active, very out of investigation language. It's all part of the post training. And it's kind of interesting, exciting that it's similar to how it should teach. The idea that I just mentioned seems to be. It seems to be because a lot of this we don't know. Most of them the companies are not telling us exactly. There are various insights trying to make a transformer break the problems in simple simple steps. What the focus would represent would be equivalent to the how exactly this is happening. But you know, in summary, a lot of things that compositionality property that as I said, surprising magic thing is that compositionality. How much time do we have? 2 minutes less 2 minutes, No, yeah, it was at 12:30, but we started with today, so supposed to finish at 12:30 when we started with today 1235. OK. I want to mention for instance the original reason why I got puzzled by questions question like this. There is the cursive dimension, whatever that is that you want to approximate or optimize D variables, complexity difficulty exponentially. You know this is equivalent to the fact that you increase the dimensionality of polynomial, You know the number of the fact that you have dimension and you have say dimension 10, dimension 10/10/10 to the third. So every way. You know, today you know and why this was kind of questions and turns out the time 25 years ago machines seems to be look like shadow network. The brake seems to have the visual pathway, several visual areas. So as I said, epsilon then the explanation D approximate this F the curse of less smoothness. Not sure how much but the kind of problems. By the way, this exponential is really bad. You you say I accept 10% general, which is small 30 by 30. This this number 10 as much as 1/3. Then it's in the order of 10 to the thousand. The number of protons in the universe is only 10 to the 18. This this personal dimensionality is exponential. It's really bad you you can avoid this completely if you now realize the fraction of competition. For instance, in the example of the binary tree, the dimension that counts is not the dimension of the overall, but it's the dimension of the largest. The worst constituent part, the binary tree is 2. That's OK. That's why the curse of because the exploit composition. So anyway, I think this idea of compositionality is interesting. There is a lot more to understand about this. Yeah, I hope give you some some ideas. Thank you. Yeah. So how can we connect like the stereo case function to start this conversation? Because like the stereo case function itself is not So I, I, I think like how can we connect like the staircase function to the sparse condition? Which function? Staircase. Staircase. Yeah, staircase is a staircase function of good end functions like X1 plus X2 plus say X1 time X3. Each term I agree has made of existing variables plus at most one new one staircase. In that sense you add a small number of new variables to work each. It's because if you have a finite staircase function, the number of variables, so there they are considered sparse function. They are what's equivalent to decision trees. These are the result of that, which are again sparse to their functions. Yeah, yeah. So you don't. You have an infinite staircase. Yeah, for example the transformer based network for example models have not been able to generate. But this is just very recently worked by group that I can give you the but the test covered this was done with relatively small test. So it may not be. Yeah, the results seem to be the enforcement in different ways the ones can you speak up, we can't hear you. OK. I like I I have a question regarding the most of the modern different architecture regarding that we have compositionality, but we also have. So I was wondering how, because it's true that for example was mentioned to have like essentially learn out of there, we need a number. And I was wondering if getting this residual structure is like any higher residual structure. You mean skip connection? Yes. So I think skip connection, I think what you said is probably. In a sense they impose a bias of simplicity, lower degree on the network and I don't think that that's been explored as far as I think you know, suppose supposedly that you are replacing your. By univariate, it turns out that and we have run experiments by the way you do that in a network to replace each other, you sort of see further basically towards the same way. OK, so in that sense after the first layer and the polynomial of the gradient 4, the second layer 16. So if you have now skip connection with, add similar polynomials later later kind of producing A bias towards simplicity in this case meaning lower degree. Last question. I have a question. Like this so. Now that performance, unlike recurrent neural networks, are not during complete in absence of external memory. Of some kind. So if unlike like write some information to the extreme they have inside of some memory that has in itself, I like it performs in general general architecture don't have such in your framework. When we are talking about this progressive generation, do we imply that the number of operations and like the number of tokens that must be generated may be like kind of infinite or require the same number of steps that the tuning machine might might be necessary to write? Question the wrong question lost. I wouldn't be able to I guess if your point is that the recurring your metric with a specific memory, yeah, but your transformer. We we in in transformer architecture, we don't have like external memory that it can write. Do we imply when we're talking about outer aggressive generation, do we imply that this number of bits that are generated in outer aggressive weight may be the same like possibly infinite, like the Turing machine? You're always assuming that the Turing machine will runtime capital T At that point, it's a state. It's a finite state machine. So we imply that our Turing machine definitely holds at some forward the function that we are trying to compute. I would suggest is a fantastic discussion offline since we are a bit late and lunch is there and since we're a little late maybe we can come back at 2:15 or something. But we feel like it. But we'll gravitate to here. We are in a rush, but the more earlier we start, earlier you can.