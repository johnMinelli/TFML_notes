The difference between optimization involved in the treatment of these communication problems. The first point is the linear parameterization. So the class of functions that are in age can be written as like like this. So I write them like this. X is this the input variable, W is the parameter. So W here is and the important thing here is the fact that my function depends linearly on the parameter. In this sense I call this linear parameterization. They can be more general than this. You can replace X with a feature mark you will see. On the other hand, he said you can do non linear parameterizations and I do not discuss them now because we will do it later. In particular there are networks are an example of this. So why I want to talk you into these two examples because here basically this is the good part. The easy part functions that empirical risks or expected risks that need to be minimized with respect to the parameter value in this case are convex. So in this part we are in the convex optimization scenario which is very well established, super studied. Basically we have tools that are comparable to the ones that are available for linear algebra. So it's really a part which is theoretically rounded and also well developed from the software point of view. So here we have convexity and it is super nice and instead here non convex problems immediately even if your loss is the best possible one, let's take the square loss. You will end up with non linear non convex optimization problems. And non convex optimization is difficult not to say in general impossible, right? So the hope here is to in some sense or hopefully what people do and in practice they manage to do it is to leverage the special structure of the optimization problems that are non convex but yet not so well, not so bad behaved because in the end they're finding something that is acceptable for the applications that we need to solve. So the this is the main difference between the two settings. What I want to say is that what are the? I want to stress a bit what are the similarities between the two cases? So in both cases we want to solve to solve problems like this. And in particular this problem as you can see have a very special structure. The first one is an expectation, the second one is a finite sum. So the optimization methods that have been developed in order to deal with these kind of problems are tailored to these kind of problems. So from one hand they they leverage the sound structure and on the other hand they also leverage the usual regularity assumptions that you have on the loss. So when we see that depending on the loss that we choose, we will have different optimization methods. Other things that are in common between the convex and in convex world are the following. So here the parameter is written explicitly please in Rd. Here the parameter is not written but it will be composed by all the coefficients in both the weights involved in our network. So usually here is the B. So common features B is B. So here is B because maybe the data live in a big space and here is that is big because the number of ways that it should be saving. So nowadays for a neural network, maybe you know better than me, but B is of for sure of order of millions to B, but even more that's so we are we are Speaking of what is called a large scale optimization. So B is second observation is common again, at least for modern machine learning applications, this is not always the case, but it's very common. And and these two, these two characteristics motivated, they did the development of very specific optimization problems that must deal with these reflected. And the methods that are referring to are called 1st order methods. These are not among the most effective methods in optimization that make them applicable in this context. So these methods at the end of the 80s were not very used a lot because there were interior point methods, second second order methods that are much more effective to find a solution with a small number of iterations. But the problem is the cost of each iteration. So 2nd order methods are expensive. They need to compute second derivative and then invert. Why? 1st order methods just need the gradient and we see not always the full gradient and just involve just involve matrix factor multiplications. OK, so the point of rest of the methods is that they are in that each iteration is cheap. Then we know that because we know that methods are slow, we will see that they are slow. But in machine learning, usually when you solve this empirical risk minimization, you don't really care for solving this with the high precision because you are not interested in the minimizer of this problem, but you are interested in the minimizer of this ideal problem. And so the solutions that you look are low accuracy solutions. You are not interested in a high accuracy solution. And this is the other reason why 1st order methods may be used. So they they are say the fact that they converge slowly does not harm too much, right. OK, so this is the obligation. And then I said that the methods that we use are also tailored to the fact that this objective function is the sum for an expectation. And this motivated that the of so-called stochastic for them at the moment methods. And then well, really we are not that we are decreasing the request that we make each iteration and do not not required to compute the full gradient, but just make the gradient of one single sum. OK, so this is the recipe between the case. And then there is another piece that we see tomorrow that is still related to the structure that we have on machine learning optimization problem. That is due to the fact that for classification very often, for instance, you would like to use the hinge loss, you know it, this is not differentiable. So what can I do? I cannot use the gradient so we still use first order methods, but I will use methods that are able to minimize functions that are not infraction. So there has been a huge development both in the machine learning community and in the complex optimization community to develop to treat optimization problems that involve non reflection. So we have non smooth optimization. And then the last, last ingredient, then we will start with more here I just wrote in critical risk. I don't know if this morning you have you already seen the regularization? Yes. So you have seen regularization with the square, the normal and also L1. So in this case, maybe your loss is differentiable but your regularizer is not. And in that case you would like to to use not to lose the property of differentiability of the square loss, let's say and. But on the other hand, you have their own nutrition ability introduced by the regularization. So you want to treat these two pieces independently to exploit differentiability of the loss on one hand and to deal with non differentiability of the L1 law. These give this motivated the study of so-called the splitting algorithms where yes, the area here is the area that I can treat this is my optimization problem as let's say single blocks activated them independent. So as I would think I will mainly focus on the convex setting. I will say something in the non convex setting just on Friday. The motivation is that first I know this is what I know. So I know convex optimization. So I will tell you what I know, but also the fact that the methods that are used in the two scenarios are really the same. So complex. The algorithms that implement are the same in the complex setting and in the non complex one. What is really different is the kind of results that you can prove on them, the kind of convergence that you can expect in the two cases. In the convex case you can say much more and that's why I will focus on that state to the convergence results. The underlying ideas are really the same after questions in this part. So what do we see today? Is this for the analytical complexity in the complex of the community and in the machine learning as well? Usually you study convergence of the algorithms from a very specific point of view that is this one. So you first fits A+ of functions and here my functions I will call. I will call it because I want it to be either the empirical list or the expected 1. And here I'm assuming that it goes from RB. So basically what you do when you want to study the complexity of the algorithm from the convex optimization perspective is that you need to fix a class. For those of you that more or less know convex optimization, one very nice class is the class of convex functions with Lipschitz continuous gradient. The conditions are like that. Basically, fix the regularity class for your problems and then you fix an algorithm. And then you want to understand what is the complexity of this algorithm and so you study if I pull to give an FA sucking point, define WK which is the algorithm they applied to F and to this sucking point all the parameters that are fixed are hidden in the a. And then I want to find. So I want to understand how my algorithm performs on the worst function in the class. So I'm looking I'm computing each function in the class on its own sequence and I'm going to take the soup over this class. So I'm really here I'm really looking at how the algorithm goes on the worst possible function. And in order to say to study the complexity of the algorithm, what I look at is this quantity. So I fix an accuracy as you know, and then so is the number of iterations. You see that I must be able to find a number of iterations that depend of course on the accuracy that I fix. That is fine for every function in the class. OK, here I'm a bit sloping with the notation, but it is really the idea. So I want to find a number of iterations that works for everybody. I maybe I need to fix something more. I need to fix the, you know that it will depend on how I initialize the algorithm. So maybe I need to fix the distance from the minimizer for everybody. But I want, I really want a number of iterations that is fine for everybody. This is for the analytical complexity and very often is related. Why this is interesting because this is related to the Oracle concept. So this kind of study has been proposed by and The idea is that I want to study complexity of a given algorithm that uses a given Oracle. In our case, the Oracle will be the gradient, so I can ask AP iteration to an Oracle the gradient of the function of that the important piece of is Oracle. The gradient is the fact that the Oracle depends just on the function around the point where I compute it. It's not a global Oracle, but it's a local Oracle and the other one is the factor that. So this and the thing that I want to say, the Oracle you for the gradient is a first order Oracle. So it requires let's say first order information. What they say, and I agree makes sense to compare different algorithms if they use the same Oracle because using different oracles may have a different cost. So the gradient is the same for the algorithm. So we will focus on 1st order oracles. And here the second assumption, and that makes this kind of study meaningful is the fact that given an algorithm, the number of calls to the Oracle is constant along the iterations. In most cases it will be 1. So I do 1 gradient step, I compute 1 gradient. So once that I know the number of iterations that I need. Once they know how costly is the Oracle, I can multiply this and I get the complexity that one that computer science people study the real one. OK, so This is why it's really important to understand what what kind of Oracle doing. OK, So this is the complexity and very often is it clear, right? The fact that once you fix the Oracle and you're able here we found here we are interested in upper bound quantity, but it's very interesting also to look at lower bounds. So I will not enter precisely into this, but it's really interesting because if I fix the Oracle and I let myself the choice of the algorithms and I have a lower bound, I can speak of optimal algorithms. So optimal methods that use just the grid, we see one optimal method today. Another way to study this kind of complexity is to reverse, let say the role of epsilon and K. And so very often what we do is not really directly this, but it is an intermediate step that is this one. So we study bounds that are for the worst case convergence. Sound look like this does not depend very often. Instead of fixing epsilon and looking at the number of iterations that I need in order to go below accuracy epsilon, what I do is that I prove an upper bound through a medical sequence here. This is sequence AK. Hopefully AK goes to 0 as K goes to classification and is decreasing maybe. And then once I get this inverting, AI can find the complexity of the method. This is the most common way to start convergence of the algorithm. So to prove worst case convergence bounds, this bound should call for area. So here we focus on convergence in function values. This is usually called if I look at the value of the function at on my iterates. So I let's say that my algorithm generates these iterations and what I'm looking at here is these. I'm looking at these numbers here and then looking on how and how they approximate the minimum. So I'm looking at this sequence very often, not so often, machine learning, I have to say. The one is interested not only in the convergence of the function to its minimum, but also in the convergence of the sequence double K. So this is another other measure of other possible quantity of interest. Minus a minimizer when you see a star is always minimizer, so I will always assume that the minimizer exists in all I do today. So the. So the point is that I'm interested also in checking if my sequence converges to a minimizer. So in general, as I would say this is not very interesting in the machine learning. It is in one case for instance, when you look at sparsity. So if you consider LASSO and you're interested in sparsity property for your solution and then you want to know how WK is converging to W stuff, right? For instance, if you know that W study sparse, you would like to know that WK is sparse as well, right? So if these are the the other possible started when we treat it in optimization method. So gradient descent and in particular so and we start and we go straight. So I think motivation. Now let's let's wait a bit before arriving to the first algorithm. We describe the class of functions for which the algorithm works well. So we start with the daily production convex functions, let's say. Is there anyone who haven't? Never. Sorry, one question. With the minimization or the equality written there, are we trying to find which function calculates the minimum K or which function calculates the minimum and difference between F and can you sorry repeat like are we trying to find AK or are we trying to find an F to be inequality? Like are we trying to find an algorithm which reaches minimization in the SO? Yes, WK is given by. So I fixed an algorithm. Let's fix gradient sector so WK is like this. And then I fix. I need to fix also starting point and so given that WK, I want to understand what I can put here, what I can put here. This will give me a convergence rate on my function. And as you will see, this quantity does not depend on X but just on the properties that fits on the class of function that you will see later when I write the data. This is the goal, to find the sequence that works for every function in the class. Are there other questions? The access was F of WK, so sorry I hope Intel and instead was not superior. So the point is this, let's see. So this is W 0. Then I do something and I switch to that one and then I go to W3 let's say. So here my let's say RV is here and here. So when I minimize my functions are always real value. I have an order one R so I can minimize the CZ W1, sorry, zero FW one. So what I'm interested is in understanding how let's say here this is the first quantity, FW0 minus the minimum, then FW 1 minus the minimum, FW 2 minus the minimum. I want to see that this quantity decreases to zero. OK, and I don't want to check on every function, but I want to have this under for every function in the past. OK, so in order to to have these kind of complexity results, we need to introduce the class of functions that we and basically today we will see two classes. Let's assume 1st that the function is complex. So I was asking is there anybody who doesn't know what a convex function is? You all know? OK, so I'll be so you go maybe a bit quick on this. So the we have functions that goes from let's assume Rd. So today in order to be practical, I will do everything in a finite dimensional case. But most of the things that I say work neighbor space exactly is implementable. OK. But the results, the conversion results are OK. OK, so F goes from Rd. to R and this is A of the convex optimizer. You let that your function assumes the value plus and function can assume the value plus and so if Lambda in 01 for every W in ID. So these inequality chords, when you allow yourself to use plus Infinity, you want to check that the sums are OK. We cannot treat plus minus Infinity. So you have to write it in this way and it's fun. OK, there will no big problem. And then I'm in the picture. What I have is a function here. Then I have two points West and U, and what I'm looking at is F evaluated at 1 minus Lambda W plus Lambda U for Lambda 01. So I'm looking at the function evaluated on these points in the segment. Here I have and then what I want to have is that this value, the function evaluated on this point is below this value. Believe me, you can see this as a parameterization of this segment. OK, so there are you are asking is that for any couple of points that you choose for any point inside of this segment, the value of your function is below the second. OK then what you can do is these are complex functions and so I need to add a couple of names that we use them. So since the functions are assumed to be the value plus Infinity, we need a name for the effective domain that are the points where the function is finite. We call Dom F set of WS such that F of West is less than plus Infinity and maybe so this is one example of a convex function. I want to make another example when I use plus Infinity and I try to motivate why we like plus Infinity in the function values and. The example is this one. So I take a convex set C convex subset of RB here, and then my function that is a convex function is 0 on C and plus Infinity otherwise. So these are called indicator functions and that's the functions that allow us to treat constrained optimization as if it was unconstrained. So I cannot in which way I can always have constraints to my optimization problem. If I want to minimize F on the set C is equal to minimize F plus the indicator over the whole space. So in convex optimization you don't have the distinction between constrained optimization and unconstrained optimization. You can always think to to be to solve an unconstrained optimization problem. So the cost of convex functions is nice, but not so nice for optimization. You can do even better. And so now I will introduce another definition of convexity, which is, which strengthens this one, which is really the best possible function class that you can imagine for. Yeah. Sorry, why? Do we need to? Space C to the convex function. Otherwise this is not convex. So if you let's make an example here, if I have to see another more piece here, then you will have that here the function is 0, here the function is 0. Here is plus Infinity. So the inequality does not work. So it's really an equivalence. Other questions I can find the problem is that. I would like to go a bit farther these convex functions and to improve the function class even more. Because in this class there are functions, extreme example, but there are functions like this. So this is a convex function. But why I don't like it? Because in these parts the segment is the same with the graph of the function. So in these parts I have a coordinate here, OK. And this is not nice for optimization as we see who like to quantify the distance between the segment and the function itself. So we can introduce what is called the strongly quota for the strongly complex functions. That is a an extreme, again, an extreme correction of this behavior in the sense that is the stronger the strongest possible. What I mean is that there are other classes in between, so this is all the same as before. I need the function will be so this will be satisfied but. Can you see it? The minus and then we put minus the positive term. So I have the quadratic term that can be subtracted from my left hand side, right hand side, sorry. And as you see this, this really tells me how much this quantity is smaller than this one. So it has to do with the length of this segment here. So what I'm asking is that is the length of this segment here. As you can see, it's related to the square distance between West and U And of course, I have Lambda 1 minus Lambda because this has to take into account that when I go to the extremes, this segment will become small. And yeah, we'll see in a moment how this is related, say of course to having a curvature not to be flat is rules out the flat functions, but also how it is related to some kind of quadratic. So the there are some when I have the. Definition of convex functions. This is not so manageable in the sense that it's not very practical. And it's in general it's difficult to assess if a function is convex or not. And it's even more, maybe more difficult to order the same, to assess if it's strongly convex or not. So this is not always easy. So it's very useful to have some rules, some algebra for convex functions and strongly convex functions. Better starting from some elemental blocks that we know that are convex can allow us to be the more complex convex functions or to assess that some complex functions are convex. Yeah, no, sorry. So I was very, I apologize. So here the function is strongly. So I didn't write it, but F is strongly convex of parameter MU. Strictly positive if the above inequality holds. Sorry, it's like the the correct definition is like this. So I say that the function is new, strongly convex. You can if I can find this positive parameter. Yeah. This class of functions, so many. No, in this class of not yet. This is spoiler, Yeah, we don't deal with. So in this setting, we don't deal with that. This is really complexity on the whole space, right? Yeah, it's a global definition. Other questions if not, OK, so this is the the the correct definition. And then as I would say, I will go a bit fast here. You have these operations that preserve compensity and that you can safely do without altering the compensity of your problem and that's the they write them here. Point is that if you have F&G that are convex then the sum is convex. Then the other one that I like that you use is that if you have a positive coefficient even 0 at the convex and then alpha F is convex. You can fly by positive salads form functions and then the other one picture you take your. Two convex functions and you take the maximum easy steel convex. Why can't alpha be 0A be 0? Because you get 0 and this is a compact function. Function is complex, so constant functions are complex and OK, let's not strongly complex. OK, so you can take 0, yeah, it's not the meaningful, but you get. Then the other thing that you can do is take the maximum of convex functions. You will be convex. And in general here you can be much more general than this. You can take the maximum 5 number or even shoot of an infinite of infinite family of convex functions. And then the other one that is very useful you know in machine learning is the chain rule let's say. So if you have F from Rd. to R which is convex and X that goes from RN to Rd. which is linear, then the composition is convex. So composition with linear functions is OK, right? You can adopt these results to the strongly convex setting. So we will not detail them here. But if you have convex plus strongly convex is a strongly convex, that is the main thing that we use later. OK, so there is a you see how from the end. So there is a strong connection between convexity and the gradient in the sense that we have the 1st order characterization of convexity. So this was let's say the zero order definition in the sense that I don't need any differential equity, but I can transfer convexity to the derivative if the function is differential. So now, so I will treat this part. Treat this part. When I speak of differentiable functions, I make a simplification that the function is a real value and is differentiable everywhere. OK, so F goes from RDR and these differentiable that F is convex is bigger than I'm sure that you know maybe even better than the complexity definition. So sometimes is used as a another definition of convexity. So if you have a function which is differentiable then you in order to check the density you can check that at every point West. Here you build the tangent the graph and you need 12 and the graph of your function must be above the graph of the graph of this tangent line at any point. This must be two for every U once you fix West, but should be true also for any West. So here if I move West, my graph should be above this tangent above this one and all the tangent lines should be below the graph at the same. So we will not see the proof. At the same time you can use the same characterization for strongly complex function, very similar, but a little different. So I like it here. So now strong convexity is characterized to a similar inequality, but a little different in the sense that at each point your graph you should be able to find. So the function is strongly convex. If you are able to find a quadratic function with a fixed curvature that is always below, that is tangent at the point West and always below your graph. So let's put it here. Here I should be able to prove below my graph the graph of the quadratic function something like. Something like this and this quadratic function, the curvature of this function which is given by MU which should be the same at every point, so I translate it, but the MU is always the same. So in this sense that strong convexity is related to quadratic growth at every point, quadratic behavior at every point. Can. Is it fine? So let's do another step. And can characterize convexity and strong convexity even in terms of the second derivative and. So let's assume that differentiable twice differentiable and then I have that differentiable. I have that the second derivative which I denote by this gradient with the two FW is a positive semi definite market. So this quantity is always positive for at least U and the strong convexity is similar in the sense that we just modify this and we get that F is strongly convex to parameter MU if and your leaf, the smallest eigen value of our natural matrix is bigger than MU. So here I can lower bound this quantity by MU U square. So, as your colleague was anticipating, so the class of convex or strongly convex functions is not a good class for gradient method. Then if we assume differentiability, we need to assume another additional assumption. Proceed, and in particular I will introduce now the notion of, let's say L smoothness, usually called L smoothness, that is, continuity of the gradient, call it L smoothness. So you say that F is L, smooth is differentiable and the gradient that is Lipschitz continuous with the constant L. This will be so convex plus Lipschitz continuity of the gradient is a good plus for gradient descent. I immediately write, so this is not really what does it mean? I mean, you see, but it's not really, I don't know geometrically easily interpretable, but we have the characterization for convex functions of Lipschitz continuity of the gradient, which gives a very easy geometric interpretation. That is the descent lemma. The name is not at the moment related to the center, but it will be once we introduce gradient center. So this settlement tells tell us the following thing. Tell us that is convex and differentiable. Then we have an equivalence between. Then the following are equivalent. One the gradient is which is continuous and 2 maybe writing on the other blackboard which is important. So this is the most important technical ingredient of today and this is the key to convergence of gradient descent. This is the following. The following inequality holds. So on one hand I have Lipschitz continuity of the gradient. I have a convex function which is differentiable. On this one I have an inequality that they get to be easily interpreted from the geometric point of view. I have my convex function F and then what the this inequality asks that this must be true. As usual I forgot the quantifiers, but what I what I ask is that at any point I can build this thing on the right, which is again quadratic function with the prescribed L that is always above My F is tangent at the point WF of West. It has the same gradient and is always above the function and I can put these. Let's say this should be a parabola. I can put this everywhere. These are, so this must be tangent. These are the various points where I compute my parabola. And then these parabolas are always been above my convex functionality. So think for a moment. We have a strongly convex function with a Lipschitz continuous gradient. We can put a quadratic function below. A quadratic function above. These functions are very is in this sense. I have an upper bound which is a quadratic upper bound at each point, and I have a quadratic upper bound that is lower bound at each point. This should give you a sense of why this class of functions is particularly easy to treat, right? So in a sense, very often you say that this L mootness is called the property as mootness is a dual property with respect to strong convexity from convexity bounds from below, which is continuity of the gradient bounds from above, right. So then I wanted to make the proof of this because it's easy and I think significant, but I maybe I have been a bit slow. So I want to proceed and tell you at least the gradient, the convergence of gradient descent. So here we have the for a strongly convex and smooth functions. We have a number, a magic number for let's say MU over L or maybe L over MU. It is called the condition number. So if I take L and I divide it by MU I will understand how well behaved is my function. OK, the curve actually is more or less constant around the space. So this this L is always bigger than MU. You reduce it from here writing the other inequality and say the problem it will be as easier as this quantity is closer to 1. Just a couple of additional comments. Basic comments because we want to compute minimizers, but I didn't discuss at all one, the existence of minimizer and 2nd why convex optimization is so easy. And the point is the following, that for convex and imperatible functions, the minimizers are the only critical points. So this is a theorem, no proposition or no, that's what it theorem is. So on that then exit the star minimizer, the gradient. So in order to find the minimizers of the functions, we just need to find the critical point. This cannot happen, right? This cannot happen this point. If I have a point where the gradient is 0, it must be a global. So I think if I started and you can see it easily. So the nicer part of complex optimization is that you can draw stuff and most of the time the intuition, geometric intuition that you get now, even now is 2 is correct except for the case where you go to dimensions where maybe sometimes something doesn't go as you expect, but you finite dimensions very often being pushed. And so for convex functions this is effective and super nice. The other thing that is needed, so this is a characterization, but does not ensure that the convex function as a minimizer. Of course you can think of examples of convex functions, but there are no minimizers. One of the X is a common function which does not. So we need to introduce some kind of coercity in order to issue the existence of the minimizers. So basically you must be able to apply bias trust theorem. So we will not discuss this here. But what happens is that if at another point if F is strong complex then here I I need the differentiable consumers is enough then there exists minimizer. I call it the set of minimizers. I call it argument. And not only and it is unique existed then. So this is another reason why we like strongly complex functions again, because not only for them a minimizer exists, but is also unique. When you have a strongly complex function, you have to like happy. So let's introduce training set so fast. Good. Let's fix a function F which is from R and I have two assumptions. So F is convex and L smooth. Then we define this algorithm. Which is called the gradient. So let's have a look at this algorithm. I start. This is we have in the convex and and smooth setting, I can. The first thing that must be noticed is the fact that I can start wherever I want. This is the first two news. So this W 0 is arbitrary in RB. In the convex scenario you can start whatever you want of course if you start close to the minimizer faster. But in general for any level 0. And then how the algorithm is defined, the internal procedure, I am at the point WK and I move in a direction which is the one of the opposite to the gradient. OK, so if I draw the level sets of the functions of the function F. So what I do is that I am here, this is gamma K and here I will follow the directions of minus the gradient and gamma which is autonomous the level set and points in the direction in the set direction. OK. How much I go in this direction is defined by gamma, which is the step size for in machine learning course learning rate. Very often in the machine learning community, this is called learning rate and it's really the step size must be positive at the moment. And as you can see, this is another good news for this method, which is very simple, is the fact that this is the only free parameter that I have in this algorithm, which is 1B. So in order to implement the algorithm and make it go and say I just need to choose a single parameter. The convergence of the algorithm is really related to the choice of gamma. So I will just do a picture, draw a picture and then write the theorem. The picture is 1D and the function is F of West is West square divided by two. So this is the traffic function, you know where the minimizer is. And basically what the gradient descent tell us to do is that if we're here, we move in this direction, when here we move in this direction, OK, so this direction it's fine. What is the problem is that if we move in this direct, so let's start right here and suppose that we chose a very big gamma. What may happen is that we do a 2 long step and we go there and then we do a 2 long step and we do that. And so the algorithm will diverge. Gamma too big is bad, OK? And if I take gamma too short, what will happen is that yes, I will move, but I will move very, very, very slowly. So the point here, the art of making gradient is to choose the right step. This is the main take home message for gradient step. The only thing that we have to do and here and tell us how to do the step size. And as you may imagine, the step size is related to the furniture of the function, how steep this function is, and this is encoded by this constant N, right? So basically we have our assumptions that are the ones that I wrote there. I have one that I always forget there exists a minimizer. That under these assumptions I can prove the following theorem. We have choose gamma in this interval, so you can choose it positive and less strictly less than 2 / L where L is the Lipschitz constant of the gradient. So here the other thing that I I should stress the fact that this gamma is a constant. OK, so it's just give you 1 parameter does not depend on the iteration. And this gamma should be as you see small enough and how small is dictated by the Lipschitz constant of the gradient. Then I have two beautiful things. One I can prove a convergence rate on this function class that you see like this. Just simplify a bit of the statement. I just write the final statement for gamma less than 1 / l and I tell you what. So the algorithm works until 2 / l What I get is it worst case convergence rate, so I can prove a convergence rate on the function values. I can say that this quantity decreases as constant over K type. This is called a sub linear convergence rate type. It's a slow convergence rate. It's not a good result, but it's the best we can hope for the gradient. These convergence rates depend on the choice of gamma and as you can see on the quality of your initialization. These are just constants, but will affect the quantity that I see here on there. Another comment that has to be made is the fact that this quantity is cannot be computed. So if we fix a priority in epsilon, we will not know how many iterations we need in order to go below what curiosity epsilon. And this must be heuristically determined, right? So this is a theoretical convergence rate, though it's useful because it tell us how the algorithm is even if we cannot compute the 60s. The these these inequality holds for any W star in the set of minimizers. And so here I can basically do distance from the set of minimum. As I I was saying here for simplicity, I choose gamma less than 1 / L because then the step size enters in this easy way. Otherwise the constant is a bit more involved for gamma between 1 / L and 2 / L That's why I wrote it just in this case. The 2nd result is convergence of the iterates. So I can prove that my iterations converge to the minimizer even if the minimizer is not. So even if let's assume here that this is the set of minimizers I will have that my sequence will go somewhere here single minimize. Difficult choice in machine learning is gamma equal to 1 / L and then here you get an L in front. OK, so you see that if L is smaller, it's better in order to prove convert. So how does the prove one? Here is really the descent lemma that I didn't show you before, but it's the key to prove this one. This one instead is more, I would say a bit more deeper, a bit deeper and needs the support lemma. I don't know if you know it, but is a generalization of the contraction the theorem? So I told you that the best possible class for gradient descent is the class of strongly convex functions. Can we say more if the function is strongly convex? Yes we can. And strongly convex acting. So now as you see here, I don't have to assume that the minimizer exists because we already know that there will be a unique minimizer. While here this is necessary in order to prove especially the 2nd result. I can prove some some some kind of subtle dates of the first one, but the second result really needs to assume that the minimizer exists, otherwise doesn't work and it's a strongly convex setting. I can prove this so if F is new, strongly convex and. So here you know the 20s I need to choose the step size that I didn't do so this is true. That's if gamma is less than two over MU plus L. So as you see the the kind of convergence is completely different. Now we have here a quantity that is less than one and so the convergence becomes exponential fast. So this is called linear convergence. This quantity decreases exponentially and the same is true for the iterates here. To prove the theorem you go in another way. So you use the Banachachopoli fixed point theorem and you prove that WK converges to West star. And then from this inequality, this quantity, you reduce 1. So it's really a completely different proof technique. And the idea here is really to interpret the gradient descent as a fixed point iteration. Here we have that identity minus gamma, the gradient applied, and this is constant which is less than one as you can see here. Again, this quantity, this convergence depends on the choice of gamma. And it turns out that the best possible choice you can make for gamma is exactly 2 over MU plus L. So what? What does it mean? The optimal choice is the choice that minimizes this upper bound. OK, so I'm minimizing the upper bound and with that choice what they get is. That quantity becomes. Right. So if I replace the gamma, the optimal gamma that I found, I get that the convergence rate, as you can see, depends on this ratio between L and MU. This is big, this quantity will be close to 1, the convergence will be slow. If this is small, this quantity will be smaller and the convergence will be faster. So the this is the problem one of the weakness of first of the methods, they depend on the conditioning of the problem. This is also why precondition techniques are developed in order to change the problem and improve these constants here. So maybe we have extra 5 minutes, but I wanted just to say tell you, I don't do the computations, I like them for you and I want just to mention two possible applications. So sometimes you have seen this morning how to solve, how to deal with the square loss. In that case, the minds of the empirical risk that can be computed explicitly. But if you are in a larger setting, maybe you can use gradient set like O'Donnell, the conditioner maybe, but it's fine. There are let's say scenarios in which you can use gradient, maybe advantages to use gradient center. And so the the two main applications that we are in mind are let's say FW equal to 1. I think you use this notation right? This this case, this is a function satisfies the assumptions. And so usually this is for short convex. It has a Lipschitz continuous gradient. You can compute explicitly the the Lipschitz constant of the gradient. So this is the gradient and you can put the L which is the maximum eigen value of these matrix. You as long as you are able to compute this you can choose the step size. This is setting where you can apply what we have seen today and if you want. The other case is where you regularize F. So you pass region regression and then in this case you gain. What do you gain regularizing? You can see that you gain from the statistical point of view because you will be able to prove generalization bounds. But you also gain from the numerical point of view. It is a win win because the function becomes. This is convex. In general, this is strongly convex. OK, so this is always strongly convex because this is strongly convex and the MU is bigger or equal than let's is equal to the Lambda. MU is Lambda. Another setting that works well for gradient descent is the one of supervised learning with logistic regret logistic laws. So logistic regression. Also, in that case you can do The parallel is another case where the objective function as a Lipschitz continuous gradient and is convex, and the strongly convex is the natural one. Right, so this one is a. You can compute the explicitly. I can do it. This is a little longer, but you can compute the gradient. And the so the constant now I don't remember depends on the normal exercise, OK, if you regularize it will become strong. So there are other techniques, but I will tell you tomorrow maybe. And so I didn't cover what I had in mind that are acceleration techniques. And adaptive choices of gamma. This is these are related what we usually is called the moving tool in machine learning and this is related instead the other star and the other Adam. So we will see them after the. Here I just summarize what we have seen. Convex functions strongly convex, convex plus Lipschitz continuity of the gradient Convex plus Lipschitz continues OK, but when so strongly convex plus Lipschitz continuity of the gradient, great gradient descent super simple, just one parameter. The plan for the next hour or so is that for 1015 minutes you'll be 20. We're going to welcome.