# Statistical Learning Theory and Linear Models

## Table of Contents
1. [Introduction to Statistical Learning](#introduction-to-statistical-learning)
2. [The Curse of Dimensionality](#the-curse-of-dimensionality)
3. [Statistical Model and Assumptions](#statistical-model-and-assumptions)
4. [Loss Functions and Risk Minimization](#loss-functions-and-risk-minimization)
5. [Target Function and Bayes Risk](#target-function-and-bayes-risk)
6. [Consistency and Convergence](#consistency-and-convergence)
7. [Linear Models](#linear-models)
8. [Regularization](#regularization)
9. [Kernel Trick and Feature Maps](#kernel-trick-and-feature-maps)
10. [Sparse Solutions and LASSO](#sparse-solutions-and-lasso)

---

## Introduction to Statistical Learning

Statistical learning theory provides the mathematical foundation for machine learning algorithms. The core problem involves predicting outcomes based on observed features through data-driven approaches.

### The House Price Prediction Example

Consider predicting house prices based on features like:
- Area (square footage)
- Number of bathrooms  
- Furniture quality
- Location

We represent these features as a vector **x** ‚àà ‚Ñù·µà and want to predict price y ‚àà ‚Ñù.

### Key Components of Learning Problems

- **Input Space (ùí≥)**: The space of all possible feature vectors
- **Output Space (ùí¥)**: The space of all possible outcomes  
- **Training Set**: {(x‚ÇÅ, y‚ÇÅ), ..., (x‚Çô, y‚Çô)} - collection of n examples
- **Learning Algorithm**: A method that maps training data to a prediction function f: ùí≥ ‚Üí ùí¥

### Three Classes of Problems in Statistical Learning

1. **Statistical Properties**: Understanding convergence, consistency, and optimality
2. **Function Spaces**: Choosing appropriate hypothesis classes and selection rules
3. **Optimization**: Computational methods to implement learning rules efficiently

---

## The Curse of Dimensionality

A fundamental challenge in high-dimensional learning is the **curse of dimensionality**.

### Mathematical Formulation

Consider sampling n points uniformly in a d-dimensional unit cube [0,1]·µà. The expected distance to the nearest neighbor is:

**E[distance to nearest neighbor] = C_d √ó n^(-1/d)**

where C_d is a dimension-dependent constant.

### Implications

- For d = 2: distance ‚àù n^(-1/2) (reasonable scaling)
- For d = 10: distance ‚àù n^(-1/10) (very slow improvement)  
- For d = 100: distance ‚àù n^(-1/100) (practically no improvement)

**Key Insight**: In high dimensions, data points become approximately equidistant, making similarity-based learning extremely difficult.

### Practical Example

Modern datasets often have:
- **Images**: ~180,000 pixels (d ‚âà 180,000)
- **Examples**: Millions of samples
- **Challenge**: Even with millions of examples, nearest neighbors are not meaningful in such high dimensions

---

## Statistical Model and Assumptions

### Basic Statistical Framework

We assume the training set is generated by:
- **(X‚ÇÅ, Y‚ÇÅ), ..., (X‚Çô, Y‚Çô)** are **independent and identically distributed (i.i.d.)**
- Each pair follows the same distribution œÅ on ùí≥ √ó ùí¥

### Mathematical Setup

- **ùí≥**: Input space (e.g., ‚Ñù·µà, subset of ‚Ñù·µà, or more general measurable space)
- **ùí¥**: Output space (e.g., ‚Ñù for regression, {0,1} for binary classification)
- **œÅ**: Joint probability measure on ùí≥ √ó ùí¥

### Joint Distribution Decomposition

The joint distribution can be written as:

**œÅ(x,y) = œÅ_X(x) √ó œÅ(y|x)**

where:
- **œÅ_X(x)**: marginal distribution of inputs
- **œÅ(y|x)**: conditional distribution of outputs given inputs

### Tower Property

For any integrable function g:

**‚à´ g(x,y) dœÅ(x,y) = ‚à´ [‚à´ g(x,y) dœÅ(y|x)] dœÅ_X(x)**

This fundamental result allows us to decompose expectations over the joint distribution.

### Conditional Expectation

The **conditional expectation** E[Y|X = x] is a function of x that satisfies:

**E[g(X,Y)] = E[E[g(X,Y)|X]]**

for any integrable function g.

---

## Loss Functions and Risk Minimization

### Loss Function Definition

A **loss function** ‚Ñì: ùí¥ √ó ùí¥ ‚Üí ‚Ñù‚Çä quantifies the cost of predicting ≈∑ when the true value is y.

**Common Loss Functions:**
- **Squared Loss**: ‚Ñì(y, ≈∑) = (y - ≈∑)¬≤
- **Absolute Loss**: ‚Ñì(y, ≈∑) = |y - ≈∑|
- **0-1 Loss** (classification): ‚Ñì(y, ≈∑) = ùüô{y ‚â† ≈∑}
- **Hinge Loss** (SVM): ‚Ñì(y, ≈∑) = max(0, 1 - y≈∑)

### Risk Function

The **risk** (or expected loss) of a function f is:

**R(f) = E[‚Ñì(Y, f(X))] = ‚à´ ‚Ñì(y, f(x)) dœÅ(x,y)**

**Goal**: Find f that minimizes R(f).

### Properties of Squared Loss

The squared loss ‚Ñì(y, ≈∑) = (y - ≈∑)¬≤ has important properties:
- **Convex** in ≈∑
- **Continuous** 
- **Differentiable**

These properties make optimization tractable and theory well-developed.

---

## Target Function and Bayes Risk

### Optimal Predictor

The **target function** (or Bayes predictor) is:

**f*_œÅ(x) = argmin_{≈∑} E[‚Ñì(Y, ≈∑)|X = x]**

### For Squared Loss

Using the tower property and calculus of variations:

**f*_œÅ(x) = E[Y|X = x]**

This is the **regression function** - the conditional expectation of Y given X.

### Bayes Risk

The **Bayes risk** is the minimum achievable risk:

**R* = R(f*_œÅ) = E[‚Ñì(Y, f*_œÅ(X))]**

### Excess Risk

For any predictor f, the **excess risk** is:

**R(f) - R* ‚â• 0**

For squared loss:
**R(f) - R* = E[(f(X) - f*_œÅ(X))¬≤]**

This shows that minimizing excess risk is equivalent to approximating the regression function in L¬≤ norm.

---

## Consistency and Convergence

### Empirical Risk

Since we don't know œÅ, we approximate the risk using the training set:

**RÃÇ_n(f) = (1/n) Œ£·µ¢‚Çå‚ÇÅ‚Åø ‚Ñì(y·µ¢, f(x·µ¢))**

This is the **empirical risk**.

### Learning Algorithm

A **learning algorithm** produces a function fÃÇ_n based on the training set:

**fÃÇ_n = argmin_{f‚ààH} RÃÇ_n(f)**

where H is a **hypothesis class** (space of candidate functions).

### Types of Consistency

#### 1. Consistency in Probability

An algorithm is **consistent in probability** if:

**‚àÄŒµ > 0: P(|R(fÃÇ_n) - R*| > Œµ) ‚Üí 0** as n ‚Üí ‚àû

#### 2. Consistency in Expectation  

An algorithm is **consistent in expectation** if:

**E[R(fÃÇ_n)] ‚Üí R*** as n ‚Üí ‚àû

### Relationship Between Consistency Types

**Theorem**: Consistency in expectation implies consistency in probability (by Markov's inequality).

The converse is not generally true, but holds under additional conditions (bounded convergence theorem).

### Finite Sample Bounds

Instead of asymptotic consistency, we often want **finite sample bounds**:

**P(R(fÃÇ_n) - R* ‚â§ Œµ_n(Œ¥)) ‚â• 1 - Œ¥**

where Œµ_n(Œ¥) ‚Üí 0 as n ‚Üí ‚àû.

### No Free Lunch Theorem

**Fundamental Result**: There exist distributions œÅ such that for any learning algorithm, the convergence rate is arbitrarily slow.

**Implication**: Uniform convergence rates over all distributions are impossible. We need assumptions about the data distribution.

---

## Linear Models

### Motivation

Linear models are fundamental because:
- They're interpretable
- Optimization is convex  
- Theory is well-developed
- They form building blocks for more complex models

### Parametric Form

Consider the hypothesis class of linear functions:

**H = {f(x) = w^T x + b : w ‚àà ‚Ñù·µà, b ‚àà ‚Ñù}**

### Matrix Formulation

Given training data (X, y) where:
- **X ‚àà ‚Ñù^{n√ód}**: design matrix (rows are examples)
- **y ‚àà ‚Ñù‚Åø**: output vector

The empirical risk for squared loss becomes:

**RÃÇ_n(w, b) = (1/n)||Xw + bùüô - y||¬≤**

### Normal Equations

Taking derivatives and setting to zero:

**‚àÇRÃÇ_n/‚àÇw = (2/n)X^T(Xw + bùüô - y) = 0**
**‚àÇRÃÇ_n/‚àÇb = (2/n)ùüô^T(Xw + bùüô - y) = 0**

### Centered Data Approach

If we center the data (subtract means), the solution simplifies:

**≈µ = (X^T X)^{-1} X^T y**

### Gram Matrix Properties

The **Gram matrix** G = X^T X has important properties:
- **Symmetric**: G = G^T
- **Positive semidefinite**: v^T Gv ‚â• 0 for all v
- **Rank**: rank(G) = rank(X) ‚â§ min(n, d)

### Eigendecomposition

G has eigendecomposition:
**G = Œ£·µ¢ Œª·µ¢ v·µ¢ v·µ¢^T**

where Œª·µ¢ ‚â• 0 and {v·µ¢} form an orthonormal basis.

### Two Regimes

#### Underparameterized (n > d)
- If X has full column rank, then G is invertible
- Unique solution: **≈µ = G^{-1} X^T y**
- The solution interpolates the data if the system is exactly determined

#### Overparameterized (n < d)  
- G is singular (not invertible)
- Infinitely many solutions exist
- Need additional criteria to select a solution

### Pseudoinverse Solution

The **minimum norm solution** is:

**≈µ = G^‚Ä† X^T y**

where G^‚Ä† is the **Moore-Penrose pseudoinverse**.

**Key Property**: Among all solutions that minimize empirical risk, this has the smallest ‚Ñì¬≤ norm.

### Pseudoinverse Formula

**G^‚Ä† = Œ£·µ¢: Œª·µ¢>0 (1/Œª·µ¢) v·µ¢ v·µ¢^T**

This is the inverse on the range of G, and zero on the kernel.

### Representation Theorem

**Important Result**: The minimum norm solution can always be written as:

**≈µ = X^T Œ±**

for some Œ± ‚àà ‚Ñù‚Åø. This means the optimal weight vector lies in the span of the training examples.

### Dual Formulation

Substituting w = X^T Œ± into the optimization problem:

**min_Œ± (1/n)||XX^T Œ± - y||¬≤**

The solution is:
**Œ±ÃÇ = (XX^T)^{-1} y**

**Key Advantage**: This involves an n√ón matrix instead of d√ód, which is beneficial when n < d.

---

## Regularization

### Motivation for Regularization

The empirical risk minimization problem can be **ill-posed**:
- Solutions may not exist (inconsistent systems)
- Solutions may not be unique (underdetermined systems)  
- Solutions may be unstable (small changes in data cause large changes in solution)

### Ridge Regression

**Ridge regression** adds an ‚Ñì¬≤ penalty:

**min_w (1/n)||Xw - y||¬≤ + Œª||w||¬≤**

where Œª > 0 is the **regularization parameter**.

### Properties of Ridge Regression

1. **Well-posed**: Always has a unique solution
2. **Stable**: Small changes in data lead to small changes in solution
3. **Continuous dependence**: Solution depends continuously on Œª

### Ridge Solution

The ridge solution is:

**≈µ_Œª = (X^T X + nŒªI)^{-1} X^T y**

### Regularization Effects

As Œª varies:
- **Œª ‚Üí 0**: Approaches unregularized solution (may be unstable)
- **Œª ‚Üí ‚àû**: Solution approaches zero (over-regularized)
- **Optimal Œª**: Balances fit and complexity

### Bias-Variance Tradeoff

Ridge regression exhibits the fundamental **bias-variance tradeoff**:
- **Unregularized (Œª = 0)**: Low bias, high variance
- **Regularized (Œª > 0)**: Higher bias, lower variance  
- **Optimal Œª**: Minimizes total expected loss

### Eigenvalue Perspective

Using the eigendecomposition of X^T X:

**≈µ_Œª = Œ£·µ¢ (Œª·µ¢/(Œª·µ¢ + nŒª)) (v·µ¢^T X^T y) v·µ¢**

**Interpretation**: 
- Large eigenvalues (Œª·µ¢ >> nŒª): Coefficients barely shrunk
- Small eigenvalues (Œª·µ¢ << nŒª): Coefficients heavily shrunk

This provides **automatic relevance determination** - the method naturally identifies and downweights noisy directions.

---

## Kernel Trick and Feature Maps

### Feature Maps

We can make linear models more expressive using **feature maps**:

**œÜ: ùí≥ ‚Üí ‚Ñù·µñ**

The model becomes:
**f(x) = w^T œÜ(x)**

### Examples of Feature Maps

1. **Polynomial features**: œÜ(x) = [1, x, x¬≤, ..., x·µñ]
2. **Interaction terms**: œÜ(x‚ÇÅ, x‚ÇÇ) = [1, x‚ÇÅ, x‚ÇÇ, x‚ÇÅx‚ÇÇ, x‚ÇÅ¬≤, x‚ÇÇ¬≤]
3. **Trigonometric**: œÜ(x) = [1, sin(x), cos(x), sin(2x), cos(2x), ...]

### High-Dimensional Feature Spaces

Feature maps can map to very high (even infinite) dimensional spaces:
- **Polynomial kernel** of degree d: O(d·µñ) dimensions
- **RBF kernel**: Infinite dimensions

### Representer Theorem

**Key Result**: For ridge regression with feature map œÜ, the optimal solution has the form:

**≈µ = Œ£·µ¢‚Çå‚ÇÅ‚Åø Œ±·µ¢ œÜ(x·µ¢)**

**Proof Sketch**: Decompose w into components parallel and orthogonal to span{œÜ(x·µ¢)}. The orthogonal component doesn't affect the empirical risk but increases the penalty.

### Kernel Formulation

Define the **kernel function**:
**K(x, x') = œÜ(x)^T œÜ(x')**

The **kernel matrix** is:
**K_{ij} = K(x·µ¢, x‚±º)**

### Dual Problem

The optimization becomes:
**min_Œ± Œ±^T K Œ± + nŒªŒ±^T Œ± - 2Œ±^T y**

Solution: **Œ±ÃÇ = (K + nŒªI)^{-1} y**

Prediction: **f(x) = Œ£·µ¢ Œ±·µ¢ K(x, x·µ¢)**

### Common Kernels

1. **Linear**: K(x, x') = x^T x'
2. **Polynomial**: K(x, x') = (x^T x' + c)·µà  
3. **RBF/Gaussian**: K(x, x') = exp(-Œ≥||x - x'||¬≤)
4. **Sigmoid**: K(x, x') = tanh(ax^T x' + b)

---

## Sparse Solutions and LASSO

### Motivation for Sparsity

In many applications, we want **sparse solutions** where most coefficients are exactly zero:
- **Feature selection**: Identify relevant features
- **Interpretability**: Simpler models are easier to understand
- **Computational efficiency**: Fewer non-zero parameters

### Limitation of Ridge Regression

Ridge regression shrinks coefficients toward zero but never sets them exactly to zero. The solution is **dense**.

### LASSO Regression

**LASSO** (Least Absolute Shrinkage and Selection Operator) uses ‚Ñì¬π penalty:

**min_w (1/n)||Xw - y||¬≤ + Œª||w||‚ÇÅ**

where ||w||‚ÇÅ = Œ£·µ¢ |w·µ¢|.

### Geometric Intuition

Consider the constraint form of LASSO:
**min_w ||Xw - y||¬≤  subject to  ||w||‚ÇÅ ‚â§ t**

**Key Insight**: The ‚Ñì¬π ball has corners at the coordinate axes, promoting sparse solutions.

### 1D Example

For a single parameter w ‚àà ‚Ñù, the LASSO solution is:

**≈µ = sign(w_ols) max(0, |w_ols| - Œª)**

This is the **soft thresholding** operator.

### Sparsity Regimes

Consider the case with normalized data:

1. **|w_ols| > Œª**: ≈µ ‚â† 0 (feature selected)
2. **|w_ols| ‚â§ Œª**: ≈µ = 0 (feature discarded)

### LASSO Properties

1. **Automatic feature selection**: Sets irrelevant coefficients to exactly zero
2. **Convex optimization**: Global optimum can be found efficiently
3. **Regularization path**: Solution changes continuously with Œª
4. **Model selection**: Cross-validation can select optimal Œª

### Comparison: Ridge vs LASSO

| Property | Ridge (‚Ñì¬≤) | LASSO (‚Ñì¬π) |
|----------|------------|------------|
| Penalty shape | Circle | Diamond |
| Solution | Dense | Sparse |
| Feature selection | No | Yes |
| Correlated features | Keeps all | Picks one |
| Computation | Closed form | Iterative |

### Elastic Net

Combines both penalties:
**min_w ||Xw - y||¬≤ + Œª‚ÇÅ||w||‚ÇÅ + Œª‚ÇÇ||w||¬≤**

**Benefits**:
- Sparsity from ‚Ñì¬π term
- Stability from ‚Ñì¬≤ term  
- Handles correlated features better than pure LASSO

---

## Summary and Key Takeaways

### Fundamental Concepts

1. **Statistical learning** provides the mathematical framework for machine learning
2. **Curse of dimensionality** shows why naive approaches fail in high dimensions
3. **Risk minimization** formalizes the learning objective
4. **Consistency** ensures algorithms work asymptotically

### Linear Models

1. **Ridge regression** provides stable, unique solutions via ‚Ñì¬≤ regularization
2. **LASSO** achieves sparsity via ‚Ñì¬π regularization  
3. **Feature maps** and **kernels** extend linear models to nonlinear problems
4. **Representer theorem** shows optimal solutions lie in span of training data

### Practical Insights

1. **Always regularize** when d ‚â• n (high-dimensional regime)
2. **Cross-validation** for hyperparameter selection
3. **Feature engineering** often more important than complex algorithms
4. **Interpretability vs accuracy** tradeoff guides method selection

### Next Steps

This foundation enables understanding of:
- Neural networks (composition of linear transformations)
- Support vector machines (kernel methods with different losses)
- Ensemble methods (combinations of simple models)
- Deep learning (very deep feature maps)